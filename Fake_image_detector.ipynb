{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulrajesh23/fake-image-detector/blob/main/Fake_image_detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsywQWEI8pzj"
      },
      "source": [
        "# Overview\n",
        "We implement some classifiers to predict whether or not images of cats are \"deepfakes\", i.e., generated by AI. (I used SD 1.5, and down-sampled to match CIFAR-10, which we use for real images.)\n",
        "\n",
        "We will use the functionality of PyTorch, HuggingFace \"transformers\" library for getting pretrained models, scikit-learn (for cross validation utility and for baseline logistic regression), matplotlib for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM7-Cx4H-jTt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import SGD\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "import torchvision.utils as vutils\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Turning off annoying convergence warnings from sklearn\n",
        "from warnings import simplefilter\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "simplefilter(\"ignore\", category=ConvergenceWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR37r18FtEPo"
      },
      "source": [
        "You can download the data file here:\n",
        " https://elearn.ucr.edu/courses/125165/files/12619307/download?download_frd=1\n",
        "You'll have to make them available locally or upload them to your colab instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2-zIA5vYOHA",
        "outputId": "2ea9c7a2-006f-4916-cb07-e15b4c2c1d2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shapes before flattening:\n",
            "X: torch.Size([2000, 3, 32, 32])\n",
            "y: torch.Size([2000])\n",
            "X shape after flattening: torch.Size([2000, 3072])\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load dataset and visualize\n",
        "X, y = torch.load('hw2_data.pt')\n",
        "\n",
        "print('Data shapes before flattening:')\n",
        "print('X:', X.shape)  # 2000, 3, 32, 32, 2000 images, channel, height width\n",
        "print('y:', y.shape)  # 2000 binary labels 0 is real, 1 is fake\n",
        "\n",
        "# Printing examples from each class\n",
        "grid = vutils.make_grid(X[y==0][:8], nrow=4, padding=2, normalize=True)\n",
        "fig, axs = plt.subplots(2, 1, figsize=(8, 8))\n",
        "axs[0].axis('off')\n",
        "axs[0].set_title('REAL Cat images')\n",
        "axs[0].imshow(grid.numpy().transpose((1, 2, 0)))\n",
        "\n",
        "grid = vutils.make_grid(X[y==1][:8], nrow=4, padding=2, normalize=True)\n",
        "axs[1].axis('off')\n",
        "axs[1].set_title('FAKE Cat images')\n",
        "axs[1].imshow(grid.numpy().transpose((1, 2, 0)))\n",
        "\n",
        "\n",
        "X = X.flatten(start_dim=1)  # From now on, we work with the flattened vector\n",
        "print(f\"X shape after flattening: {X.shape}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J15OnWpYy7WH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00c08401-9b86-435c-fd4c-ae3438250b61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy and standard error:\t 0.986 +/- 0.001\n",
            "Validation Accuracy and standard error:\t 0.619 +/- 0.004\n"
          ]
        }
      ],
      "source": [
        "# Using scikit-learn logistic regression (with default hyper-parameters)\n",
        "# with 5-fold CV to get the train and validation accuracies\n",
        "# for a simple linear classifier - a good baseline for our MLP\n",
        "n_folds = 5\n",
        "val_accs = []  # to store validation accuracy for each fold\n",
        "train_accs = []  # to store training accuracy for each fold\n",
        "\n",
        "# iterating over folds, using \"shuffle=True\", as datapoints are not shuffled\n",
        "kf = KFold(n_splits=n_folds, shuffle=True)\n",
        "\n",
        "for train_index, val_index in kf.split(X):\n",
        "\n",
        "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
        "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
        "\n",
        "    # Fitting model on training data\n",
        "\n",
        "    logistic_model = LogisticRegression()\n",
        "    logistic_model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "    # Computing and storing accuracy on train data\n",
        "\n",
        "    train_acc = accuracy_score(y_train_fold, logistic_model.predict(X_train_fold))\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    # Computing and storing accuracy on validation data\n",
        "\n",
        "    val_acc = accuracy_score(y_val_fold, logistic_model.predict(X_val_fold))\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "train_std, train_mean = torch.std_mean(torch.tensor(train_accs))\n",
        "val_std, val_mean = torch.std_mean(torch.tensor(val_accs))\n",
        "\n",
        "# Standard error is standard deviation / sqrt(n), it is more typical to report this\n",
        "rootn = torch.sqrt(torch.tensor(n_folds))  # n is number of folds\n",
        "print(f'Train Accuracy and standard error:\\t {train_mean:.3f} +/- {train_std / rootn:.3f}')\n",
        "print(f'Validation Accuracy and standard error:\\t {val_mean:.3f} +/- {val_std / rootn:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9xatjLpy7WH"
      },
      "source": [
        "## Defining the model\n",
        "\n",
        "- Using Linear layers with ReLU activations for the hidden layers\n",
        "- 2 layers of hidden units. First layer has 128 hidden units, second layer has 64 hidden units.\n",
        "- Output represents *binary* logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "265qk4h2YUrc"
      },
      "outputs": [],
      "source": [
        "class MyMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(32*32*3, 128)  # input size: 32*32*3, hidden layer output(1) size: 128\n",
        "        self.fc2 = nn.Linear(128, 64)  # input size: 128, hidden layer(2) output size: 64\n",
        "        self.fc3 = nn.Linear(64, 2)  # input size: 64, final output size: 2 (logits representing real, fake image classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eknrmjTay7WI"
      },
      "source": [
        "## Train function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FEh0B2jy7WI"
      },
      "outputs": [],
      "source": [
        "# Function to train the model\n",
        "# (called for each hyper-parameter and fold)\n",
        "\n",
        "def train(model, train_loader, val_loader, n_epochs, optimizer, criterion, verbose=False):\n",
        "    \"\"\"Training the model using data from train_loader over n_epochs,\n",
        "    using a Pytorch \"optimizer\" object (SGD in this case)\n",
        "    and \"criterion\" as the loss function (CrossEntropyLoss in this case).\n",
        "    \"\"\"\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()  # Setting the model to training mode\n",
        "        train_loss = 0.0\n",
        "\n",
        "        # Training loop\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()  # Zero the parameter gradients\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs).softmax(dim=1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        # Compute average training loss for the epoch\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Setting the model to evaluation mode\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        # Compute average validation loss for the epoch\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "\n",
        "        if verbose:\n",
        "            print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9T57-qDy7WI"
      },
      "source": [
        "Loop over hyper-parameters and do 5-fold cross-validation for each setting, saving the train and validation mean accuracy and standard error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeZmt1Dsy7WI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1893fd6c-5b16-45e9-a24c-d9c443a9edad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6952, Validation Loss: 0.6936\n",
            "Epoch 2/100, Train Loss: 0.6947, Validation Loss: 0.6928\n",
            "Epoch 3/100, Train Loss: 0.6943, Validation Loss: 0.6921\n",
            "Epoch 4/100, Train Loss: 0.6939, Validation Loss: 0.6913\n",
            "Epoch 5/100, Train Loss: 0.6934, Validation Loss: 0.6906\n",
            "Epoch 6/100, Train Loss: 0.6930, Validation Loss: 0.6899\n",
            "Epoch 7/100, Train Loss: 0.6926, Validation Loss: 0.6892\n",
            "Epoch 8/100, Train Loss: 0.6922, Validation Loss: 0.6885\n",
            "Epoch 9/100, Train Loss: 0.6918, Validation Loss: 0.6879\n",
            "Epoch 10/100, Train Loss: 0.6914, Validation Loss: 0.6872\n",
            "Epoch 11/100, Train Loss: 0.6910, Validation Loss: 0.6865\n",
            "Epoch 12/100, Train Loss: 0.6906, Validation Loss: 0.6859\n",
            "Epoch 13/100, Train Loss: 0.6902, Validation Loss: 0.6852\n",
            "Epoch 14/100, Train Loss: 0.6898, Validation Loss: 0.6846\n",
            "Epoch 15/100, Train Loss: 0.6895, Validation Loss: 0.6840\n",
            "Epoch 16/100, Train Loss: 0.6891, Validation Loss: 0.6833\n",
            "Epoch 17/100, Train Loss: 0.6887, Validation Loss: 0.6827\n",
            "Epoch 18/100, Train Loss: 0.6884, Validation Loss: 0.6821\n",
            "Epoch 19/100, Train Loss: 0.6880, Validation Loss: 0.6814\n",
            "Epoch 20/100, Train Loss: 0.6876, Validation Loss: 0.6808\n",
            "Epoch 21/100, Train Loss: 0.6873, Validation Loss: 0.6802\n",
            "Epoch 22/100, Train Loss: 0.6869, Validation Loss: 0.6796\n",
            "Epoch 23/100, Train Loss: 0.6865, Validation Loss: 0.6789\n",
            "Epoch 24/100, Train Loss: 0.6862, Validation Loss: 0.6783\n",
            "Epoch 25/100, Train Loss: 0.6858, Validation Loss: 0.6777\n",
            "Epoch 26/100, Train Loss: 0.6854, Validation Loss: 0.6770\n",
            "Epoch 27/100, Train Loss: 0.6850, Validation Loss: 0.6764\n",
            "Epoch 28/100, Train Loss: 0.6847, Validation Loss: 0.6758\n",
            "Epoch 29/100, Train Loss: 0.6843, Validation Loss: 0.6751\n",
            "Epoch 30/100, Train Loss: 0.6839, Validation Loss: 0.6744\n",
            "Epoch 31/100, Train Loss: 0.6835, Validation Loss: 0.6738\n",
            "Epoch 32/100, Train Loss: 0.6831, Validation Loss: 0.6731\n",
            "Epoch 33/100, Train Loss: 0.6827, Validation Loss: 0.6724\n",
            "Epoch 34/100, Train Loss: 0.6823, Validation Loss: 0.6717\n",
            "Epoch 35/100, Train Loss: 0.6819, Validation Loss: 0.6710\n",
            "Epoch 36/100, Train Loss: 0.6815, Validation Loss: 0.6703\n",
            "Epoch 37/100, Train Loss: 0.6811, Validation Loss: 0.6696\n",
            "Epoch 38/100, Train Loss: 0.6806, Validation Loss: 0.6689\n",
            "Epoch 39/100, Train Loss: 0.6802, Validation Loss: 0.6681\n",
            "Epoch 40/100, Train Loss: 0.6798, Validation Loss: 0.6674\n",
            "Epoch 41/100, Train Loss: 0.6793, Validation Loss: 0.6667\n",
            "Epoch 42/100, Train Loss: 0.6789, Validation Loss: 0.6659\n",
            "Epoch 43/100, Train Loss: 0.6784, Validation Loss: 0.6652\n",
            "Epoch 44/100, Train Loss: 0.6780, Validation Loss: 0.6644\n",
            "Epoch 45/100, Train Loss: 0.6775, Validation Loss: 0.6636\n",
            "Epoch 46/100, Train Loss: 0.6771, Validation Loss: 0.6629\n",
            "Epoch 47/100, Train Loss: 0.6766, Validation Loss: 0.6621\n",
            "Epoch 48/100, Train Loss: 0.6761, Validation Loss: 0.6613\n",
            "Epoch 49/100, Train Loss: 0.6756, Validation Loss: 0.6605\n",
            "Epoch 50/100, Train Loss: 0.6752, Validation Loss: 0.6597\n",
            "Epoch 51/100, Train Loss: 0.6747, Validation Loss: 0.6588\n",
            "Epoch 52/100, Train Loss: 0.6742, Validation Loss: 0.6580\n",
            "Epoch 53/100, Train Loss: 0.6737, Validation Loss: 0.6572\n",
            "Epoch 54/100, Train Loss: 0.6732, Validation Loss: 0.6563\n",
            "Epoch 55/100, Train Loss: 0.6727, Validation Loss: 0.6555\n",
            "Epoch 56/100, Train Loss: 0.6721, Validation Loss: 0.6546\n",
            "Epoch 57/100, Train Loss: 0.6716, Validation Loss: 0.6537\n",
            "Epoch 58/100, Train Loss: 0.6711, Validation Loss: 0.6529\n",
            "Epoch 59/100, Train Loss: 0.6706, Validation Loss: 0.6520\n",
            "Epoch 60/100, Train Loss: 0.6700, Validation Loss: 0.6511\n",
            "Epoch 61/100, Train Loss: 0.6695, Validation Loss: 0.6502\n",
            "Epoch 62/100, Train Loss: 0.6689, Validation Loss: 0.6492\n",
            "Epoch 63/100, Train Loss: 0.6684, Validation Loss: 0.6483\n",
            "Epoch 64/100, Train Loss: 0.6678, Validation Loss: 0.6474\n",
            "Epoch 65/100, Train Loss: 0.6672, Validation Loss: 0.6464\n",
            "Epoch 66/100, Train Loss: 0.6667, Validation Loss: 0.6455\n",
            "Epoch 67/100, Train Loss: 0.6661, Validation Loss: 0.6445\n",
            "Epoch 68/100, Train Loss: 0.6655, Validation Loss: 0.6436\n",
            "Epoch 69/100, Train Loss: 0.6649, Validation Loss: 0.6426\n",
            "Epoch 70/100, Train Loss: 0.6643, Validation Loss: 0.6416\n",
            "Epoch 71/100, Train Loss: 0.6637, Validation Loss: 0.6406\n",
            "Epoch 72/100, Train Loss: 0.6631, Validation Loss: 0.6396\n",
            "Epoch 73/100, Train Loss: 0.6624, Validation Loss: 0.6386\n",
            "Epoch 74/100, Train Loss: 0.6618, Validation Loss: 0.6375\n",
            "Epoch 75/100, Train Loss: 0.6612, Validation Loss: 0.6365\n",
            "Epoch 76/100, Train Loss: 0.6605, Validation Loss: 0.6355\n",
            "Epoch 77/100, Train Loss: 0.6599, Validation Loss: 0.6344\n",
            "Epoch 78/100, Train Loss: 0.6592, Validation Loss: 0.6334\n",
            "Epoch 79/100, Train Loss: 0.6586, Validation Loss: 0.6323\n",
            "Epoch 80/100, Train Loss: 0.6579, Validation Loss: 0.6313\n",
            "Epoch 81/100, Train Loss: 0.6572, Validation Loss: 0.6302\n",
            "Epoch 82/100, Train Loss: 0.6566, Validation Loss: 0.6291\n",
            "Epoch 83/100, Train Loss: 0.6559, Validation Loss: 0.6280\n",
            "Epoch 84/100, Train Loss: 0.6552, Validation Loss: 0.6269\n",
            "Epoch 85/100, Train Loss: 0.6545, Validation Loss: 0.6258\n",
            "Epoch 86/100, Train Loss: 0.6538, Validation Loss: 0.6247\n",
            "Epoch 87/100, Train Loss: 0.6531, Validation Loss: 0.6236\n",
            "Epoch 88/100, Train Loss: 0.6524, Validation Loss: 0.6225\n",
            "Epoch 89/100, Train Loss: 0.6517, Validation Loss: 0.6213\n",
            "Epoch 90/100, Train Loss: 0.6510, Validation Loss: 0.6202\n",
            "Epoch 91/100, Train Loss: 0.6503, Validation Loss: 0.6191\n",
            "Epoch 92/100, Train Loss: 0.6495, Validation Loss: 0.6179\n",
            "Epoch 93/100, Train Loss: 0.6488, Validation Loss: 0.6168\n",
            "Epoch 94/100, Train Loss: 0.6481, Validation Loss: 0.6156\n",
            "Epoch 95/100, Train Loss: 0.6473, Validation Loss: 0.6145\n",
            "Epoch 96/100, Train Loss: 0.6466, Validation Loss: 0.6133\n",
            "Epoch 97/100, Train Loss: 0.6458, Validation Loss: 0.6121\n",
            "Epoch 98/100, Train Loss: 0.6450, Validation Loss: 0.6110\n",
            "Epoch 99/100, Train Loss: 0.6443, Validation Loss: 0.6098\n",
            "Epoch 100/100, Train Loss: 0.6435, Validation Loss: 0.6086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6927, Validation Loss: 0.6990\n",
            "Epoch 2/100, Train Loss: 0.6923, Validation Loss: 0.6983\n",
            "Epoch 3/100, Train Loss: 0.6918, Validation Loss: 0.6976\n",
            "Epoch 4/100, Train Loss: 0.6913, Validation Loss: 0.6969\n",
            "Epoch 5/100, Train Loss: 0.6908, Validation Loss: 0.6962\n",
            "Epoch 6/100, Train Loss: 0.6904, Validation Loss: 0.6955\n",
            "Epoch 7/100, Train Loss: 0.6899, Validation Loss: 0.6947\n",
            "Epoch 8/100, Train Loss: 0.6894, Validation Loss: 0.6940\n",
            "Epoch 9/100, Train Loss: 0.6889, Validation Loss: 0.6932\n",
            "Epoch 10/100, Train Loss: 0.6884, Validation Loss: 0.6924\n",
            "Epoch 11/100, Train Loss: 0.6879, Validation Loss: 0.6917\n",
            "Epoch 12/100, Train Loss: 0.6874, Validation Loss: 0.6909\n",
            "Epoch 13/100, Train Loss: 0.6869, Validation Loss: 0.6901\n",
            "Epoch 14/100, Train Loss: 0.6864, Validation Loss: 0.6893\n",
            "Epoch 15/100, Train Loss: 0.6859, Validation Loss: 0.6885\n",
            "Epoch 16/100, Train Loss: 0.6853, Validation Loss: 0.6877\n",
            "Epoch 17/100, Train Loss: 0.6848, Validation Loss: 0.6868\n",
            "Epoch 18/100, Train Loss: 0.6843, Validation Loss: 0.6860\n",
            "Epoch 19/100, Train Loss: 0.6838, Validation Loss: 0.6852\n",
            "Epoch 20/100, Train Loss: 0.6832, Validation Loss: 0.6844\n",
            "Epoch 21/100, Train Loss: 0.6827, Validation Loss: 0.6835\n",
            "Epoch 22/100, Train Loss: 0.6822, Validation Loss: 0.6827\n",
            "Epoch 23/100, Train Loss: 0.6816, Validation Loss: 0.6819\n",
            "Epoch 24/100, Train Loss: 0.6811, Validation Loss: 0.6811\n",
            "Epoch 25/100, Train Loss: 0.6806, Validation Loss: 0.6802\n",
            "Epoch 26/100, Train Loss: 0.6800, Validation Loss: 0.6794\n",
            "Epoch 27/100, Train Loss: 0.6795, Validation Loss: 0.6786\n",
            "Epoch 28/100, Train Loss: 0.6789, Validation Loss: 0.6778\n",
            "Epoch 29/100, Train Loss: 0.6784, Validation Loss: 0.6769\n",
            "Epoch 30/100, Train Loss: 0.6778, Validation Loss: 0.6761\n",
            "Epoch 31/100, Train Loss: 0.6773, Validation Loss: 0.6753\n",
            "Epoch 32/100, Train Loss: 0.6767, Validation Loss: 0.6744\n",
            "Epoch 33/100, Train Loss: 0.6762, Validation Loss: 0.6736\n",
            "Epoch 34/100, Train Loss: 0.6756, Validation Loss: 0.6728\n",
            "Epoch 35/100, Train Loss: 0.6750, Validation Loss: 0.6720\n",
            "Epoch 36/100, Train Loss: 0.6745, Validation Loss: 0.6711\n",
            "Epoch 37/100, Train Loss: 0.6739, Validation Loss: 0.6703\n",
            "Epoch 38/100, Train Loss: 0.6734, Validation Loss: 0.6695\n",
            "Epoch 39/100, Train Loss: 0.6728, Validation Loss: 0.6686\n",
            "Epoch 40/100, Train Loss: 0.6722, Validation Loss: 0.6678\n",
            "Epoch 41/100, Train Loss: 0.6717, Validation Loss: 0.6670\n",
            "Epoch 42/100, Train Loss: 0.6711, Validation Loss: 0.6661\n",
            "Epoch 43/100, Train Loss: 0.6705, Validation Loss: 0.6653\n",
            "Epoch 44/100, Train Loss: 0.6699, Validation Loss: 0.6645\n",
            "Epoch 45/100, Train Loss: 0.6694, Validation Loss: 0.6636\n",
            "Epoch 46/100, Train Loss: 0.6688, Validation Loss: 0.6628\n",
            "Epoch 47/100, Train Loss: 0.6682, Validation Loss: 0.6620\n",
            "Epoch 48/100, Train Loss: 0.6676, Validation Loss: 0.6611\n",
            "Epoch 49/100, Train Loss: 0.6670, Validation Loss: 0.6603\n",
            "Epoch 50/100, Train Loss: 0.6665, Validation Loss: 0.6594\n",
            "Epoch 51/100, Train Loss: 0.6659, Validation Loss: 0.6586\n",
            "Epoch 52/100, Train Loss: 0.6653, Validation Loss: 0.6578\n",
            "Epoch 53/100, Train Loss: 0.6647, Validation Loss: 0.6569\n",
            "Epoch 54/100, Train Loss: 0.6641, Validation Loss: 0.6561\n",
            "Epoch 55/100, Train Loss: 0.6635, Validation Loss: 0.6553\n",
            "Epoch 56/100, Train Loss: 0.6629, Validation Loss: 0.6544\n",
            "Epoch 57/100, Train Loss: 0.6623, Validation Loss: 0.6536\n",
            "Epoch 58/100, Train Loss: 0.6617, Validation Loss: 0.6527\n",
            "Epoch 59/100, Train Loss: 0.6611, Validation Loss: 0.6519\n",
            "Epoch 60/100, Train Loss: 0.6605, Validation Loss: 0.6511\n",
            "Epoch 61/100, Train Loss: 0.6599, Validation Loss: 0.6502\n",
            "Epoch 62/100, Train Loss: 0.6593, Validation Loss: 0.6494\n",
            "Epoch 63/100, Train Loss: 0.6587, Validation Loss: 0.6485\n",
            "Epoch 64/100, Train Loss: 0.6581, Validation Loss: 0.6477\n",
            "Epoch 65/100, Train Loss: 0.6575, Validation Loss: 0.6468\n",
            "Epoch 66/100, Train Loss: 0.6569, Validation Loss: 0.6460\n",
            "Epoch 67/100, Train Loss: 0.6563, Validation Loss: 0.6452\n",
            "Epoch 68/100, Train Loss: 0.6557, Validation Loss: 0.6443\n",
            "Epoch 69/100, Train Loss: 0.6551, Validation Loss: 0.6434\n",
            "Epoch 70/100, Train Loss: 0.6545, Validation Loss: 0.6426\n",
            "Epoch 71/100, Train Loss: 0.6539, Validation Loss: 0.6417\n",
            "Epoch 72/100, Train Loss: 0.6533, Validation Loss: 0.6409\n",
            "Epoch 73/100, Train Loss: 0.6526, Validation Loss: 0.6400\n",
            "Epoch 74/100, Train Loss: 0.6520, Validation Loss: 0.6392\n",
            "Epoch 75/100, Train Loss: 0.6514, Validation Loss: 0.6383\n",
            "Epoch 76/100, Train Loss: 0.6508, Validation Loss: 0.6374\n",
            "Epoch 77/100, Train Loss: 0.6502, Validation Loss: 0.6366\n",
            "Epoch 78/100, Train Loss: 0.6495, Validation Loss: 0.6357\n",
            "Epoch 79/100, Train Loss: 0.6489, Validation Loss: 0.6349\n",
            "Epoch 80/100, Train Loss: 0.6483, Validation Loss: 0.6340\n",
            "Epoch 81/100, Train Loss: 0.6476, Validation Loss: 0.6331\n",
            "Epoch 82/100, Train Loss: 0.6470, Validation Loss: 0.6322\n",
            "Epoch 83/100, Train Loss: 0.6464, Validation Loss: 0.6314\n",
            "Epoch 84/100, Train Loss: 0.6457, Validation Loss: 0.6305\n",
            "Epoch 85/100, Train Loss: 0.6451, Validation Loss: 0.6296\n",
            "Epoch 86/100, Train Loss: 0.6444, Validation Loss: 0.6287\n",
            "Epoch 87/100, Train Loss: 0.6438, Validation Loss: 0.6278\n",
            "Epoch 88/100, Train Loss: 0.6432, Validation Loss: 0.6270\n",
            "Epoch 89/100, Train Loss: 0.6425, Validation Loss: 0.6261\n",
            "Epoch 90/100, Train Loss: 0.6419, Validation Loss: 0.6252\n",
            "Epoch 91/100, Train Loss: 0.6412, Validation Loss: 0.6243\n",
            "Epoch 92/100, Train Loss: 0.6406, Validation Loss: 0.6234\n",
            "Epoch 93/100, Train Loss: 0.6399, Validation Loss: 0.6225\n",
            "Epoch 94/100, Train Loss: 0.6393, Validation Loss: 0.6217\n",
            "Epoch 95/100, Train Loss: 0.6386, Validation Loss: 0.6208\n",
            "Epoch 96/100, Train Loss: 0.6380, Validation Loss: 0.6199\n",
            "Epoch 97/100, Train Loss: 0.6373, Validation Loss: 0.6190\n",
            "Epoch 98/100, Train Loss: 0.6366, Validation Loss: 0.6181\n",
            "Epoch 99/100, Train Loss: 0.6360, Validation Loss: 0.6172\n",
            "Epoch 100/100, Train Loss: 0.6353, Validation Loss: 0.6163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6941, Validation Loss: 0.7023\n",
            "Epoch 2/100, Train Loss: 0.6935, Validation Loss: 0.7008\n",
            "Epoch 3/100, Train Loss: 0.6929, Validation Loss: 0.6993\n",
            "Epoch 4/100, Train Loss: 0.6923, Validation Loss: 0.6978\n",
            "Epoch 5/100, Train Loss: 0.6917, Validation Loss: 0.6964\n",
            "Epoch 6/100, Train Loss: 0.6911, Validation Loss: 0.6950\n",
            "Epoch 7/100, Train Loss: 0.6905, Validation Loss: 0.6936\n",
            "Epoch 8/100, Train Loss: 0.6899, Validation Loss: 0.6922\n",
            "Epoch 9/100, Train Loss: 0.6893, Validation Loss: 0.6908\n",
            "Epoch 10/100, Train Loss: 0.6887, Validation Loss: 0.6894\n",
            "Epoch 11/100, Train Loss: 0.6882, Validation Loss: 0.6880\n",
            "Epoch 12/100, Train Loss: 0.6876, Validation Loss: 0.6866\n",
            "Epoch 13/100, Train Loss: 0.6870, Validation Loss: 0.6852\n",
            "Epoch 14/100, Train Loss: 0.6864, Validation Loss: 0.6839\n",
            "Epoch 15/100, Train Loss: 0.6858, Validation Loss: 0.6825\n",
            "Epoch 16/100, Train Loss: 0.6852, Validation Loss: 0.6811\n",
            "Epoch 17/100, Train Loss: 0.6846, Validation Loss: 0.6798\n",
            "Epoch 18/100, Train Loss: 0.6840, Validation Loss: 0.6784\n",
            "Epoch 19/100, Train Loss: 0.6834, Validation Loss: 0.6770\n",
            "Epoch 20/100, Train Loss: 0.6828, Validation Loss: 0.6756\n",
            "Epoch 21/100, Train Loss: 0.6822, Validation Loss: 0.6742\n",
            "Epoch 22/100, Train Loss: 0.6816, Validation Loss: 0.6728\n",
            "Epoch 23/100, Train Loss: 0.6809, Validation Loss: 0.6714\n",
            "Epoch 24/100, Train Loss: 0.6803, Validation Loss: 0.6700\n",
            "Epoch 25/100, Train Loss: 0.6797, Validation Loss: 0.6686\n",
            "Epoch 26/100, Train Loss: 0.6791, Validation Loss: 0.6672\n",
            "Epoch 27/100, Train Loss: 0.6784, Validation Loss: 0.6657\n",
            "Epoch 28/100, Train Loss: 0.6777, Validation Loss: 0.6642\n",
            "Epoch 29/100, Train Loss: 0.6771, Validation Loss: 0.6627\n",
            "Epoch 30/100, Train Loss: 0.6764, Validation Loss: 0.6612\n",
            "Epoch 31/100, Train Loss: 0.6757, Validation Loss: 0.6597\n",
            "Epoch 32/100, Train Loss: 0.6750, Validation Loss: 0.6581\n",
            "Epoch 33/100, Train Loss: 0.6743, Validation Loss: 0.6566\n",
            "Epoch 34/100, Train Loss: 0.6736, Validation Loss: 0.6550\n",
            "Epoch 35/100, Train Loss: 0.6729, Validation Loss: 0.6534\n",
            "Epoch 36/100, Train Loss: 0.6722, Validation Loss: 0.6518\n",
            "Epoch 37/100, Train Loss: 0.6714, Validation Loss: 0.6501\n",
            "Epoch 38/100, Train Loss: 0.6707, Validation Loss: 0.6485\n",
            "Epoch 39/100, Train Loss: 0.6699, Validation Loss: 0.6468\n",
            "Epoch 40/100, Train Loss: 0.6692, Validation Loss: 0.6451\n",
            "Epoch 41/100, Train Loss: 0.6684, Validation Loss: 0.6434\n",
            "Epoch 42/100, Train Loss: 0.6676, Validation Loss: 0.6417\n",
            "Epoch 43/100, Train Loss: 0.6668, Validation Loss: 0.6400\n",
            "Epoch 44/100, Train Loss: 0.6660, Validation Loss: 0.6383\n",
            "Epoch 45/100, Train Loss: 0.6652, Validation Loss: 0.6366\n",
            "Epoch 46/100, Train Loss: 0.6644, Validation Loss: 0.6348\n",
            "Epoch 47/100, Train Loss: 0.6636, Validation Loss: 0.6331\n",
            "Epoch 48/100, Train Loss: 0.6628, Validation Loss: 0.6313\n",
            "Epoch 49/100, Train Loss: 0.6619, Validation Loss: 0.6295\n",
            "Epoch 50/100, Train Loss: 0.6611, Validation Loss: 0.6277\n",
            "Epoch 51/100, Train Loss: 0.6602, Validation Loss: 0.6260\n",
            "Epoch 52/100, Train Loss: 0.6594, Validation Loss: 0.6242\n",
            "Epoch 53/100, Train Loss: 0.6585, Validation Loss: 0.6224\n",
            "Epoch 54/100, Train Loss: 0.6576, Validation Loss: 0.6206\n",
            "Epoch 55/100, Train Loss: 0.6568, Validation Loss: 0.6188\n",
            "Epoch 56/100, Train Loss: 0.6559, Validation Loss: 0.6170\n",
            "Epoch 57/100, Train Loss: 0.6550, Validation Loss: 0.6152\n",
            "Epoch 58/100, Train Loss: 0.6541, Validation Loss: 0.6134\n",
            "Epoch 59/100, Train Loss: 0.6533, Validation Loss: 0.6116\n",
            "Epoch 60/100, Train Loss: 0.6524, Validation Loss: 0.6098\n",
            "Epoch 61/100, Train Loss: 0.6515, Validation Loss: 0.6080\n",
            "Epoch 62/100, Train Loss: 0.6506, Validation Loss: 0.6062\n",
            "Epoch 63/100, Train Loss: 0.6497, Validation Loss: 0.6044\n",
            "Epoch 64/100, Train Loss: 0.6488, Validation Loss: 0.6026\n",
            "Epoch 65/100, Train Loss: 0.6479, Validation Loss: 0.6008\n",
            "Epoch 66/100, Train Loss: 0.6470, Validation Loss: 0.5990\n",
            "Epoch 67/100, Train Loss: 0.6461, Validation Loss: 0.5973\n",
            "Epoch 68/100, Train Loss: 0.6452, Validation Loss: 0.5955\n",
            "Epoch 69/100, Train Loss: 0.6443, Validation Loss: 0.5937\n",
            "Epoch 70/100, Train Loss: 0.6434, Validation Loss: 0.5920\n",
            "Epoch 71/100, Train Loss: 0.6424, Validation Loss: 0.5902\n",
            "Epoch 72/100, Train Loss: 0.6415, Validation Loss: 0.5884\n",
            "Epoch 73/100, Train Loss: 0.6406, Validation Loss: 0.5867\n",
            "Epoch 74/100, Train Loss: 0.6397, Validation Loss: 0.5849\n",
            "Epoch 75/100, Train Loss: 0.6388, Validation Loss: 0.5832\n",
            "Epoch 76/100, Train Loss: 0.6379, Validation Loss: 0.5814\n",
            "Epoch 77/100, Train Loss: 0.6370, Validation Loss: 0.5797\n",
            "Epoch 78/100, Train Loss: 0.6360, Validation Loss: 0.5780\n",
            "Epoch 79/100, Train Loss: 0.6351, Validation Loss: 0.5763\n",
            "Epoch 80/100, Train Loss: 0.6342, Validation Loss: 0.5746\n",
            "Epoch 81/100, Train Loss: 0.6333, Validation Loss: 0.5728\n",
            "Epoch 82/100, Train Loss: 0.6323, Validation Loss: 0.5711\n",
            "Epoch 83/100, Train Loss: 0.6314, Validation Loss: 0.5694\n",
            "Epoch 84/100, Train Loss: 0.6305, Validation Loss: 0.5678\n",
            "Epoch 85/100, Train Loss: 0.6296, Validation Loss: 0.5661\n",
            "Epoch 86/100, Train Loss: 0.6286, Validation Loss: 0.5644\n",
            "Epoch 87/100, Train Loss: 0.6277, Validation Loss: 0.5628\n",
            "Epoch 88/100, Train Loss: 0.6268, Validation Loss: 0.5611\n",
            "Epoch 89/100, Train Loss: 0.6258, Validation Loss: 0.5594\n",
            "Epoch 90/100, Train Loss: 0.6249, Validation Loss: 0.5578\n",
            "Epoch 91/100, Train Loss: 0.6240, Validation Loss: 0.5562\n",
            "Epoch 92/100, Train Loss: 0.6231, Validation Loss: 0.5545\n",
            "Epoch 93/100, Train Loss: 0.6221, Validation Loss: 0.5529\n",
            "Epoch 94/100, Train Loss: 0.6212, Validation Loss: 0.5513\n",
            "Epoch 95/100, Train Loss: 0.6203, Validation Loss: 0.5497\n",
            "Epoch 96/100, Train Loss: 0.6193, Validation Loss: 0.5481\n",
            "Epoch 97/100, Train Loss: 0.6184, Validation Loss: 0.5465\n",
            "Epoch 98/100, Train Loss: 0.6175, Validation Loss: 0.5449\n",
            "Epoch 99/100, Train Loss: 0.6166, Validation Loss: 0.5433\n",
            "Epoch 100/100, Train Loss: 0.6156, Validation Loss: 0.5417\n",
            "Epoch 1/100, Train Loss: 0.6922, Validation Loss: 0.6978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100, Train Loss: 0.6919, Validation Loss: 0.6972\n",
            "Epoch 3/100, Train Loss: 0.6916, Validation Loss: 0.6967\n",
            "Epoch 4/100, Train Loss: 0.6913, Validation Loss: 0.6961\n",
            "Epoch 5/100, Train Loss: 0.6910, Validation Loss: 0.6955\n",
            "Epoch 6/100, Train Loss: 0.6907, Validation Loss: 0.6950\n",
            "Epoch 7/100, Train Loss: 0.6904, Validation Loss: 0.6944\n",
            "Epoch 8/100, Train Loss: 0.6901, Validation Loss: 0.6939\n",
            "Epoch 9/100, Train Loss: 0.6897, Validation Loss: 0.6933\n",
            "Epoch 10/100, Train Loss: 0.6894, Validation Loss: 0.6928\n",
            "Epoch 11/100, Train Loss: 0.6891, Validation Loss: 0.6922\n",
            "Epoch 12/100, Train Loss: 0.6888, Validation Loss: 0.6917\n",
            "Epoch 13/100, Train Loss: 0.6885, Validation Loss: 0.6911\n",
            "Epoch 14/100, Train Loss: 0.6882, Validation Loss: 0.6906\n",
            "Epoch 15/100, Train Loss: 0.6879, Validation Loss: 0.6900\n",
            "Epoch 16/100, Train Loss: 0.6876, Validation Loss: 0.6895\n",
            "Epoch 17/100, Train Loss: 0.6873, Validation Loss: 0.6889\n",
            "Epoch 18/100, Train Loss: 0.6870, Validation Loss: 0.6884\n",
            "Epoch 19/100, Train Loss: 0.6867, Validation Loss: 0.6878\n",
            "Epoch 20/100, Train Loss: 0.6864, Validation Loss: 0.6872\n",
            "Epoch 21/100, Train Loss: 0.6861, Validation Loss: 0.6866\n",
            "Epoch 22/100, Train Loss: 0.6858, Validation Loss: 0.6861\n",
            "Epoch 23/100, Train Loss: 0.6854, Validation Loss: 0.6855\n",
            "Epoch 24/100, Train Loss: 0.6851, Validation Loss: 0.6849\n",
            "Epoch 25/100, Train Loss: 0.6848, Validation Loss: 0.6843\n",
            "Epoch 26/100, Train Loss: 0.6845, Validation Loss: 0.6837\n",
            "Epoch 27/100, Train Loss: 0.6842, Validation Loss: 0.6831\n",
            "Epoch 28/100, Train Loss: 0.6838, Validation Loss: 0.6825\n",
            "Epoch 29/100, Train Loss: 0.6835, Validation Loss: 0.6819\n",
            "Epoch 30/100, Train Loss: 0.6831, Validation Loss: 0.6813\n",
            "Epoch 31/100, Train Loss: 0.6828, Validation Loss: 0.6806\n",
            "Epoch 32/100, Train Loss: 0.6825, Validation Loss: 0.6800\n",
            "Epoch 33/100, Train Loss: 0.6821, Validation Loss: 0.6794\n",
            "Epoch 34/100, Train Loss: 0.6818, Validation Loss: 0.6787\n",
            "Epoch 35/100, Train Loss: 0.6814, Validation Loss: 0.6780\n",
            "Epoch 36/100, Train Loss: 0.6810, Validation Loss: 0.6774\n",
            "Epoch 37/100, Train Loss: 0.6807, Validation Loss: 0.6767\n",
            "Epoch 38/100, Train Loss: 0.6803, Validation Loss: 0.6760\n",
            "Epoch 39/100, Train Loss: 0.6800, Validation Loss: 0.6753\n",
            "Epoch 40/100, Train Loss: 0.6796, Validation Loss: 0.6746\n",
            "Epoch 41/100, Train Loss: 0.6792, Validation Loss: 0.6739\n",
            "Epoch 42/100, Train Loss: 0.6788, Validation Loss: 0.6732\n",
            "Epoch 43/100, Train Loss: 0.6784, Validation Loss: 0.6725\n",
            "Epoch 44/100, Train Loss: 0.6780, Validation Loss: 0.6718\n",
            "Epoch 45/100, Train Loss: 0.6776, Validation Loss: 0.6710\n",
            "Epoch 46/100, Train Loss: 0.6772, Validation Loss: 0.6703\n",
            "Epoch 47/100, Train Loss: 0.6768, Validation Loss: 0.6695\n",
            "Epoch 48/100, Train Loss: 0.6764, Validation Loss: 0.6688\n",
            "Epoch 49/100, Train Loss: 0.6760, Validation Loss: 0.6680\n",
            "Epoch 50/100, Train Loss: 0.6756, Validation Loss: 0.6672\n",
            "Epoch 51/100, Train Loss: 0.6751, Validation Loss: 0.6664\n",
            "Epoch 52/100, Train Loss: 0.6747, Validation Loss: 0.6656\n",
            "Epoch 53/100, Train Loss: 0.6742, Validation Loss: 0.6648\n",
            "Epoch 54/100, Train Loss: 0.6738, Validation Loss: 0.6640\n",
            "Epoch 55/100, Train Loss: 0.6734, Validation Loss: 0.6631\n",
            "Epoch 56/100, Train Loss: 0.6729, Validation Loss: 0.6623\n",
            "Epoch 57/100, Train Loss: 0.6724, Validation Loss: 0.6615\n",
            "Epoch 58/100, Train Loss: 0.6720, Validation Loss: 0.6606\n",
            "Epoch 59/100, Train Loss: 0.6715, Validation Loss: 0.6597\n",
            "Epoch 60/100, Train Loss: 0.6710, Validation Loss: 0.6589\n",
            "Epoch 61/100, Train Loss: 0.6705, Validation Loss: 0.6580\n",
            "Epoch 62/100, Train Loss: 0.6701, Validation Loss: 0.6571\n",
            "Epoch 63/100, Train Loss: 0.6696, Validation Loss: 0.6562\n",
            "Epoch 64/100, Train Loss: 0.6691, Validation Loss: 0.6553\n",
            "Epoch 65/100, Train Loss: 0.6686, Validation Loss: 0.6543\n",
            "Epoch 66/100, Train Loss: 0.6681, Validation Loss: 0.6534\n",
            "Epoch 67/100, Train Loss: 0.6675, Validation Loss: 0.6524\n",
            "Epoch 68/100, Train Loss: 0.6670, Validation Loss: 0.6515\n",
            "Epoch 69/100, Train Loss: 0.6665, Validation Loss: 0.6505\n",
            "Epoch 70/100, Train Loss: 0.6660, Validation Loss: 0.6495\n",
            "Epoch 71/100, Train Loss: 0.6654, Validation Loss: 0.6485\n",
            "Epoch 72/100, Train Loss: 0.6649, Validation Loss: 0.6475\n",
            "Epoch 73/100, Train Loss: 0.6644, Validation Loss: 0.6465\n",
            "Epoch 74/100, Train Loss: 0.6638, Validation Loss: 0.6455\n",
            "Epoch 75/100, Train Loss: 0.6632, Validation Loss: 0.6445\n",
            "Epoch 76/100, Train Loss: 0.6627, Validation Loss: 0.6435\n",
            "Epoch 77/100, Train Loss: 0.6621, Validation Loss: 0.6424\n",
            "Epoch 78/100, Train Loss: 0.6615, Validation Loss: 0.6414\n",
            "Epoch 79/100, Train Loss: 0.6610, Validation Loss: 0.6403\n",
            "Epoch 80/100, Train Loss: 0.6604, Validation Loss: 0.6392\n",
            "Epoch 81/100, Train Loss: 0.6598, Validation Loss: 0.6381\n",
            "Epoch 82/100, Train Loss: 0.6592, Validation Loss: 0.6371\n",
            "Epoch 83/100, Train Loss: 0.6586, Validation Loss: 0.6360\n",
            "Epoch 84/100, Train Loss: 0.6580, Validation Loss: 0.6349\n",
            "Epoch 85/100, Train Loss: 0.6574, Validation Loss: 0.6338\n",
            "Epoch 86/100, Train Loss: 0.6568, Validation Loss: 0.6326\n",
            "Epoch 87/100, Train Loss: 0.6562, Validation Loss: 0.6315\n",
            "Epoch 88/100, Train Loss: 0.6556, Validation Loss: 0.6304\n",
            "Epoch 89/100, Train Loss: 0.6550, Validation Loss: 0.6293\n",
            "Epoch 90/100, Train Loss: 0.6543, Validation Loss: 0.6281\n",
            "Epoch 91/100, Train Loss: 0.6537, Validation Loss: 0.6270\n",
            "Epoch 92/100, Train Loss: 0.6531, Validation Loss: 0.6258\n",
            "Epoch 93/100, Train Loss: 0.6525, Validation Loss: 0.6246\n",
            "Epoch 94/100, Train Loss: 0.6518, Validation Loss: 0.6235\n",
            "Epoch 95/100, Train Loss: 0.6512, Validation Loss: 0.6223\n",
            "Epoch 96/100, Train Loss: 0.6505, Validation Loss: 0.6211\n",
            "Epoch 97/100, Train Loss: 0.6499, Validation Loss: 0.6199\n",
            "Epoch 98/100, Train Loss: 0.6492, Validation Loss: 0.6187\n",
            "Epoch 99/100, Train Loss: 0.6486, Validation Loss: 0.6175\n",
            "Epoch 100/100, Train Loss: 0.6479, Validation Loss: 0.6163\n",
            "Epoch 1/100, Train Loss: 0.6926, Validation Loss: 0.6960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100, Train Loss: 0.6922, Validation Loss: 0.6953\n",
            "Epoch 3/100, Train Loss: 0.6918, Validation Loss: 0.6946\n",
            "Epoch 4/100, Train Loss: 0.6913, Validation Loss: 0.6939\n",
            "Epoch 5/100, Train Loss: 0.6909, Validation Loss: 0.6932\n",
            "Epoch 6/100, Train Loss: 0.6905, Validation Loss: 0.6925\n",
            "Epoch 7/100, Train Loss: 0.6901, Validation Loss: 0.6918\n",
            "Epoch 8/100, Train Loss: 0.6897, Validation Loss: 0.6911\n",
            "Epoch 9/100, Train Loss: 0.6893, Validation Loss: 0.6904\n",
            "Epoch 10/100, Train Loss: 0.6888, Validation Loss: 0.6898\n",
            "Epoch 11/100, Train Loss: 0.6884, Validation Loss: 0.6891\n",
            "Epoch 12/100, Train Loss: 0.6880, Validation Loss: 0.6885\n",
            "Epoch 13/100, Train Loss: 0.6876, Validation Loss: 0.6878\n",
            "Epoch 14/100, Train Loss: 0.6872, Validation Loss: 0.6871\n",
            "Epoch 15/100, Train Loss: 0.6868, Validation Loss: 0.6865\n",
            "Epoch 16/100, Train Loss: 0.6864, Validation Loss: 0.6858\n",
            "Epoch 17/100, Train Loss: 0.6859, Validation Loss: 0.6852\n",
            "Epoch 18/100, Train Loss: 0.6855, Validation Loss: 0.6845\n",
            "Epoch 19/100, Train Loss: 0.6851, Validation Loss: 0.6839\n",
            "Epoch 20/100, Train Loss: 0.6847, Validation Loss: 0.6832\n",
            "Epoch 21/100, Train Loss: 0.6842, Validation Loss: 0.6825\n",
            "Epoch 22/100, Train Loss: 0.6838, Validation Loss: 0.6819\n",
            "Epoch 23/100, Train Loss: 0.6833, Validation Loss: 0.6812\n",
            "Epoch 24/100, Train Loss: 0.6829, Validation Loss: 0.6805\n",
            "Epoch 25/100, Train Loss: 0.6824, Validation Loss: 0.6798\n",
            "Epoch 26/100, Train Loss: 0.6820, Validation Loss: 0.6791\n",
            "Epoch 27/100, Train Loss: 0.6815, Validation Loss: 0.6784\n",
            "Epoch 28/100, Train Loss: 0.6811, Validation Loss: 0.6777\n",
            "Epoch 29/100, Train Loss: 0.6806, Validation Loss: 0.6769\n",
            "Epoch 30/100, Train Loss: 0.6801, Validation Loss: 0.6762\n",
            "Epoch 31/100, Train Loss: 0.6797, Validation Loss: 0.6755\n",
            "Epoch 32/100, Train Loss: 0.6792, Validation Loss: 0.6747\n",
            "Epoch 33/100, Train Loss: 0.6787, Validation Loss: 0.6740\n",
            "Epoch 34/100, Train Loss: 0.6782, Validation Loss: 0.6732\n",
            "Epoch 35/100, Train Loss: 0.6777, Validation Loss: 0.6725\n",
            "Epoch 36/100, Train Loss: 0.6772, Validation Loss: 0.6717\n",
            "Epoch 37/100, Train Loss: 0.6767, Validation Loss: 0.6709\n",
            "Epoch 38/100, Train Loss: 0.6762, Validation Loss: 0.6701\n",
            "Epoch 39/100, Train Loss: 0.6757, Validation Loss: 0.6693\n",
            "Epoch 40/100, Train Loss: 0.6752, Validation Loss: 0.6685\n",
            "Epoch 41/100, Train Loss: 0.6746, Validation Loss: 0.6677\n",
            "Epoch 42/100, Train Loss: 0.6741, Validation Loss: 0.6669\n",
            "Epoch 43/100, Train Loss: 0.6735, Validation Loss: 0.6660\n",
            "Epoch 44/100, Train Loss: 0.6730, Validation Loss: 0.6652\n",
            "Epoch 45/100, Train Loss: 0.6724, Validation Loss: 0.6643\n",
            "Epoch 46/100, Train Loss: 0.6718, Validation Loss: 0.6634\n",
            "Epoch 47/100, Train Loss: 0.6713, Validation Loss: 0.6625\n",
            "Epoch 48/100, Train Loss: 0.6707, Validation Loss: 0.6616\n",
            "Epoch 49/100, Train Loss: 0.6701, Validation Loss: 0.6607\n",
            "Epoch 50/100, Train Loss: 0.6695, Validation Loss: 0.6598\n",
            "Epoch 51/100, Train Loss: 0.6688, Validation Loss: 0.6588\n",
            "Epoch 52/100, Train Loss: 0.6682, Validation Loss: 0.6579\n",
            "Epoch 53/100, Train Loss: 0.6676, Validation Loss: 0.6570\n",
            "Epoch 54/100, Train Loss: 0.6670, Validation Loss: 0.6560\n",
            "Epoch 55/100, Train Loss: 0.6663, Validation Loss: 0.6550\n",
            "Epoch 56/100, Train Loss: 0.6657, Validation Loss: 0.6540\n",
            "Epoch 57/100, Train Loss: 0.6650, Validation Loss: 0.6531\n",
            "Epoch 58/100, Train Loss: 0.6643, Validation Loss: 0.6521\n",
            "Epoch 59/100, Train Loss: 0.6637, Validation Loss: 0.6511\n",
            "Epoch 60/100, Train Loss: 0.6630, Validation Loss: 0.6500\n",
            "Epoch 61/100, Train Loss: 0.6623, Validation Loss: 0.6490\n",
            "Epoch 62/100, Train Loss: 0.6616, Validation Loss: 0.6480\n",
            "Epoch 63/100, Train Loss: 0.6609, Validation Loss: 0.6469\n",
            "Epoch 64/100, Train Loss: 0.6602, Validation Loss: 0.6459\n",
            "Epoch 65/100, Train Loss: 0.6594, Validation Loss: 0.6448\n",
            "Epoch 66/100, Train Loss: 0.6587, Validation Loss: 0.6438\n",
            "Epoch 67/100, Train Loss: 0.6580, Validation Loss: 0.6427\n",
            "Epoch 68/100, Train Loss: 0.6572, Validation Loss: 0.6416\n",
            "Epoch 69/100, Train Loss: 0.6565, Validation Loss: 0.6406\n",
            "Epoch 70/100, Train Loss: 0.6557, Validation Loss: 0.6395\n",
            "Epoch 71/100, Train Loss: 0.6550, Validation Loss: 0.6384\n",
            "Epoch 72/100, Train Loss: 0.6542, Validation Loss: 0.6373\n",
            "Epoch 73/100, Train Loss: 0.6534, Validation Loss: 0.6361\n",
            "Epoch 74/100, Train Loss: 0.6526, Validation Loss: 0.6350\n",
            "Epoch 75/100, Train Loss: 0.6518, Validation Loss: 0.6339\n",
            "Epoch 76/100, Train Loss: 0.6510, Validation Loss: 0.6327\n",
            "Epoch 77/100, Train Loss: 0.6502, Validation Loss: 0.6316\n",
            "Epoch 78/100, Train Loss: 0.6494, Validation Loss: 0.6305\n",
            "Epoch 79/100, Train Loss: 0.6486, Validation Loss: 0.6293\n",
            "Epoch 80/100, Train Loss: 0.6478, Validation Loss: 0.6282\n",
            "Epoch 81/100, Train Loss: 0.6470, Validation Loss: 0.6270\n",
            "Epoch 82/100, Train Loss: 0.6461, Validation Loss: 0.6259\n",
            "Epoch 83/100, Train Loss: 0.6453, Validation Loss: 0.6247\n",
            "Epoch 84/100, Train Loss: 0.6445, Validation Loss: 0.6236\n",
            "Epoch 85/100, Train Loss: 0.6436, Validation Loss: 0.6224\n",
            "Epoch 86/100, Train Loss: 0.6428, Validation Loss: 0.6213\n",
            "Epoch 87/100, Train Loss: 0.6419, Validation Loss: 0.6201\n",
            "Epoch 88/100, Train Loss: 0.6411, Validation Loss: 0.6190\n",
            "Epoch 89/100, Train Loss: 0.6402, Validation Loss: 0.6178\n",
            "Epoch 90/100, Train Loss: 0.6394, Validation Loss: 0.6166\n",
            "Epoch 91/100, Train Loss: 0.6385, Validation Loss: 0.6154\n",
            "Epoch 92/100, Train Loss: 0.6376, Validation Loss: 0.6143\n",
            "Epoch 93/100, Train Loss: 0.6368, Validation Loss: 0.6131\n",
            "Epoch 94/100, Train Loss: 0.6359, Validation Loss: 0.6119\n",
            "Epoch 95/100, Train Loss: 0.6350, Validation Loss: 0.6107\n",
            "Epoch 96/100, Train Loss: 0.6341, Validation Loss: 0.6095\n",
            "Epoch 97/100, Train Loss: 0.6333, Validation Loss: 0.6083\n",
            "Epoch 98/100, Train Loss: 0.6324, Validation Loss: 0.6071\n",
            "Epoch 99/100, Train Loss: 0.6315, Validation Loss: 0.6060\n",
            "Epoch 100/100, Train Loss: 0.6306, Validation Loss: 0.6048\n",
            "Epoch 1/100, Train Loss: 0.6940, Validation Loss: 0.6934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100, Train Loss: 0.6937, Validation Loss: 0.6929\n",
            "Epoch 3/100, Train Loss: 0.6933, Validation Loss: 0.6923\n",
            "Epoch 4/100, Train Loss: 0.6930, Validation Loss: 0.6917\n",
            "Epoch 5/100, Train Loss: 0.6927, Validation Loss: 0.6911\n",
            "Epoch 6/100, Train Loss: 0.6924, Validation Loss: 0.6906\n",
            "Epoch 7/100, Train Loss: 0.6920, Validation Loss: 0.6900\n",
            "Epoch 8/100, Train Loss: 0.6917, Validation Loss: 0.6895\n",
            "Epoch 9/100, Train Loss: 0.6914, Validation Loss: 0.6889\n",
            "Epoch 10/100, Train Loss: 0.6911, Validation Loss: 0.6884\n",
            "Epoch 11/100, Train Loss: 0.6908, Validation Loss: 0.6879\n",
            "Epoch 12/100, Train Loss: 0.6905, Validation Loss: 0.6874\n",
            "Epoch 13/100, Train Loss: 0.6903, Validation Loss: 0.6869\n",
            "Epoch 14/100, Train Loss: 0.6900, Validation Loss: 0.6864\n",
            "Epoch 15/100, Train Loss: 0.6897, Validation Loss: 0.6859\n",
            "Epoch 16/100, Train Loss: 0.6894, Validation Loss: 0.6854\n",
            "Epoch 17/100, Train Loss: 0.6891, Validation Loss: 0.6849\n",
            "Epoch 18/100, Train Loss: 0.6889, Validation Loss: 0.6844\n",
            "Epoch 19/100, Train Loss: 0.6886, Validation Loss: 0.6840\n",
            "Epoch 20/100, Train Loss: 0.6883, Validation Loss: 0.6835\n",
            "Epoch 21/100, Train Loss: 0.6880, Validation Loss: 0.6830\n",
            "Epoch 22/100, Train Loss: 0.6878, Validation Loss: 0.6826\n",
            "Epoch 23/100, Train Loss: 0.6875, Validation Loss: 0.6821\n",
            "Epoch 24/100, Train Loss: 0.6872, Validation Loss: 0.6816\n",
            "Epoch 25/100, Train Loss: 0.6870, Validation Loss: 0.6811\n",
            "Epoch 26/100, Train Loss: 0.6867, Validation Loss: 0.6807\n",
            "Epoch 27/100, Train Loss: 0.6864, Validation Loss: 0.6802\n",
            "Epoch 28/100, Train Loss: 0.6861, Validation Loss: 0.6797\n",
            "Epoch 29/100, Train Loss: 0.6859, Validation Loss: 0.6792\n",
            "Epoch 30/100, Train Loss: 0.6856, Validation Loss: 0.6787\n",
            "Epoch 31/100, Train Loss: 0.6853, Validation Loss: 0.6783\n",
            "Epoch 32/100, Train Loss: 0.6850, Validation Loss: 0.6778\n",
            "Epoch 33/100, Train Loss: 0.6847, Validation Loss: 0.6773\n",
            "Epoch 34/100, Train Loss: 0.6845, Validation Loss: 0.6768\n",
            "Epoch 35/100, Train Loss: 0.6842, Validation Loss: 0.6763\n",
            "Epoch 36/100, Train Loss: 0.6839, Validation Loss: 0.6758\n",
            "Epoch 37/100, Train Loss: 0.6836, Validation Loss: 0.6753\n",
            "Epoch 38/100, Train Loss: 0.6833, Validation Loss: 0.6748\n",
            "Epoch 39/100, Train Loss: 0.6830, Validation Loss: 0.6743\n",
            "Epoch 40/100, Train Loss: 0.6827, Validation Loss: 0.6738\n",
            "Epoch 41/100, Train Loss: 0.6824, Validation Loss: 0.6732\n",
            "Epoch 42/100, Train Loss: 0.6821, Validation Loss: 0.6727\n",
            "Epoch 43/100, Train Loss: 0.6818, Validation Loss: 0.6722\n",
            "Epoch 44/100, Train Loss: 0.6815, Validation Loss: 0.6716\n",
            "Epoch 45/100, Train Loss: 0.6812, Validation Loss: 0.6711\n",
            "Epoch 46/100, Train Loss: 0.6809, Validation Loss: 0.6705\n",
            "Epoch 47/100, Train Loss: 0.6805, Validation Loss: 0.6700\n",
            "Epoch 48/100, Train Loss: 0.6802, Validation Loss: 0.6694\n",
            "Epoch 49/100, Train Loss: 0.6799, Validation Loss: 0.6688\n",
            "Epoch 50/100, Train Loss: 0.6795, Validation Loss: 0.6683\n",
            "Epoch 51/100, Train Loss: 0.6792, Validation Loss: 0.6677\n",
            "Epoch 52/100, Train Loss: 0.6789, Validation Loss: 0.6671\n",
            "Epoch 53/100, Train Loss: 0.6785, Validation Loss: 0.6665\n",
            "Epoch 54/100, Train Loss: 0.6782, Validation Loss: 0.6659\n",
            "Epoch 55/100, Train Loss: 0.6778, Validation Loss: 0.6653\n",
            "Epoch 56/100, Train Loss: 0.6775, Validation Loss: 0.6646\n",
            "Epoch 57/100, Train Loss: 0.6771, Validation Loss: 0.6640\n",
            "Epoch 58/100, Train Loss: 0.6767, Validation Loss: 0.6634\n",
            "Epoch 59/100, Train Loss: 0.6763, Validation Loss: 0.6627\n",
            "Epoch 60/100, Train Loss: 0.6760, Validation Loss: 0.6621\n",
            "Epoch 61/100, Train Loss: 0.6756, Validation Loss: 0.6614\n",
            "Epoch 62/100, Train Loss: 0.6752, Validation Loss: 0.6608\n",
            "Epoch 63/100, Train Loss: 0.6748, Validation Loss: 0.6601\n",
            "Epoch 64/100, Train Loss: 0.6744, Validation Loss: 0.6594\n",
            "Epoch 65/100, Train Loss: 0.6740, Validation Loss: 0.6587\n",
            "Epoch 66/100, Train Loss: 0.6736, Validation Loss: 0.6580\n",
            "Epoch 67/100, Train Loss: 0.6732, Validation Loss: 0.6573\n",
            "Epoch 68/100, Train Loss: 0.6728, Validation Loss: 0.6566\n",
            "Epoch 69/100, Train Loss: 0.6723, Validation Loss: 0.6559\n",
            "Epoch 70/100, Train Loss: 0.6719, Validation Loss: 0.6552\n",
            "Epoch 71/100, Train Loss: 0.6715, Validation Loss: 0.6544\n",
            "Epoch 72/100, Train Loss: 0.6710, Validation Loss: 0.6537\n",
            "Epoch 73/100, Train Loss: 0.6706, Validation Loss: 0.6529\n",
            "Epoch 74/100, Train Loss: 0.6701, Validation Loss: 0.6521\n",
            "Epoch 75/100, Train Loss: 0.6697, Validation Loss: 0.6514\n",
            "Epoch 76/100, Train Loss: 0.6692, Validation Loss: 0.6506\n",
            "Epoch 77/100, Train Loss: 0.6687, Validation Loss: 0.6498\n",
            "Epoch 78/100, Train Loss: 0.6682, Validation Loss: 0.6490\n",
            "Epoch 79/100, Train Loss: 0.6678, Validation Loss: 0.6482\n",
            "Epoch 80/100, Train Loss: 0.6673, Validation Loss: 0.6474\n",
            "Epoch 81/100, Train Loss: 0.6668, Validation Loss: 0.6465\n",
            "Epoch 82/100, Train Loss: 0.6663, Validation Loss: 0.6457\n",
            "Epoch 83/100, Train Loss: 0.6658, Validation Loss: 0.6449\n",
            "Epoch 84/100, Train Loss: 0.6653, Validation Loss: 0.6440\n",
            "Epoch 85/100, Train Loss: 0.6648, Validation Loss: 0.6431\n",
            "Epoch 86/100, Train Loss: 0.6642, Validation Loss: 0.6423\n",
            "Epoch 87/100, Train Loss: 0.6637, Validation Loss: 0.6414\n",
            "Epoch 88/100, Train Loss: 0.6632, Validation Loss: 0.6405\n",
            "Epoch 89/100, Train Loss: 0.6627, Validation Loss: 0.6397\n",
            "Epoch 90/100, Train Loss: 0.6621, Validation Loss: 0.6388\n",
            "Epoch 91/100, Train Loss: 0.6616, Validation Loss: 0.6379\n",
            "Epoch 92/100, Train Loss: 0.6610, Validation Loss: 0.6370\n",
            "Epoch 93/100, Train Loss: 0.6605, Validation Loss: 0.6360\n",
            "Epoch 94/100, Train Loss: 0.6599, Validation Loss: 0.6351\n",
            "Epoch 95/100, Train Loss: 0.6594, Validation Loss: 0.6342\n",
            "Epoch 96/100, Train Loss: 0.6588, Validation Loss: 0.6333\n",
            "Epoch 97/100, Train Loss: 0.6583, Validation Loss: 0.6323\n",
            "Epoch 98/100, Train Loss: 0.6577, Validation Loss: 0.6314\n",
            "Epoch 99/100, Train Loss: 0.6571, Validation Loss: 0.6304\n",
            "Epoch 100/100, Train Loss: 0.6565, Validation Loss: 0.6295\n",
            "Epoch 1/100, Train Loss: 0.6944, Validation Loss: 0.6953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100, Train Loss: 0.6940, Validation Loss: 0.6948\n",
            "Epoch 3/100, Train Loss: 0.6937, Validation Loss: 0.6943\n",
            "Epoch 4/100, Train Loss: 0.6933, Validation Loss: 0.6937\n",
            "Epoch 5/100, Train Loss: 0.6930, Validation Loss: 0.6932\n",
            "Epoch 6/100, Train Loss: 0.6926, Validation Loss: 0.6926\n",
            "Epoch 7/100, Train Loss: 0.6923, Validation Loss: 0.6921\n",
            "Epoch 8/100, Train Loss: 0.6919, Validation Loss: 0.6915\n",
            "Epoch 9/100, Train Loss: 0.6916, Validation Loss: 0.6910\n",
            "Epoch 10/100, Train Loss: 0.6912, Validation Loss: 0.6904\n",
            "Epoch 11/100, Train Loss: 0.6909, Validation Loss: 0.6898\n",
            "Epoch 12/100, Train Loss: 0.6905, Validation Loss: 0.6893\n",
            "Epoch 13/100, Train Loss: 0.6901, Validation Loss: 0.6887\n",
            "Epoch 14/100, Train Loss: 0.6898, Validation Loss: 0.6881\n",
            "Epoch 15/100, Train Loss: 0.6894, Validation Loss: 0.6875\n",
            "Epoch 16/100, Train Loss: 0.6890, Validation Loss: 0.6868\n",
            "Epoch 17/100, Train Loss: 0.6886, Validation Loss: 0.6862\n",
            "Epoch 18/100, Train Loss: 0.6882, Validation Loss: 0.6856\n",
            "Epoch 19/100, Train Loss: 0.6878, Validation Loss: 0.6849\n",
            "Epoch 20/100, Train Loss: 0.6874, Validation Loss: 0.6843\n",
            "Epoch 21/100, Train Loss: 0.6870, Validation Loss: 0.6836\n",
            "Epoch 22/100, Train Loss: 0.6866, Validation Loss: 0.6830\n",
            "Epoch 23/100, Train Loss: 0.6862, Validation Loss: 0.6823\n",
            "Epoch 24/100, Train Loss: 0.6858, Validation Loss: 0.6816\n",
            "Epoch 25/100, Train Loss: 0.6853, Validation Loss: 0.6809\n",
            "Epoch 26/100, Train Loss: 0.6849, Validation Loss: 0.6802\n",
            "Epoch 27/100, Train Loss: 0.6845, Validation Loss: 0.6795\n",
            "Epoch 28/100, Train Loss: 0.6840, Validation Loss: 0.6788\n",
            "Epoch 29/100, Train Loss: 0.6836, Validation Loss: 0.6781\n",
            "Epoch 30/100, Train Loss: 0.6832, Validation Loss: 0.6774\n",
            "Epoch 31/100, Train Loss: 0.6827, Validation Loss: 0.6767\n",
            "Epoch 32/100, Train Loss: 0.6823, Validation Loss: 0.6759\n",
            "Epoch 33/100, Train Loss: 0.6818, Validation Loss: 0.6752\n",
            "Epoch 34/100, Train Loss: 0.6813, Validation Loss: 0.6745\n",
            "Epoch 35/100, Train Loss: 0.6809, Validation Loss: 0.6737\n",
            "Epoch 36/100, Train Loss: 0.6804, Validation Loss: 0.6730\n",
            "Epoch 37/100, Train Loss: 0.6800, Validation Loss: 0.6722\n",
            "Epoch 38/100, Train Loss: 0.6795, Validation Loss: 0.6714\n",
            "Epoch 39/100, Train Loss: 0.6790, Validation Loss: 0.6706\n",
            "Epoch 40/100, Train Loss: 0.6785, Validation Loss: 0.6699\n",
            "Epoch 41/100, Train Loss: 0.6781, Validation Loss: 0.6691\n",
            "Epoch 42/100, Train Loss: 0.6776, Validation Loss: 0.6683\n",
            "Epoch 43/100, Train Loss: 0.6771, Validation Loss: 0.6675\n",
            "Epoch 44/100, Train Loss: 0.6766, Validation Loss: 0.6667\n",
            "Epoch 45/100, Train Loss: 0.6761, Validation Loss: 0.6659\n",
            "Epoch 46/100, Train Loss: 0.6757, Validation Loss: 0.6651\n",
            "Epoch 47/100, Train Loss: 0.6752, Validation Loss: 0.6643\n",
            "Epoch 48/100, Train Loss: 0.6747, Validation Loss: 0.6634\n",
            "Epoch 49/100, Train Loss: 0.6742, Validation Loss: 0.6626\n",
            "Epoch 50/100, Train Loss: 0.6737, Validation Loss: 0.6618\n",
            "Epoch 51/100, Train Loss: 0.6732, Validation Loss: 0.6610\n",
            "Epoch 52/100, Train Loss: 0.6727, Validation Loss: 0.6601\n",
            "Epoch 53/100, Train Loss: 0.6722, Validation Loss: 0.6593\n",
            "Epoch 54/100, Train Loss: 0.6717, Validation Loss: 0.6584\n",
            "Epoch 55/100, Train Loss: 0.6712, Validation Loss: 0.6576\n",
            "Epoch 56/100, Train Loss: 0.6706, Validation Loss: 0.6567\n",
            "Epoch 57/100, Train Loss: 0.6701, Validation Loss: 0.6559\n",
            "Epoch 58/100, Train Loss: 0.6696, Validation Loss: 0.6550\n",
            "Epoch 59/100, Train Loss: 0.6691, Validation Loss: 0.6542\n",
            "Epoch 60/100, Train Loss: 0.6686, Validation Loss: 0.6533\n",
            "Epoch 61/100, Train Loss: 0.6680, Validation Loss: 0.6525\n",
            "Epoch 62/100, Train Loss: 0.6675, Validation Loss: 0.6516\n",
            "Epoch 63/100, Train Loss: 0.6670, Validation Loss: 0.6507\n",
            "Epoch 64/100, Train Loss: 0.6664, Validation Loss: 0.6499\n",
            "Epoch 65/100, Train Loss: 0.6659, Validation Loss: 0.6490\n",
            "Epoch 66/100, Train Loss: 0.6654, Validation Loss: 0.6481\n",
            "Epoch 67/100, Train Loss: 0.6648, Validation Loss: 0.6473\n",
            "Epoch 68/100, Train Loss: 0.6643, Validation Loss: 0.6464\n",
            "Epoch 69/100, Train Loss: 0.6637, Validation Loss: 0.6455\n",
            "Epoch 70/100, Train Loss: 0.6632, Validation Loss: 0.6446\n",
            "Epoch 71/100, Train Loss: 0.6626, Validation Loss: 0.6438\n",
            "Epoch 72/100, Train Loss: 0.6621, Validation Loss: 0.6429\n",
            "Epoch 73/100, Train Loss: 0.6615, Validation Loss: 0.6420\n",
            "Epoch 74/100, Train Loss: 0.6610, Validation Loss: 0.6411\n",
            "Epoch 75/100, Train Loss: 0.6604, Validation Loss: 0.6402\n",
            "Epoch 76/100, Train Loss: 0.6599, Validation Loss: 0.6393\n",
            "Epoch 77/100, Train Loss: 0.6593, Validation Loss: 0.6384\n",
            "Epoch 78/100, Train Loss: 0.6588, Validation Loss: 0.6376\n",
            "Epoch 79/100, Train Loss: 0.6582, Validation Loss: 0.6367\n",
            "Epoch 80/100, Train Loss: 0.6577, Validation Loss: 0.6358\n",
            "Epoch 81/100, Train Loss: 0.6571, Validation Loss: 0.6349\n",
            "Epoch 82/100, Train Loss: 0.6565, Validation Loss: 0.6340\n",
            "Epoch 83/100, Train Loss: 0.6560, Validation Loss: 0.6331\n",
            "Epoch 84/100, Train Loss: 0.6554, Validation Loss: 0.6322\n",
            "Epoch 85/100, Train Loss: 0.6548, Validation Loss: 0.6313\n",
            "Epoch 86/100, Train Loss: 0.6543, Validation Loss: 0.6305\n",
            "Epoch 87/100, Train Loss: 0.6537, Validation Loss: 0.6296\n",
            "Epoch 88/100, Train Loss: 0.6532, Validation Loss: 0.6287\n",
            "Epoch 89/100, Train Loss: 0.6526, Validation Loss: 0.6278\n",
            "Epoch 90/100, Train Loss: 0.6520, Validation Loss: 0.6269\n",
            "Epoch 91/100, Train Loss: 0.6514, Validation Loss: 0.6260\n",
            "Epoch 92/100, Train Loss: 0.6509, Validation Loss: 0.6252\n",
            "Epoch 93/100, Train Loss: 0.6503, Validation Loss: 0.6243\n",
            "Epoch 94/100, Train Loss: 0.6497, Validation Loss: 0.6234\n",
            "Epoch 95/100, Train Loss: 0.6491, Validation Loss: 0.6225\n",
            "Epoch 96/100, Train Loss: 0.6486, Validation Loss: 0.6216\n",
            "Epoch 97/100, Train Loss: 0.6480, Validation Loss: 0.6207\n",
            "Epoch 98/100, Train Loss: 0.6474, Validation Loss: 0.6198\n",
            "Epoch 99/100, Train Loss: 0.6468, Validation Loss: 0.6189\n",
            "Epoch 100/100, Train Loss: 0.6462, Validation Loss: 0.6180\n",
            "Epoch 1/100, Train Loss: 0.6926, Validation Loss: 0.6934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100, Train Loss: 0.6922, Validation Loss: 0.6927\n",
            "Epoch 3/100, Train Loss: 0.6917, Validation Loss: 0.6919\n",
            "Epoch 4/100, Train Loss: 0.6913, Validation Loss: 0.6912\n",
            "Epoch 5/100, Train Loss: 0.6908, Validation Loss: 0.6905\n",
            "Epoch 6/100, Train Loss: 0.6903, Validation Loss: 0.6897\n",
            "Epoch 7/100, Train Loss: 0.6898, Validation Loss: 0.6889\n",
            "Epoch 8/100, Train Loss: 0.6894, Validation Loss: 0.6882\n",
            "Epoch 9/100, Train Loss: 0.6889, Validation Loss: 0.6874\n",
            "Epoch 10/100, Train Loss: 0.6884, Validation Loss: 0.6866\n",
            "Epoch 11/100, Train Loss: 0.6879, Validation Loss: 0.6858\n",
            "Epoch 12/100, Train Loss: 0.6874, Validation Loss: 0.6850\n",
            "Epoch 13/100, Train Loss: 0.6869, Validation Loss: 0.6841\n",
            "Epoch 14/100, Train Loss: 0.6864, Validation Loss: 0.6833\n",
            "Epoch 15/100, Train Loss: 0.6859, Validation Loss: 0.6825\n",
            "Epoch 16/100, Train Loss: 0.6854, Validation Loss: 0.6816\n",
            "Epoch 17/100, Train Loss: 0.6848, Validation Loss: 0.6808\n",
            "Epoch 18/100, Train Loss: 0.6843, Validation Loss: 0.6799\n",
            "Epoch 19/100, Train Loss: 0.6838, Validation Loss: 0.6791\n",
            "Epoch 20/100, Train Loss: 0.6833, Validation Loss: 0.6782\n",
            "Epoch 21/100, Train Loss: 0.6827, Validation Loss: 0.6773\n",
            "Epoch 22/100, Train Loss: 0.6822, Validation Loss: 0.6764\n",
            "Epoch 23/100, Train Loss: 0.6816, Validation Loss: 0.6755\n",
            "Epoch 24/100, Train Loss: 0.6811, Validation Loss: 0.6746\n",
            "Epoch 25/100, Train Loss: 0.6805, Validation Loss: 0.6737\n",
            "Epoch 26/100, Train Loss: 0.6800, Validation Loss: 0.6728\n",
            "Epoch 27/100, Train Loss: 0.6794, Validation Loss: 0.6718\n",
            "Epoch 28/100, Train Loss: 0.6788, Validation Loss: 0.6709\n",
            "Epoch 29/100, Train Loss: 0.6782, Validation Loss: 0.6699\n",
            "Epoch 30/100, Train Loss: 0.6777, Validation Loss: 0.6689\n",
            "Epoch 31/100, Train Loss: 0.6771, Validation Loss: 0.6680\n",
            "Epoch 32/100, Train Loss: 0.6765, Validation Loss: 0.6670\n",
            "Epoch 33/100, Train Loss: 0.6759, Validation Loss: 0.6660\n",
            "Epoch 34/100, Train Loss: 0.6753, Validation Loss: 0.6650\n",
            "Epoch 35/100, Train Loss: 0.6746, Validation Loss: 0.6640\n",
            "Epoch 36/100, Train Loss: 0.6740, Validation Loss: 0.6630\n",
            "Epoch 37/100, Train Loss: 0.6734, Validation Loss: 0.6620\n",
            "Epoch 38/100, Train Loss: 0.6728, Validation Loss: 0.6610\n",
            "Epoch 39/100, Train Loss: 0.6721, Validation Loss: 0.6600\n",
            "Epoch 40/100, Train Loss: 0.6715, Validation Loss: 0.6590\n",
            "Epoch 41/100, Train Loss: 0.6709, Validation Loss: 0.6580\n",
            "Epoch 42/100, Train Loss: 0.6702, Validation Loss: 0.6569\n",
            "Epoch 43/100, Train Loss: 0.6696, Validation Loss: 0.6559\n",
            "Epoch 44/100, Train Loss: 0.6689, Validation Loss: 0.6549\n",
            "Epoch 45/100, Train Loss: 0.6683, Validation Loss: 0.6539\n",
            "Epoch 46/100, Train Loss: 0.6676, Validation Loss: 0.6528\n",
            "Epoch 47/100, Train Loss: 0.6670, Validation Loss: 0.6518\n",
            "Epoch 48/100, Train Loss: 0.6663, Validation Loss: 0.6508\n",
            "Epoch 49/100, Train Loss: 0.6657, Validation Loss: 0.6498\n",
            "Epoch 50/100, Train Loss: 0.6650, Validation Loss: 0.6487\n",
            "Epoch 51/100, Train Loss: 0.6644, Validation Loss: 0.6477\n",
            "Epoch 52/100, Train Loss: 0.6637, Validation Loss: 0.6467\n",
            "Epoch 53/100, Train Loss: 0.6630, Validation Loss: 0.6456\n",
            "Epoch 54/100, Train Loss: 0.6624, Validation Loss: 0.6446\n",
            "Epoch 55/100, Train Loss: 0.6617, Validation Loss: 0.6436\n",
            "Epoch 56/100, Train Loss: 0.6610, Validation Loss: 0.6425\n",
            "Epoch 57/100, Train Loss: 0.6603, Validation Loss: 0.6415\n",
            "Epoch 58/100, Train Loss: 0.6597, Validation Loss: 0.6405\n",
            "Epoch 59/100, Train Loss: 0.6590, Validation Loss: 0.6395\n",
            "Epoch 60/100, Train Loss: 0.6583, Validation Loss: 0.6385\n",
            "Epoch 61/100, Train Loss: 0.6577, Validation Loss: 0.6375\n",
            "Epoch 62/100, Train Loss: 0.6570, Validation Loss: 0.6365\n",
            "Epoch 63/100, Train Loss: 0.6563, Validation Loss: 0.6355\n",
            "Epoch 64/100, Train Loss: 0.6557, Validation Loss: 0.6345\n",
            "Epoch 65/100, Train Loss: 0.6550, Validation Loss: 0.6335\n",
            "Epoch 66/100, Train Loss: 0.6543, Validation Loss: 0.6325\n",
            "Epoch 67/100, Train Loss: 0.6537, Validation Loss: 0.6315\n",
            "Epoch 68/100, Train Loss: 0.6530, Validation Loss: 0.6305\n",
            "Epoch 69/100, Train Loss: 0.6523, Validation Loss: 0.6296\n",
            "Epoch 70/100, Train Loss: 0.6517, Validation Loss: 0.6286\n",
            "Epoch 71/100, Train Loss: 0.6510, Validation Loss: 0.6276\n",
            "Epoch 72/100, Train Loss: 0.6504, Validation Loss: 0.6266\n",
            "Epoch 73/100, Train Loss: 0.6497, Validation Loss: 0.6257\n",
            "Epoch 74/100, Train Loss: 0.6490, Validation Loss: 0.6247\n",
            "Epoch 75/100, Train Loss: 0.6484, Validation Loss: 0.6237\n",
            "Epoch 76/100, Train Loss: 0.6477, Validation Loss: 0.6228\n",
            "Epoch 77/100, Train Loss: 0.6471, Validation Loss: 0.6218\n",
            "Epoch 78/100, Train Loss: 0.6464, Validation Loss: 0.6209\n",
            "Epoch 79/100, Train Loss: 0.6458, Validation Loss: 0.6199\n",
            "Epoch 80/100, Train Loss: 0.6451, Validation Loss: 0.6190\n",
            "Epoch 81/100, Train Loss: 0.6445, Validation Loss: 0.6180\n",
            "Epoch 82/100, Train Loss: 0.6438, Validation Loss: 0.6171\n",
            "Epoch 83/100, Train Loss: 0.6432, Validation Loss: 0.6162\n",
            "Epoch 84/100, Train Loss: 0.6425, Validation Loss: 0.6153\n",
            "Epoch 85/100, Train Loss: 0.6419, Validation Loss: 0.6143\n",
            "Epoch 86/100, Train Loss: 0.6413, Validation Loss: 0.6134\n",
            "Epoch 87/100, Train Loss: 0.6406, Validation Loss: 0.6125\n",
            "Epoch 88/100, Train Loss: 0.6400, Validation Loss: 0.6116\n",
            "Epoch 89/100, Train Loss: 0.6393, Validation Loss: 0.6107\n",
            "Epoch 90/100, Train Loss: 0.6387, Validation Loss: 0.6098\n",
            "Epoch 91/100, Train Loss: 0.6381, Validation Loss: 0.6089\n",
            "Epoch 92/100, Train Loss: 0.6374, Validation Loss: 0.6079\n",
            "Epoch 93/100, Train Loss: 0.6368, Validation Loss: 0.6070\n",
            "Epoch 94/100, Train Loss: 0.6362, Validation Loss: 0.6062\n",
            "Epoch 95/100, Train Loss: 0.6356, Validation Loss: 0.6053\n",
            "Epoch 96/100, Train Loss: 0.6349, Validation Loss: 0.6044\n",
            "Epoch 97/100, Train Loss: 0.6343, Validation Loss: 0.6035\n",
            "Epoch 98/100, Train Loss: 0.6337, Validation Loss: 0.6026\n",
            "Epoch 99/100, Train Loss: 0.6331, Validation Loss: 0.6017\n",
            "Epoch 100/100, Train Loss: 0.6324, Validation Loss: 0.6008\n",
            "Epoch 1/100, Train Loss: 0.6955, Validation Loss: 0.7009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100, Train Loss: 0.6947, Validation Loss: 0.6993\n",
            "Epoch 3/100, Train Loss: 0.6940, Validation Loss: 0.6978\n",
            "Epoch 4/100, Train Loss: 0.6933, Validation Loss: 0.6963\n",
            "Epoch 5/100, Train Loss: 0.6926, Validation Loss: 0.6949\n",
            "Epoch 6/100, Train Loss: 0.6919, Validation Loss: 0.6935\n",
            "Epoch 7/100, Train Loss: 0.6913, Validation Loss: 0.6921\n",
            "Epoch 8/100, Train Loss: 0.6906, Validation Loss: 0.6909\n",
            "Epoch 9/100, Train Loss: 0.6900, Validation Loss: 0.6896\n",
            "Epoch 10/100, Train Loss: 0.6894, Validation Loss: 0.6884\n",
            "Epoch 11/100, Train Loss: 0.6888, Validation Loss: 0.6873\n",
            "Epoch 12/100, Train Loss: 0.6882, Validation Loss: 0.6861\n",
            "Epoch 13/100, Train Loss: 0.6877, Validation Loss: 0.6850\n",
            "Epoch 14/100, Train Loss: 0.6871, Validation Loss: 0.6839\n",
            "Epoch 15/100, Train Loss: 0.6865, Validation Loss: 0.6829\n",
            "Epoch 16/100, Train Loss: 0.6860, Validation Loss: 0.6818\n",
            "Epoch 17/100, Train Loss: 0.6854, Validation Loss: 0.6807\n",
            "Epoch 18/100, Train Loss: 0.6849, Validation Loss: 0.6797\n",
            "Epoch 19/100, Train Loss: 0.6843, Validation Loss: 0.6787\n",
            "Epoch 20/100, Train Loss: 0.6838, Validation Loss: 0.6777\n",
            "Epoch 21/100, Train Loss: 0.6832, Validation Loss: 0.6766\n",
            "Epoch 22/100, Train Loss: 0.6827, Validation Loss: 0.6756\n",
            "Epoch 23/100, Train Loss: 0.6821, Validation Loss: 0.6745\n",
            "Epoch 24/100, Train Loss: 0.6815, Validation Loss: 0.6735\n",
            "Epoch 25/100, Train Loss: 0.6810, Validation Loss: 0.6725\n",
            "Epoch 26/100, Train Loss: 0.6804, Validation Loss: 0.6714\n",
            "Epoch 27/100, Train Loss: 0.6798, Validation Loss: 0.6704\n",
            "Epoch 28/100, Train Loss: 0.6793, Validation Loss: 0.6694\n",
            "Epoch 29/100, Train Loss: 0.6787, Validation Loss: 0.6683\n",
            "Epoch 30/100, Train Loss: 0.6781, Validation Loss: 0.6673\n",
            "Epoch 31/100, Train Loss: 0.6775, Validation Loss: 0.6662\n",
            "Epoch 32/100, Train Loss: 0.6769, Validation Loss: 0.6652\n",
            "Epoch 33/100, Train Loss: 0.6763, Validation Loss: 0.6641\n",
            "Epoch 34/100, Train Loss: 0.6757, Validation Loss: 0.6630\n",
            "Epoch 35/100, Train Loss: 0.6751, Validation Loss: 0.6620\n",
            "Epoch 36/100, Train Loss: 0.6745, Validation Loss: 0.6609\n",
            "Epoch 37/100, Train Loss: 0.6739, Validation Loss: 0.6598\n",
            "Epoch 38/100, Train Loss: 0.6733, Validation Loss: 0.6587\n",
            "Epoch 39/100, Train Loss: 0.6727, Validation Loss: 0.6577\n",
            "Epoch 40/100, Train Loss: 0.6721, Validation Loss: 0.6566\n",
            "Epoch 41/100, Train Loss: 0.6714, Validation Loss: 0.6555\n",
            "Epoch 42/100, Train Loss: 0.6708, Validation Loss: 0.6544\n",
            "Epoch 43/100, Train Loss: 0.6702, Validation Loss: 0.6534\n",
            "Epoch 44/100, Train Loss: 0.6696, Validation Loss: 0.6523\n",
            "Epoch 45/100, Train Loss: 0.6689, Validation Loss: 0.6512\n",
            "Epoch 46/100, Train Loss: 0.6683, Validation Loss: 0.6501\n",
            "Epoch 47/100, Train Loss: 0.6676, Validation Loss: 0.6490\n",
            "Epoch 48/100, Train Loss: 0.6670, Validation Loss: 0.6480\n",
            "Epoch 49/100, Train Loss: 0.6663, Validation Loss: 0.6469\n",
            "Epoch 50/100, Train Loss: 0.6656, Validation Loss: 0.6458\n",
            "Epoch 51/100, Train Loss: 0.6650, Validation Loss: 0.6447\n",
            "Epoch 52/100, Train Loss: 0.6643, Validation Loss: 0.6436\n",
            "Epoch 53/100, Train Loss: 0.6636, Validation Loss: 0.6425\n",
            "Epoch 54/100, Train Loss: 0.6629, Validation Loss: 0.6413\n",
            "Epoch 55/100, Train Loss: 0.6622, Validation Loss: 0.6402\n",
            "Epoch 56/100, Train Loss: 0.6616, Validation Loss: 0.6391\n",
            "Epoch 57/100, Train Loss: 0.6609, Validation Loss: 0.6380\n",
            "Epoch 58/100, Train Loss: 0.6602, Validation Loss: 0.6369\n",
            "Epoch 59/100, Train Loss: 0.6594, Validation Loss: 0.6357\n",
            "Epoch 60/100, Train Loss: 0.6587, Validation Loss: 0.6346\n",
            "Epoch 61/100, Train Loss: 0.6580, Validation Loss: 0.6335\n",
            "Epoch 62/100, Train Loss: 0.6573, Validation Loss: 0.6323\n",
            "Epoch 63/100, Train Loss: 0.6566, Validation Loss: 0.6312\n",
            "Epoch 64/100, Train Loss: 0.6558, Validation Loss: 0.6300\n",
            "Epoch 65/100, Train Loss: 0.6551, Validation Loss: 0.6289\n",
            "Epoch 66/100, Train Loss: 0.6544, Validation Loss: 0.6277\n",
            "Epoch 67/100, Train Loss: 0.6536, Validation Loss: 0.6266\n",
            "Epoch 68/100, Train Loss: 0.6529, Validation Loss: 0.6254\n",
            "Epoch 69/100, Train Loss: 0.6521, Validation Loss: 0.6242\n",
            "Epoch 70/100, Train Loss: 0.6514, Validation Loss: 0.6230\n",
            "Epoch 71/100, Train Loss: 0.6506, Validation Loss: 0.6219\n",
            "Epoch 72/100, Train Loss: 0.6499, Validation Loss: 0.6207\n",
            "Epoch 73/100, Train Loss: 0.6491, Validation Loss: 0.6195\n",
            "Epoch 74/100, Train Loss: 0.6483, Validation Loss: 0.6183\n",
            "Epoch 75/100, Train Loss: 0.6475, Validation Loss: 0.6171\n",
            "Epoch 76/100, Train Loss: 0.6468, Validation Loss: 0.6159\n",
            "Epoch 77/100, Train Loss: 0.6460, Validation Loss: 0.6147\n",
            "Epoch 78/100, Train Loss: 0.6452, Validation Loss: 0.6135\n",
            "Epoch 79/100, Train Loss: 0.6444, Validation Loss: 0.6123\n",
            "Epoch 80/100, Train Loss: 0.6436, Validation Loss: 0.6111\n",
            "Epoch 81/100, Train Loss: 0.6428, Validation Loss: 0.6099\n",
            "Epoch 82/100, Train Loss: 0.6420, Validation Loss: 0.6087\n",
            "Epoch 83/100, Train Loss: 0.6412, Validation Loss: 0.6074\n",
            "Epoch 84/100, Train Loss: 0.6404, Validation Loss: 0.6062\n",
            "Epoch 85/100, Train Loss: 0.6395, Validation Loss: 0.6050\n",
            "Epoch 86/100, Train Loss: 0.6387, Validation Loss: 0.6038\n",
            "Epoch 87/100, Train Loss: 0.6379, Validation Loss: 0.6025\n",
            "Epoch 88/100, Train Loss: 0.6371, Validation Loss: 0.6013\n",
            "Epoch 89/100, Train Loss: 0.6362, Validation Loss: 0.6001\n",
            "Epoch 90/100, Train Loss: 0.6354, Validation Loss: 0.5989\n",
            "Epoch 91/100, Train Loss: 0.6345, Validation Loss: 0.5976\n",
            "Epoch 92/100, Train Loss: 0.6337, Validation Loss: 0.5964\n",
            "Epoch 93/100, Train Loss: 0.6328, Validation Loss: 0.5951\n",
            "Epoch 94/100, Train Loss: 0.6320, Validation Loss: 0.5939\n",
            "Epoch 95/100, Train Loss: 0.6311, Validation Loss: 0.5927\n",
            "Epoch 96/100, Train Loss: 0.6303, Validation Loss: 0.5914\n",
            "Epoch 97/100, Train Loss: 0.6294, Validation Loss: 0.5902\n",
            "Epoch 98/100, Train Loss: 0.6286, Validation Loss: 0.5890\n",
            "Epoch 99/100, Train Loss: 0.6277, Validation Loss: 0.5878\n",
            "Epoch 100/100, Train Loss: 0.6269, Validation Loss: 0.5865\n",
            "Epoch 1/100, Train Loss: 0.6936, Validation Loss: 0.6914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100, Train Loss: 0.6931, Validation Loss: 0.6905\n",
            "Epoch 3/100, Train Loss: 0.6927, Validation Loss: 0.6896\n",
            "Epoch 4/100, Train Loss: 0.6922, Validation Loss: 0.6888\n",
            "Epoch 5/100, Train Loss: 0.6917, Validation Loss: 0.6879\n",
            "Epoch 6/100, Train Loss: 0.6912, Validation Loss: 0.6870\n",
            "Epoch 7/100, Train Loss: 0.6907, Validation Loss: 0.6861\n",
            "Epoch 8/100, Train Loss: 0.6903, Validation Loss: 0.6853\n",
            "Epoch 9/100, Train Loss: 0.6898, Validation Loss: 0.6844\n",
            "Epoch 10/100, Train Loss: 0.6893, Validation Loss: 0.6835\n",
            "Epoch 11/100, Train Loss: 0.6889, Validation Loss: 0.6827\n",
            "Epoch 12/100, Train Loss: 0.6884, Validation Loss: 0.6818\n",
            "Epoch 13/100, Train Loss: 0.6879, Validation Loss: 0.6809\n",
            "Epoch 14/100, Train Loss: 0.6875, Validation Loss: 0.6800\n",
            "Epoch 15/100, Train Loss: 0.6870, Validation Loss: 0.6792\n",
            "Epoch 16/100, Train Loss: 0.6865, Validation Loss: 0.6783\n",
            "Epoch 17/100, Train Loss: 0.6860, Validation Loss: 0.6774\n",
            "Epoch 18/100, Train Loss: 0.6855, Validation Loss: 0.6765\n",
            "Epoch 19/100, Train Loss: 0.6850, Validation Loss: 0.6756\n",
            "Epoch 20/100, Train Loss: 0.6846, Validation Loss: 0.6747\n",
            "Epoch 21/100, Train Loss: 0.6841, Validation Loss: 0.6738\n",
            "Epoch 22/100, Train Loss: 0.6836, Validation Loss: 0.6729\n",
            "Epoch 23/100, Train Loss: 0.6831, Validation Loss: 0.6721\n",
            "Epoch 24/100, Train Loss: 0.6826, Validation Loss: 0.6712\n",
            "Epoch 25/100, Train Loss: 0.6821, Validation Loss: 0.6703\n",
            "Epoch 26/100, Train Loss: 0.6816, Validation Loss: 0.6694\n",
            "Epoch 27/100, Train Loss: 0.6811, Validation Loss: 0.6685\n",
            "Epoch 28/100, Train Loss: 0.6806, Validation Loss: 0.6676\n",
            "Epoch 29/100, Train Loss: 0.6801, Validation Loss: 0.6667\n",
            "Epoch 30/100, Train Loss: 0.6796, Validation Loss: 0.6658\n",
            "Epoch 31/100, Train Loss: 0.6791, Validation Loss: 0.6649\n",
            "Epoch 32/100, Train Loss: 0.6786, Validation Loss: 0.6640\n",
            "Epoch 33/100, Train Loss: 0.6781, Validation Loss: 0.6631\n",
            "Epoch 34/100, Train Loss: 0.6775, Validation Loss: 0.6622\n",
            "Epoch 35/100, Train Loss: 0.6770, Validation Loss: 0.6612\n",
            "Epoch 36/100, Train Loss: 0.6765, Validation Loss: 0.6603\n",
            "Epoch 37/100, Train Loss: 0.6760, Validation Loss: 0.6594\n",
            "Epoch 38/100, Train Loss: 0.6754, Validation Loss: 0.6584\n",
            "Epoch 39/100, Train Loss: 0.6749, Validation Loss: 0.6575\n",
            "Epoch 40/100, Train Loss: 0.6744, Validation Loss: 0.6565\n",
            "Epoch 41/100, Train Loss: 0.6738, Validation Loss: 0.6556\n",
            "Epoch 42/100, Train Loss: 0.6733, Validation Loss: 0.6546\n",
            "Epoch 43/100, Train Loss: 0.6727, Validation Loss: 0.6537\n",
            "Epoch 44/100, Train Loss: 0.6722, Validation Loss: 0.6527\n",
            "Epoch 45/100, Train Loss: 0.6716, Validation Loss: 0.6517\n",
            "Epoch 46/100, Train Loss: 0.6710, Validation Loss: 0.6507\n",
            "Epoch 47/100, Train Loss: 0.6705, Validation Loss: 0.6497\n",
            "Epoch 48/100, Train Loss: 0.6699, Validation Loss: 0.6487\n",
            "Epoch 49/100, Train Loss: 0.6693, Validation Loss: 0.6477\n",
            "Epoch 50/100, Train Loss: 0.6687, Validation Loss: 0.6466\n",
            "Epoch 51/100, Train Loss: 0.6682, Validation Loss: 0.6456\n",
            "Epoch 52/100, Train Loss: 0.6676, Validation Loss: 0.6446\n",
            "Epoch 53/100, Train Loss: 0.6670, Validation Loss: 0.6436\n",
            "Epoch 54/100, Train Loss: 0.6664, Validation Loss: 0.6425\n",
            "Epoch 55/100, Train Loss: 0.6658, Validation Loss: 0.6415\n",
            "Epoch 56/100, Train Loss: 0.6652, Validation Loss: 0.6404\n",
            "Epoch 57/100, Train Loss: 0.6646, Validation Loss: 0.6393\n",
            "Epoch 58/100, Train Loss: 0.6640, Validation Loss: 0.6383\n",
            "Epoch 59/100, Train Loss: 0.6633, Validation Loss: 0.6372\n",
            "Epoch 60/100, Train Loss: 0.6627, Validation Loss: 0.6361\n",
            "Epoch 61/100, Train Loss: 0.6621, Validation Loss: 0.6350\n",
            "Epoch 62/100, Train Loss: 0.6615, Validation Loss: 0.6339\n",
            "Epoch 63/100, Train Loss: 0.6608, Validation Loss: 0.6328\n",
            "Epoch 64/100, Train Loss: 0.6602, Validation Loss: 0.6317\n",
            "Epoch 65/100, Train Loss: 0.6595, Validation Loss: 0.6306\n",
            "Epoch 66/100, Train Loss: 0.6589, Validation Loss: 0.6295\n",
            "Epoch 67/100, Train Loss: 0.6582, Validation Loss: 0.6284\n",
            "Epoch 68/100, Train Loss: 0.6576, Validation Loss: 0.6273\n",
            "Epoch 69/100, Train Loss: 0.6569, Validation Loss: 0.6262\n",
            "Epoch 70/100, Train Loss: 0.6562, Validation Loss: 0.6251\n",
            "Epoch 71/100, Train Loss: 0.6556, Validation Loss: 0.6240\n",
            "Epoch 72/100, Train Loss: 0.6549, Validation Loss: 0.6228\n",
            "Epoch 73/100, Train Loss: 0.6542, Validation Loss: 0.6217\n",
            "Epoch 74/100, Train Loss: 0.6535, Validation Loss: 0.6206\n",
            "Epoch 75/100, Train Loss: 0.6528, Validation Loss: 0.6194\n",
            "Epoch 76/100, Train Loss: 0.6521, Validation Loss: 0.6183\n",
            "Epoch 77/100, Train Loss: 0.6514, Validation Loss: 0.6171\n",
            "Epoch 78/100, Train Loss: 0.6507, Validation Loss: 0.6160\n",
            "Epoch 79/100, Train Loss: 0.6500, Validation Loss: 0.6148\n",
            "Epoch 80/100, Train Loss: 0.6493, Validation Loss: 0.6137\n",
            "Epoch 81/100, Train Loss: 0.6486, Validation Loss: 0.6125\n",
            "Epoch 82/100, Train Loss: 0.6479, Validation Loss: 0.6113\n",
            "Epoch 83/100, Train Loss: 0.6472, Validation Loss: 0.6102\n",
            "Epoch 84/100, Train Loss: 0.6465, Validation Loss: 0.6090\n",
            "Epoch 85/100, Train Loss: 0.6457, Validation Loss: 0.6078\n",
            "Epoch 86/100, Train Loss: 0.6450, Validation Loss: 0.6067\n",
            "Epoch 87/100, Train Loss: 0.6443, Validation Loss: 0.6055\n",
            "Epoch 88/100, Train Loss: 0.6436, Validation Loss: 0.6043\n",
            "Epoch 89/100, Train Loss: 0.6428, Validation Loss: 0.6031\n",
            "Epoch 90/100, Train Loss: 0.6421, Validation Loss: 0.6020\n",
            "Epoch 91/100, Train Loss: 0.6413, Validation Loss: 0.6008\n",
            "Epoch 92/100, Train Loss: 0.6406, Validation Loss: 0.5996\n",
            "Epoch 93/100, Train Loss: 0.6398, Validation Loss: 0.5984\n",
            "Epoch 94/100, Train Loss: 0.6391, Validation Loss: 0.5972\n",
            "Epoch 95/100, Train Loss: 0.6383, Validation Loss: 0.5961\n",
            "Epoch 96/100, Train Loss: 0.6376, Validation Loss: 0.5949\n",
            "Epoch 97/100, Train Loss: 0.6368, Validation Loss: 0.5937\n",
            "Epoch 98/100, Train Loss: 0.6360, Validation Loss: 0.5925\n",
            "Epoch 99/100, Train Loss: 0.6353, Validation Loss: 0.5913\n",
            "Epoch 100/100, Train Loss: 0.6345, Validation Loss: 0.5901\n",
            "Epoch 1/100, Train Loss: 0.6930, Validation Loss: 0.6887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100, Train Loss: 0.6892, Validation Loss: 0.6827\n",
            "Epoch 3/100, Train Loss: 0.6856, Validation Loss: 0.6770\n",
            "Epoch 4/100, Train Loss: 0.6820, Validation Loss: 0.6712\n",
            "Epoch 5/100, Train Loss: 0.6782, Validation Loss: 0.6649\n",
            "Epoch 6/100, Train Loss: 0.6741, Validation Loss: 0.6580\n",
            "Epoch 7/100, Train Loss: 0.6695, Validation Loss: 0.6503\n",
            "Epoch 8/100, Train Loss: 0.6643, Validation Loss: 0.6416\n",
            "Epoch 9/100, Train Loss: 0.6584, Validation Loss: 0.6320\n",
            "Epoch 10/100, Train Loss: 0.6518, Validation Loss: 0.6213\n",
            "Epoch 11/100, Train Loss: 0.6445, Validation Loss: 0.6099\n",
            "Epoch 12/100, Train Loss: 0.6365, Validation Loss: 0.5979\n",
            "Epoch 13/100, Train Loss: 0.6281, Validation Loss: 0.5857\n",
            "Epoch 14/100, Train Loss: 0.6194, Validation Loss: 0.5735\n",
            "Epoch 15/100, Train Loss: 0.6105, Validation Loss: 0.5617\n",
            "Epoch 16/100, Train Loss: 0.6016, Validation Loss: 0.5506\n",
            "Epoch 17/100, Train Loss: 0.5928, Validation Loss: 0.5403\n",
            "Epoch 18/100, Train Loss: 0.5843, Validation Loss: 0.5310\n",
            "Epoch 19/100, Train Loss: 0.5761, Validation Loss: 0.5224\n",
            "Epoch 20/100, Train Loss: 0.5683, Validation Loss: 0.5148\n",
            "Epoch 21/100, Train Loss: 0.5609, Validation Loss: 0.5078\n",
            "Epoch 22/100, Train Loss: 0.5539, Validation Loss: 0.5020\n",
            "Epoch 23/100, Train Loss: 0.5473, Validation Loss: 0.4967\n",
            "Epoch 24/100, Train Loss: 0.5408, Validation Loss: 0.4920\n",
            "Epoch 25/100, Train Loss: 0.5347, Validation Loss: 0.4875\n",
            "Epoch 26/100, Train Loss: 0.5288, Validation Loss: 0.4843\n",
            "Epoch 27/100, Train Loss: 0.5236, Validation Loss: 0.4802\n",
            "Epoch 28/100, Train Loss: 0.5180, Validation Loss: 0.4764\n",
            "Epoch 29/100, Train Loss: 0.5130, Validation Loss: 0.4730\n",
            "Epoch 30/100, Train Loss: 0.5080, Validation Loss: 0.4693\n",
            "Epoch 31/100, Train Loss: 0.5034, Validation Loss: 0.4668\n",
            "Epoch 32/100, Train Loss: 0.4987, Validation Loss: 0.4636\n",
            "Epoch 33/100, Train Loss: 0.4944, Validation Loss: 0.4610\n",
            "Epoch 34/100, Train Loss: 0.4899, Validation Loss: 0.4589\n",
            "Epoch 35/100, Train Loss: 0.4857, Validation Loss: 0.4561\n",
            "Epoch 36/100, Train Loss: 0.4818, Validation Loss: 0.4541\n",
            "Epoch 37/100, Train Loss: 0.4778, Validation Loss: 0.4527\n",
            "Epoch 38/100, Train Loss: 0.4742, Validation Loss: 0.4493\n",
            "Epoch 39/100, Train Loss: 0.4702, Validation Loss: 0.4478\n",
            "Epoch 40/100, Train Loss: 0.4665, Validation Loss: 0.4462\n",
            "Epoch 41/100, Train Loss: 0.4627, Validation Loss: 0.4448\n",
            "Epoch 42/100, Train Loss: 0.4594, Validation Loss: 0.4432\n",
            "Epoch 43/100, Train Loss: 0.4560, Validation Loss: 0.4411\n",
            "Epoch 44/100, Train Loss: 0.4527, Validation Loss: 0.4404\n",
            "Epoch 45/100, Train Loss: 0.4494, Validation Loss: 0.4387\n",
            "Epoch 46/100, Train Loss: 0.4461, Validation Loss: 0.4383\n",
            "Epoch 47/100, Train Loss: 0.4431, Validation Loss: 0.4373\n",
            "Epoch 48/100, Train Loss: 0.4398, Validation Loss: 0.4362\n",
            "Epoch 49/100, Train Loss: 0.4369, Validation Loss: 0.4348\n",
            "Epoch 50/100, Train Loss: 0.4341, Validation Loss: 0.4335\n",
            "Epoch 51/100, Train Loss: 0.4311, Validation Loss: 0.4325\n",
            "Epoch 52/100, Train Loss: 0.4282, Validation Loss: 0.4323\n",
            "Epoch 53/100, Train Loss: 0.4252, Validation Loss: 0.4321\n",
            "Epoch 54/100, Train Loss: 0.4227, Validation Loss: 0.4325\n",
            "Epoch 55/100, Train Loss: 0.4199, Validation Loss: 0.4313\n",
            "Epoch 56/100, Train Loss: 0.4172, Validation Loss: 0.4314\n",
            "Epoch 57/100, Train Loss: 0.4148, Validation Loss: 0.4323\n",
            "Epoch 58/100, Train Loss: 0.4122, Validation Loss: 0.4323\n",
            "Epoch 59/100, Train Loss: 0.4099, Validation Loss: 0.4333\n",
            "Epoch 60/100, Train Loss: 0.4072, Validation Loss: 0.4319\n",
            "Epoch 61/100, Train Loss: 0.4050, Validation Loss: 0.4332\n",
            "Epoch 62/100, Train Loss: 0.4028, Validation Loss: 0.4353\n",
            "Epoch 63/100, Train Loss: 0.4004, Validation Loss: 0.4355\n",
            "Epoch 64/100, Train Loss: 0.3981, Validation Loss: 0.4351\n",
            "Epoch 65/100, Train Loss: 0.3959, Validation Loss: 0.4360\n",
            "Epoch 66/100, Train Loss: 0.3938, Validation Loss: 0.4370\n",
            "Epoch 67/100, Train Loss: 0.3916, Validation Loss: 0.4373\n",
            "Epoch 68/100, Train Loss: 0.3896, Validation Loss: 0.4394\n",
            "Epoch 69/100, Train Loss: 0.3876, Validation Loss: 0.4401\n",
            "Epoch 70/100, Train Loss: 0.3859, Validation Loss: 0.4399\n",
            "Epoch 71/100, Train Loss: 0.3836, Validation Loss: 0.4404\n",
            "Epoch 72/100, Train Loss: 0.3816, Validation Loss: 0.4426\n",
            "Epoch 73/100, Train Loss: 0.3798, Validation Loss: 0.4424\n",
            "Epoch 74/100, Train Loss: 0.3780, Validation Loss: 0.4476\n",
            "Epoch 75/100, Train Loss: 0.3764, Validation Loss: 0.4459\n",
            "Epoch 76/100, Train Loss: 0.3745, Validation Loss: 0.4483\n",
            "Epoch 77/100, Train Loss: 0.3729, Validation Loss: 0.4508\n",
            "Epoch 78/100, Train Loss: 0.3714, Validation Loss: 0.4501\n",
            "Epoch 79/100, Train Loss: 0.3698, Validation Loss: 0.4515\n",
            "Epoch 80/100, Train Loss: 0.3682, Validation Loss: 0.4521\n",
            "Epoch 81/100, Train Loss: 0.3665, Validation Loss: 0.4534\n",
            "Epoch 82/100, Train Loss: 0.3654, Validation Loss: 0.4580\n",
            "Epoch 83/100, Train Loss: 0.3640, Validation Loss: 0.4585\n",
            "Epoch 84/100, Train Loss: 0.3627, Validation Loss: 0.4586\n",
            "Epoch 85/100, Train Loss: 0.3614, Validation Loss: 0.4597\n",
            "Epoch 86/100, Train Loss: 0.3601, Validation Loss: 0.4627\n",
            "Epoch 87/100, Train Loss: 0.3588, Validation Loss: 0.4622\n",
            "Epoch 88/100, Train Loss: 0.3577, Validation Loss: 0.4642\n",
            "Epoch 89/100, Train Loss: 0.3565, Validation Loss: 0.4654\n",
            "Epoch 90/100, Train Loss: 0.3555, Validation Loss: 0.4667\n",
            "Epoch 91/100, Train Loss: 0.3543, Validation Loss: 0.4683\n",
            "Epoch 92/100, Train Loss: 0.3534, Validation Loss: 0.4702\n",
            "Epoch 93/100, Train Loss: 0.3524, Validation Loss: 0.4709\n",
            "Epoch 94/100, Train Loss: 0.3515, Validation Loss: 0.4754\n",
            "Epoch 95/100, Train Loss: 0.3506, Validation Loss: 0.4767\n",
            "Epoch 96/100, Train Loss: 0.3497, Validation Loss: 0.4774\n",
            "Epoch 97/100, Train Loss: 0.3488, Validation Loss: 0.4807\n",
            "Epoch 98/100, Train Loss: 0.3480, Validation Loss: 0.4811\n",
            "Epoch 99/100, Train Loss: 0.3472, Validation Loss: 0.4831\n",
            "Epoch 100/100, Train Loss: 0.3466, Validation Loss: 0.4826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6928, Validation Loss: 0.6910\n",
            "Epoch 2/100, Train Loss: 0.6902, Validation Loss: 0.6860\n",
            "Epoch 3/100, Train Loss: 0.6874, Validation Loss: 0.6806\n",
            "Epoch 4/100, Train Loss: 0.6843, Validation Loss: 0.6748\n",
            "Epoch 5/100, Train Loss: 0.6808, Validation Loss: 0.6684\n",
            "Epoch 6/100, Train Loss: 0.6768, Validation Loss: 0.6612\n",
            "Epoch 7/100, Train Loss: 0.6724, Validation Loss: 0.6534\n",
            "Epoch 8/100, Train Loss: 0.6675, Validation Loss: 0.6449\n",
            "Epoch 9/100, Train Loss: 0.6621, Validation Loss: 0.6359\n",
            "Epoch 10/100, Train Loss: 0.6564, Validation Loss: 0.6264\n",
            "Epoch 11/100, Train Loss: 0.6504, Validation Loss: 0.6168\n",
            "Epoch 12/100, Train Loss: 0.6443, Validation Loss: 0.6073\n",
            "Epoch 13/100, Train Loss: 0.6382, Validation Loss: 0.5973\n",
            "Epoch 14/100, Train Loss: 0.6320, Validation Loss: 0.5877\n",
            "Epoch 15/100, Train Loss: 0.6257, Validation Loss: 0.5778\n",
            "Epoch 16/100, Train Loss: 0.6194, Validation Loss: 0.5682\n",
            "Epoch 17/100, Train Loss: 0.6132, Validation Loss: 0.5592\n",
            "Epoch 18/100, Train Loss: 0.6067, Validation Loss: 0.5493\n",
            "Epoch 19/100, Train Loss: 0.6003, Validation Loss: 0.5395\n",
            "Epoch 20/100, Train Loss: 0.5937, Validation Loss: 0.5312\n",
            "Epoch 21/100, Train Loss: 0.5873, Validation Loss: 0.5223\n",
            "Epoch 22/100, Train Loss: 0.5809, Validation Loss: 0.5132\n",
            "Epoch 23/100, Train Loss: 0.5745, Validation Loss: 0.5053\n",
            "Epoch 24/100, Train Loss: 0.5681, Validation Loss: 0.4975\n",
            "Epoch 25/100, Train Loss: 0.5621, Validation Loss: 0.4902\n",
            "Epoch 26/100, Train Loss: 0.5559, Validation Loss: 0.4826\n",
            "Epoch 27/100, Train Loss: 0.5501, Validation Loss: 0.4786\n",
            "Epoch 28/100, Train Loss: 0.5444, Validation Loss: 0.4718\n",
            "Epoch 29/100, Train Loss: 0.5389, Validation Loss: 0.4659\n",
            "Epoch 30/100, Train Loss: 0.5334, Validation Loss: 0.4603\n",
            "Epoch 31/100, Train Loss: 0.5283, Validation Loss: 0.4551\n",
            "Epoch 32/100, Train Loss: 0.5231, Validation Loss: 0.4514\n",
            "Epoch 33/100, Train Loss: 0.5181, Validation Loss: 0.4499\n",
            "Epoch 34/100, Train Loss: 0.5134, Validation Loss: 0.4442\n",
            "Epoch 35/100, Train Loss: 0.5084, Validation Loss: 0.4390\n",
            "Epoch 36/100, Train Loss: 0.5040, Validation Loss: 0.4366\n",
            "Epoch 37/100, Train Loss: 0.4996, Validation Loss: 0.4345\n",
            "Epoch 38/100, Train Loss: 0.4951, Validation Loss: 0.4318\n",
            "Epoch 39/100, Train Loss: 0.4909, Validation Loss: 0.4285\n",
            "Epoch 40/100, Train Loss: 0.4866, Validation Loss: 0.4281\n",
            "Epoch 41/100, Train Loss: 0.4825, Validation Loss: 0.4270\n",
            "Epoch 42/100, Train Loss: 0.4788, Validation Loss: 0.4248\n",
            "Epoch 43/100, Train Loss: 0.4748, Validation Loss: 0.4231\n",
            "Epoch 44/100, Train Loss: 0.4710, Validation Loss: 0.4215\n",
            "Epoch 45/100, Train Loss: 0.4672, Validation Loss: 0.4197\n",
            "Epoch 46/100, Train Loss: 0.4637, Validation Loss: 0.4198\n",
            "Epoch 47/100, Train Loss: 0.4601, Validation Loss: 0.4209\n",
            "Epoch 48/100, Train Loss: 0.4569, Validation Loss: 0.4186\n",
            "Epoch 49/100, Train Loss: 0.4535, Validation Loss: 0.4209\n",
            "Epoch 50/100, Train Loss: 0.4502, Validation Loss: 0.4188\n",
            "Epoch 51/100, Train Loss: 0.4471, Validation Loss: 0.4198\n",
            "Epoch 52/100, Train Loss: 0.4439, Validation Loss: 0.4201\n",
            "Epoch 53/100, Train Loss: 0.4409, Validation Loss: 0.4213\n",
            "Epoch 54/100, Train Loss: 0.4380, Validation Loss: 0.4207\n",
            "Epoch 55/100, Train Loss: 0.4351, Validation Loss: 0.4238\n",
            "Epoch 56/100, Train Loss: 0.4323, Validation Loss: 0.4243\n",
            "Epoch 57/100, Train Loss: 0.4293, Validation Loss: 0.4244\n",
            "Epoch 58/100, Train Loss: 0.4269, Validation Loss: 0.4279\n",
            "Epoch 59/100, Train Loss: 0.4243, Validation Loss: 0.4279\n",
            "Epoch 60/100, Train Loss: 0.4218, Validation Loss: 0.4283\n",
            "Epoch 61/100, Train Loss: 0.4193, Validation Loss: 0.4293\n",
            "Epoch 62/100, Train Loss: 0.4167, Validation Loss: 0.4315\n",
            "Epoch 63/100, Train Loss: 0.4140, Validation Loss: 0.4327\n",
            "Epoch 64/100, Train Loss: 0.4116, Validation Loss: 0.4372\n",
            "Epoch 65/100, Train Loss: 0.4095, Validation Loss: 0.4355\n",
            "Epoch 66/100, Train Loss: 0.4070, Validation Loss: 0.4372\n",
            "Epoch 67/100, Train Loss: 0.4048, Validation Loss: 0.4401\n",
            "Epoch 68/100, Train Loss: 0.4028, Validation Loss: 0.4392\n",
            "Epoch 69/100, Train Loss: 0.4004, Validation Loss: 0.4423\n",
            "Epoch 70/100, Train Loss: 0.3985, Validation Loss: 0.4423\n",
            "Epoch 71/100, Train Loss: 0.3965, Validation Loss: 0.4436\n",
            "Epoch 72/100, Train Loss: 0.3944, Validation Loss: 0.4447\n",
            "Epoch 73/100, Train Loss: 0.3923, Validation Loss: 0.4518\n",
            "Epoch 74/100, Train Loss: 0.3905, Validation Loss: 0.4490\n",
            "Epoch 75/100, Train Loss: 0.3885, Validation Loss: 0.4539\n",
            "Epoch 76/100, Train Loss: 0.3869, Validation Loss: 0.4532\n",
            "Epoch 77/100, Train Loss: 0.3849, Validation Loss: 0.4541\n",
            "Epoch 78/100, Train Loss: 0.3832, Validation Loss: 0.4555\n",
            "Epoch 79/100, Train Loss: 0.3815, Validation Loss: 0.4586\n",
            "Epoch 80/100, Train Loss: 0.3795, Validation Loss: 0.4645\n",
            "Epoch 81/100, Train Loss: 0.3782, Validation Loss: 0.4647\n",
            "Epoch 82/100, Train Loss: 0.3764, Validation Loss: 0.4667\n",
            "Epoch 83/100, Train Loss: 0.3750, Validation Loss: 0.4686\n",
            "Epoch 84/100, Train Loss: 0.3732, Validation Loss: 0.4695\n",
            "Epoch 85/100, Train Loss: 0.3722, Validation Loss: 0.4715\n",
            "Epoch 86/100, Train Loss: 0.3705, Validation Loss: 0.4757\n",
            "Epoch 87/100, Train Loss: 0.3691, Validation Loss: 0.4778\n",
            "Epoch 88/100, Train Loss: 0.3680, Validation Loss: 0.4785\n",
            "Epoch 89/100, Train Loss: 0.3664, Validation Loss: 0.4821\n",
            "Epoch 90/100, Train Loss: 0.3652, Validation Loss: 0.4839\n",
            "Epoch 91/100, Train Loss: 0.3640, Validation Loss: 0.4922\n",
            "Epoch 92/100, Train Loss: 0.3629, Validation Loss: 0.4897\n",
            "Epoch 93/100, Train Loss: 0.3617, Validation Loss: 0.4925\n",
            "Epoch 94/100, Train Loss: 0.3605, Validation Loss: 0.4930\n",
            "Epoch 95/100, Train Loss: 0.3592, Validation Loss: 0.4984\n",
            "Epoch 96/100, Train Loss: 0.3583, Validation Loss: 0.5026\n",
            "Epoch 97/100, Train Loss: 0.3572, Validation Loss: 0.5020\n",
            "Epoch 98/100, Train Loss: 0.3562, Validation Loss: 0.5041\n",
            "Epoch 99/100, Train Loss: 0.3552, Validation Loss: 0.5044\n",
            "Epoch 100/100, Train Loss: 0.3542, Validation Loss: 0.5089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6915, Validation Loss: 0.6854\n",
            "Epoch 2/100, Train Loss: 0.6876, Validation Loss: 0.6783\n",
            "Epoch 3/100, Train Loss: 0.6838, Validation Loss: 0.6713\n",
            "Epoch 4/100, Train Loss: 0.6800, Validation Loss: 0.6642\n",
            "Epoch 5/100, Train Loss: 0.6758, Validation Loss: 0.6564\n",
            "Epoch 6/100, Train Loss: 0.6712, Validation Loss: 0.6479\n",
            "Epoch 7/100, Train Loss: 0.6662, Validation Loss: 0.6382\n",
            "Epoch 8/100, Train Loss: 0.6604, Validation Loss: 0.6275\n",
            "Epoch 9/100, Train Loss: 0.6540, Validation Loss: 0.6155\n",
            "Epoch 10/100, Train Loss: 0.6468, Validation Loss: 0.6027\n",
            "Epoch 11/100, Train Loss: 0.6391, Validation Loss: 0.5889\n",
            "Epoch 12/100, Train Loss: 0.6307, Validation Loss: 0.5747\n",
            "Epoch 13/100, Train Loss: 0.6220, Validation Loss: 0.5603\n",
            "Epoch 14/100, Train Loss: 0.6130, Validation Loss: 0.5460\n",
            "Epoch 15/100, Train Loss: 0.6042, Validation Loss: 0.5323\n",
            "Epoch 16/100, Train Loss: 0.5954, Validation Loss: 0.5189\n",
            "Epoch 17/100, Train Loss: 0.5868, Validation Loss: 0.5067\n",
            "Epoch 18/100, Train Loss: 0.5787, Validation Loss: 0.4947\n",
            "Epoch 19/100, Train Loss: 0.5708, Validation Loss: 0.4840\n",
            "Epoch 20/100, Train Loss: 0.5633, Validation Loss: 0.4741\n",
            "Epoch 21/100, Train Loss: 0.5561, Validation Loss: 0.4646\n",
            "Epoch 22/100, Train Loss: 0.5491, Validation Loss: 0.4565\n",
            "Epoch 23/100, Train Loss: 0.5426, Validation Loss: 0.4478\n",
            "Epoch 24/100, Train Loss: 0.5362, Validation Loss: 0.4404\n",
            "Epoch 25/100, Train Loss: 0.5299, Validation Loss: 0.4331\n",
            "Epoch 26/100, Train Loss: 0.5241, Validation Loss: 0.4264\n",
            "Epoch 27/100, Train Loss: 0.5184, Validation Loss: 0.4201\n",
            "Epoch 28/100, Train Loss: 0.5128, Validation Loss: 0.4143\n",
            "Epoch 29/100, Train Loss: 0.5073, Validation Loss: 0.4089\n",
            "Epoch 30/100, Train Loss: 0.5021, Validation Loss: 0.4039\n",
            "Epoch 31/100, Train Loss: 0.4970, Validation Loss: 0.3991\n",
            "Epoch 32/100, Train Loss: 0.4921, Validation Loss: 0.3950\n",
            "Epoch 33/100, Train Loss: 0.4872, Validation Loss: 0.3909\n",
            "Epoch 34/100, Train Loss: 0.4826, Validation Loss: 0.3866\n",
            "Epoch 35/100, Train Loss: 0.4781, Validation Loss: 0.3833\n",
            "Epoch 36/100, Train Loss: 0.4737, Validation Loss: 0.3800\n",
            "Epoch 37/100, Train Loss: 0.4695, Validation Loss: 0.3765\n",
            "Epoch 38/100, Train Loss: 0.4650, Validation Loss: 0.3743\n",
            "Epoch 39/100, Train Loss: 0.4612, Validation Loss: 0.3718\n",
            "Epoch 40/100, Train Loss: 0.4571, Validation Loss: 0.3711\n",
            "Epoch 41/100, Train Loss: 0.4533, Validation Loss: 0.3681\n",
            "Epoch 42/100, Train Loss: 0.4496, Validation Loss: 0.3643\n",
            "Epoch 43/100, Train Loss: 0.4456, Validation Loss: 0.3649\n",
            "Epoch 44/100, Train Loss: 0.4422, Validation Loss: 0.3643\n",
            "Epoch 45/100, Train Loss: 0.4387, Validation Loss: 0.3614\n",
            "Epoch 46/100, Train Loss: 0.4355, Validation Loss: 0.3609\n",
            "Epoch 47/100, Train Loss: 0.4319, Validation Loss: 0.3608\n",
            "Epoch 48/100, Train Loss: 0.4289, Validation Loss: 0.3592\n",
            "Epoch 49/100, Train Loss: 0.4256, Validation Loss: 0.3600\n",
            "Epoch 50/100, Train Loss: 0.4224, Validation Loss: 0.3600\n",
            "Epoch 51/100, Train Loss: 0.4194, Validation Loss: 0.3605\n",
            "Epoch 52/100, Train Loss: 0.4164, Validation Loss: 0.3642\n",
            "Epoch 53/100, Train Loss: 0.4135, Validation Loss: 0.3586\n",
            "Epoch 54/100, Train Loss: 0.4107, Validation Loss: 0.3603\n",
            "Epoch 55/100, Train Loss: 0.4083, Validation Loss: 0.3606\n",
            "Epoch 56/100, Train Loss: 0.4054, Validation Loss: 0.3620\n",
            "Epoch 57/100, Train Loss: 0.4026, Validation Loss: 0.3642\n",
            "Epoch 58/100, Train Loss: 0.4001, Validation Loss: 0.3620\n",
            "Epoch 59/100, Train Loss: 0.3975, Validation Loss: 0.3669\n",
            "Epoch 60/100, Train Loss: 0.3955, Validation Loss: 0.3643\n",
            "Epoch 61/100, Train Loss: 0.3928, Validation Loss: 0.3693\n",
            "Epoch 62/100, Train Loss: 0.3906, Validation Loss: 0.3696\n",
            "Epoch 63/100, Train Loss: 0.3885, Validation Loss: 0.3700\n",
            "Epoch 64/100, Train Loss: 0.3862, Validation Loss: 0.3722\n",
            "Epoch 65/100, Train Loss: 0.3842, Validation Loss: 0.3714\n",
            "Epoch 66/100, Train Loss: 0.3820, Validation Loss: 0.3750\n",
            "Epoch 67/100, Train Loss: 0.3802, Validation Loss: 0.3764\n",
            "Epoch 68/100, Train Loss: 0.3781, Validation Loss: 0.3778\n",
            "Epoch 69/100, Train Loss: 0.3762, Validation Loss: 0.3821\n",
            "Epoch 70/100, Train Loss: 0.3741, Validation Loss: 0.3804\n",
            "Epoch 71/100, Train Loss: 0.3727, Validation Loss: 0.3838\n",
            "Epoch 72/100, Train Loss: 0.3709, Validation Loss: 0.3832\n",
            "Epoch 73/100, Train Loss: 0.3692, Validation Loss: 0.3846\n",
            "Epoch 74/100, Train Loss: 0.3676, Validation Loss: 0.3853\n",
            "Epoch 75/100, Train Loss: 0.3660, Validation Loss: 0.3922\n",
            "Epoch 76/100, Train Loss: 0.3644, Validation Loss: 0.3931\n",
            "Epoch 77/100, Train Loss: 0.3629, Validation Loss: 0.3931\n",
            "Epoch 78/100, Train Loss: 0.3616, Validation Loss: 0.3983\n",
            "Epoch 79/100, Train Loss: 0.3602, Validation Loss: 0.4016\n",
            "Epoch 80/100, Train Loss: 0.3590, Validation Loss: 0.3975\n",
            "Epoch 81/100, Train Loss: 0.3578, Validation Loss: 0.4043\n",
            "Epoch 82/100, Train Loss: 0.3567, Validation Loss: 0.4013\n",
            "Epoch 83/100, Train Loss: 0.3553, Validation Loss: 0.4068\n",
            "Epoch 84/100, Train Loss: 0.3543, Validation Loss: 0.4115\n",
            "Epoch 85/100, Train Loss: 0.3532, Validation Loss: 0.4140\n",
            "Epoch 86/100, Train Loss: 0.3523, Validation Loss: 0.4128\n",
            "Epoch 87/100, Train Loss: 0.3512, Validation Loss: 0.4130\n",
            "Epoch 88/100, Train Loss: 0.3503, Validation Loss: 0.4133\n",
            "Epoch 89/100, Train Loss: 0.3493, Validation Loss: 0.4182\n",
            "Epoch 90/100, Train Loss: 0.3484, Validation Loss: 0.4199\n",
            "Epoch 91/100, Train Loss: 0.3477, Validation Loss: 0.4224\n",
            "Epoch 92/100, Train Loss: 0.3468, Validation Loss: 0.4232\n",
            "Epoch 93/100, Train Loss: 0.3461, Validation Loss: 0.4241\n",
            "Epoch 94/100, Train Loss: 0.3451, Validation Loss: 0.4328\n",
            "Epoch 95/100, Train Loss: 0.3445, Validation Loss: 0.4266\n",
            "Epoch 96/100, Train Loss: 0.3439, Validation Loss: 0.4313\n",
            "Epoch 97/100, Train Loss: 0.3430, Validation Loss: 0.4350\n",
            "Epoch 98/100, Train Loss: 0.3424, Validation Loss: 0.4326\n",
            "Epoch 99/100, Train Loss: 0.3417, Validation Loss: 0.4377\n",
            "Epoch 100/100, Train Loss: 0.3410, Validation Loss: 0.4407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6898, Validation Loss: 0.6843\n",
            "Epoch 2/100, Train Loss: 0.6852, Validation Loss: 0.6761\n",
            "Epoch 3/100, Train Loss: 0.6806, Validation Loss: 0.6678\n",
            "Epoch 4/100, Train Loss: 0.6759, Validation Loss: 0.6592\n",
            "Epoch 5/100, Train Loss: 0.6707, Validation Loss: 0.6498\n",
            "Epoch 6/100, Train Loss: 0.6650, Validation Loss: 0.6398\n",
            "Epoch 7/100, Train Loss: 0.6587, Validation Loss: 0.6288\n",
            "Epoch 8/100, Train Loss: 0.6520, Validation Loss: 0.6174\n",
            "Epoch 9/100, Train Loss: 0.6446, Validation Loss: 0.6054\n",
            "Epoch 10/100, Train Loss: 0.6369, Validation Loss: 0.5932\n",
            "Epoch 11/100, Train Loss: 0.6289, Validation Loss: 0.5813\n",
            "Epoch 12/100, Train Loss: 0.6208, Validation Loss: 0.5694\n",
            "Epoch 13/100, Train Loss: 0.6125, Validation Loss: 0.5569\n",
            "Epoch 14/100, Train Loss: 0.6044, Validation Loss: 0.5465\n",
            "Epoch 15/100, Train Loss: 0.5962, Validation Loss: 0.5365\n",
            "Epoch 16/100, Train Loss: 0.5883, Validation Loss: 0.5270\n",
            "Epoch 17/100, Train Loss: 0.5806, Validation Loss: 0.5179\n",
            "Epoch 18/100, Train Loss: 0.5729, Validation Loss: 0.5099\n",
            "Epoch 19/100, Train Loss: 0.5656, Validation Loss: 0.5021\n",
            "Epoch 20/100, Train Loss: 0.5586, Validation Loss: 0.4957\n",
            "Epoch 21/100, Train Loss: 0.5519, Validation Loss: 0.4905\n",
            "Epoch 22/100, Train Loss: 0.5456, Validation Loss: 0.4822\n",
            "Epoch 23/100, Train Loss: 0.5395, Validation Loss: 0.4795\n",
            "Epoch 24/100, Train Loss: 0.5334, Validation Loss: 0.4745\n",
            "Epoch 25/100, Train Loss: 0.5277, Validation Loss: 0.4721\n",
            "Epoch 26/100, Train Loss: 0.5222, Validation Loss: 0.4664\n",
            "Epoch 27/100, Train Loss: 0.5169, Validation Loss: 0.4661\n",
            "Epoch 28/100, Train Loss: 0.5120, Validation Loss: 0.4597\n",
            "Epoch 29/100, Train Loss: 0.5069, Validation Loss: 0.4577\n",
            "Epoch 30/100, Train Loss: 0.5021, Validation Loss: 0.4520\n",
            "Epoch 31/100, Train Loss: 0.4976, Validation Loss: 0.4494\n",
            "Epoch 32/100, Train Loss: 0.4931, Validation Loss: 0.4490\n",
            "Epoch 33/100, Train Loss: 0.4890, Validation Loss: 0.4465\n",
            "Epoch 34/100, Train Loss: 0.4847, Validation Loss: 0.4456\n",
            "Epoch 35/100, Train Loss: 0.4807, Validation Loss: 0.4448\n",
            "Epoch 36/100, Train Loss: 0.4767, Validation Loss: 0.4418\n",
            "Epoch 37/100, Train Loss: 0.4728, Validation Loss: 0.4377\n",
            "Epoch 38/100, Train Loss: 0.4690, Validation Loss: 0.4385\n",
            "Epoch 39/100, Train Loss: 0.4655, Validation Loss: 0.4349\n",
            "Epoch 40/100, Train Loss: 0.4618, Validation Loss: 0.4353\n",
            "Epoch 41/100, Train Loss: 0.4585, Validation Loss: 0.4344\n",
            "Epoch 42/100, Train Loss: 0.4551, Validation Loss: 0.4357\n",
            "Epoch 43/100, Train Loss: 0.4516, Validation Loss: 0.4378\n",
            "Epoch 44/100, Train Loss: 0.4483, Validation Loss: 0.4353\n",
            "Epoch 45/100, Train Loss: 0.4453, Validation Loss: 0.4324\n",
            "Epoch 46/100, Train Loss: 0.4421, Validation Loss: 0.4309\n",
            "Epoch 47/100, Train Loss: 0.4388, Validation Loss: 0.4275\n",
            "Epoch 48/100, Train Loss: 0.4359, Validation Loss: 0.4331\n",
            "Epoch 49/100, Train Loss: 0.4330, Validation Loss: 0.4312\n",
            "Epoch 50/100, Train Loss: 0.4300, Validation Loss: 0.4339\n",
            "Epoch 51/100, Train Loss: 0.4273, Validation Loss: 0.4346\n",
            "Epoch 52/100, Train Loss: 0.4246, Validation Loss: 0.4264\n",
            "Epoch 53/100, Train Loss: 0.4220, Validation Loss: 0.4298\n",
            "Epoch 54/100, Train Loss: 0.4194, Validation Loss: 0.4285\n",
            "Epoch 55/100, Train Loss: 0.4165, Validation Loss: 0.4310\n",
            "Epoch 56/100, Train Loss: 0.4140, Validation Loss: 0.4363\n",
            "Epoch 57/100, Train Loss: 0.4119, Validation Loss: 0.4348\n",
            "Epoch 58/100, Train Loss: 0.4091, Validation Loss: 0.4275\n",
            "Epoch 59/100, Train Loss: 0.4070, Validation Loss: 0.4299\n",
            "Epoch 60/100, Train Loss: 0.4047, Validation Loss: 0.4360\n",
            "Epoch 61/100, Train Loss: 0.4021, Validation Loss: 0.4249\n",
            "Epoch 62/100, Train Loss: 0.4001, Validation Loss: 0.4354\n",
            "Epoch 63/100, Train Loss: 0.3980, Validation Loss: 0.4361\n",
            "Epoch 64/100, Train Loss: 0.3956, Validation Loss: 0.4378\n",
            "Epoch 65/100, Train Loss: 0.3935, Validation Loss: 0.4345\n",
            "Epoch 66/100, Train Loss: 0.3914, Validation Loss: 0.4306\n",
            "Epoch 67/100, Train Loss: 0.3896, Validation Loss: 0.4366\n",
            "Epoch 68/100, Train Loss: 0.3877, Validation Loss: 0.4416\n",
            "Epoch 69/100, Train Loss: 0.3857, Validation Loss: 0.4453\n",
            "Epoch 70/100, Train Loss: 0.3838, Validation Loss: 0.4399\n",
            "Epoch 71/100, Train Loss: 0.3819, Validation Loss: 0.4408\n",
            "Epoch 72/100, Train Loss: 0.3802, Validation Loss: 0.4401\n",
            "Epoch 73/100, Train Loss: 0.3787, Validation Loss: 0.4446\n",
            "Epoch 74/100, Train Loss: 0.3765, Validation Loss: 0.4521\n",
            "Epoch 75/100, Train Loss: 0.3749, Validation Loss: 0.4494\n",
            "Epoch 76/100, Train Loss: 0.3732, Validation Loss: 0.4446\n",
            "Epoch 77/100, Train Loss: 0.3716, Validation Loss: 0.4512\n",
            "Epoch 78/100, Train Loss: 0.3702, Validation Loss: 0.4467\n",
            "Epoch 79/100, Train Loss: 0.3687, Validation Loss: 0.4575\n",
            "Epoch 80/100, Train Loss: 0.3671, Validation Loss: 0.4591\n",
            "Epoch 81/100, Train Loss: 0.3656, Validation Loss: 0.4598\n",
            "Epoch 82/100, Train Loss: 0.3643, Validation Loss: 0.4529\n",
            "Epoch 83/100, Train Loss: 0.3629, Validation Loss: 0.4607\n",
            "Epoch 84/100, Train Loss: 0.3616, Validation Loss: 0.4630\n",
            "Epoch 85/100, Train Loss: 0.3602, Validation Loss: 0.4666\n",
            "Epoch 86/100, Train Loss: 0.3589, Validation Loss: 0.4597\n",
            "Epoch 87/100, Train Loss: 0.3577, Validation Loss: 0.4640\n",
            "Epoch 88/100, Train Loss: 0.3566, Validation Loss: 0.4643\n",
            "Epoch 89/100, Train Loss: 0.3554, Validation Loss: 0.4717\n",
            "Epoch 90/100, Train Loss: 0.3543, Validation Loss: 0.4683\n",
            "Epoch 91/100, Train Loss: 0.3534, Validation Loss: 0.4708\n",
            "Epoch 92/100, Train Loss: 0.3523, Validation Loss: 0.4708\n",
            "Epoch 93/100, Train Loss: 0.3511, Validation Loss: 0.4712\n",
            "Epoch 94/100, Train Loss: 0.3503, Validation Loss: 0.4853\n",
            "Epoch 95/100, Train Loss: 0.3494, Validation Loss: 0.4734\n",
            "Epoch 96/100, Train Loss: 0.3485, Validation Loss: 0.4869\n",
            "Epoch 97/100, Train Loss: 0.3478, Validation Loss: 0.4828\n",
            "Epoch 98/100, Train Loss: 0.3469, Validation Loss: 0.4875\n",
            "Epoch 99/100, Train Loss: 0.3463, Validation Loss: 0.4872\n",
            "Epoch 100/100, Train Loss: 0.3455, Validation Loss: 0.4888\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6910, Validation Loss: 0.6881\n",
            "Epoch 2/100, Train Loss: 0.6854, Validation Loss: 0.6783\n",
            "Epoch 3/100, Train Loss: 0.6796, Validation Loss: 0.6683\n",
            "Epoch 4/100, Train Loss: 0.6734, Validation Loss: 0.6578\n",
            "Epoch 5/100, Train Loss: 0.6667, Validation Loss: 0.6467\n",
            "Epoch 6/100, Train Loss: 0.6596, Validation Loss: 0.6350\n",
            "Epoch 7/100, Train Loss: 0.6520, Validation Loss: 0.6228\n",
            "Epoch 8/100, Train Loss: 0.6441, Validation Loss: 0.6104\n",
            "Epoch 9/100, Train Loss: 0.6359, Validation Loss: 0.5981\n",
            "Epoch 10/100, Train Loss: 0.6276, Validation Loss: 0.5856\n",
            "Epoch 11/100, Train Loss: 0.6192, Validation Loss: 0.5734\n",
            "Epoch 12/100, Train Loss: 0.6109, Validation Loss: 0.5615\n",
            "Epoch 13/100, Train Loss: 0.6025, Validation Loss: 0.5502\n",
            "Epoch 14/100, Train Loss: 0.5943, Validation Loss: 0.5394\n",
            "Epoch 15/100, Train Loss: 0.5862, Validation Loss: 0.5292\n",
            "Epoch 16/100, Train Loss: 0.5785, Validation Loss: 0.5196\n",
            "Epoch 17/100, Train Loss: 0.5706, Validation Loss: 0.5109\n",
            "Epoch 18/100, Train Loss: 0.5631, Validation Loss: 0.5028\n",
            "Epoch 19/100, Train Loss: 0.5559, Validation Loss: 0.4950\n",
            "Epoch 20/100, Train Loss: 0.5490, Validation Loss: 0.4878\n",
            "Epoch 21/100, Train Loss: 0.5423, Validation Loss: 0.4813\n",
            "Epoch 22/100, Train Loss: 0.5358, Validation Loss: 0.4759\n",
            "Epoch 23/100, Train Loss: 0.5297, Validation Loss: 0.4698\n",
            "Epoch 24/100, Train Loss: 0.5240, Validation Loss: 0.4646\n",
            "Epoch 25/100, Train Loss: 0.5181, Validation Loss: 0.4597\n",
            "Epoch 26/100, Train Loss: 0.5125, Validation Loss: 0.4552\n",
            "Epoch 27/100, Train Loss: 0.5073, Validation Loss: 0.4515\n",
            "Epoch 28/100, Train Loss: 0.5021, Validation Loss: 0.4475\n",
            "Epoch 29/100, Train Loss: 0.4970, Validation Loss: 0.4444\n",
            "Epoch 30/100, Train Loss: 0.4921, Validation Loss: 0.4418\n",
            "Epoch 31/100, Train Loss: 0.4875, Validation Loss: 0.4395\n",
            "Epoch 32/100, Train Loss: 0.4827, Validation Loss: 0.4371\n",
            "Epoch 33/100, Train Loss: 0.4784, Validation Loss: 0.4345\n",
            "Epoch 34/100, Train Loss: 0.4741, Validation Loss: 0.4332\n",
            "Epoch 35/100, Train Loss: 0.4695, Validation Loss: 0.4326\n",
            "Epoch 36/100, Train Loss: 0.4655, Validation Loss: 0.4321\n",
            "Epoch 37/100, Train Loss: 0.4617, Validation Loss: 0.4299\n",
            "Epoch 38/100, Train Loss: 0.4578, Validation Loss: 0.4282\n",
            "Epoch 39/100, Train Loss: 0.4538, Validation Loss: 0.4271\n",
            "Epoch 40/100, Train Loss: 0.4500, Validation Loss: 0.4266\n",
            "Epoch 41/100, Train Loss: 0.4460, Validation Loss: 0.4291\n",
            "Epoch 42/100, Train Loss: 0.4429, Validation Loss: 0.4258\n",
            "Epoch 43/100, Train Loss: 0.4397, Validation Loss: 0.4267\n",
            "Epoch 44/100, Train Loss: 0.4361, Validation Loss: 0.4277\n",
            "Epoch 45/100, Train Loss: 0.4328, Validation Loss: 0.4262\n",
            "Epoch 46/100, Train Loss: 0.4297, Validation Loss: 0.4261\n",
            "Epoch 47/100, Train Loss: 0.4268, Validation Loss: 0.4255\n",
            "Epoch 48/100, Train Loss: 0.4238, Validation Loss: 0.4253\n",
            "Epoch 49/100, Train Loss: 0.4210, Validation Loss: 0.4269\n",
            "Epoch 50/100, Train Loss: 0.4179, Validation Loss: 0.4275\n",
            "Epoch 51/100, Train Loss: 0.4152, Validation Loss: 0.4289\n",
            "Epoch 52/100, Train Loss: 0.4123, Validation Loss: 0.4305\n",
            "Epoch 53/100, Train Loss: 0.4101, Validation Loss: 0.4311\n",
            "Epoch 54/100, Train Loss: 0.4074, Validation Loss: 0.4307\n",
            "Epoch 55/100, Train Loss: 0.4046, Validation Loss: 0.4356\n",
            "Epoch 56/100, Train Loss: 0.4023, Validation Loss: 0.4348\n",
            "Epoch 57/100, Train Loss: 0.3998, Validation Loss: 0.4380\n",
            "Epoch 58/100, Train Loss: 0.3977, Validation Loss: 0.4367\n",
            "Epoch 59/100, Train Loss: 0.3954, Validation Loss: 0.4385\n",
            "Epoch 60/100, Train Loss: 0.3930, Validation Loss: 0.4418\n",
            "Epoch 61/100, Train Loss: 0.3908, Validation Loss: 0.4416\n",
            "Epoch 62/100, Train Loss: 0.3887, Validation Loss: 0.4450\n",
            "Epoch 63/100, Train Loss: 0.3868, Validation Loss: 0.4468\n",
            "Epoch 64/100, Train Loss: 0.3847, Validation Loss: 0.4483\n",
            "Epoch 65/100, Train Loss: 0.3828, Validation Loss: 0.4510\n",
            "Epoch 66/100, Train Loss: 0.3808, Validation Loss: 0.4520\n",
            "Epoch 67/100, Train Loss: 0.3790, Validation Loss: 0.4555\n",
            "Epoch 68/100, Train Loss: 0.3773, Validation Loss: 0.4570\n",
            "Epoch 69/100, Train Loss: 0.3752, Validation Loss: 0.4603\n",
            "Epoch 70/100, Train Loss: 0.3735, Validation Loss: 0.4648\n",
            "Epoch 71/100, Train Loss: 0.3722, Validation Loss: 0.4663\n",
            "Epoch 72/100, Train Loss: 0.3705, Validation Loss: 0.4701\n",
            "Epoch 73/100, Train Loss: 0.3689, Validation Loss: 0.4719\n",
            "Epoch 74/100, Train Loss: 0.3677, Validation Loss: 0.4739\n",
            "Epoch 75/100, Train Loss: 0.3661, Validation Loss: 0.4779\n",
            "Epoch 76/100, Train Loss: 0.3648, Validation Loss: 0.4803\n",
            "Epoch 77/100, Train Loss: 0.3635, Validation Loss: 0.4839\n",
            "Epoch 78/100, Train Loss: 0.3623, Validation Loss: 0.4861\n",
            "Epoch 79/100, Train Loss: 0.3612, Validation Loss: 0.4883\n",
            "Epoch 80/100, Train Loss: 0.3595, Validation Loss: 0.4908\n",
            "Epoch 81/100, Train Loss: 0.3584, Validation Loss: 0.4949\n",
            "Epoch 82/100, Train Loss: 0.3573, Validation Loss: 0.4969\n",
            "Epoch 83/100, Train Loss: 0.3561, Validation Loss: 0.5009\n",
            "Epoch 84/100, Train Loss: 0.3552, Validation Loss: 0.5031\n",
            "Epoch 85/100, Train Loss: 0.3539, Validation Loss: 0.5030\n",
            "Epoch 86/100, Train Loss: 0.3529, Validation Loss: 0.5082\n",
            "Epoch 87/100, Train Loss: 0.3520, Validation Loss: 0.5092\n",
            "Epoch 88/100, Train Loss: 0.3510, Validation Loss: 0.5106\n",
            "Epoch 89/100, Train Loss: 0.3499, Validation Loss: 0.5130\n",
            "Epoch 90/100, Train Loss: 0.3491, Validation Loss: 0.5153\n",
            "Epoch 91/100, Train Loss: 0.3482, Validation Loss: 0.5184\n",
            "Epoch 92/100, Train Loss: 0.3474, Validation Loss: 0.5218\n",
            "Epoch 93/100, Train Loss: 0.3465, Validation Loss: 0.5225\n",
            "Epoch 94/100, Train Loss: 0.3455, Validation Loss: 0.5238\n",
            "Epoch 95/100, Train Loss: 0.3448, Validation Loss: 0.5260\n",
            "Epoch 96/100, Train Loss: 0.3440, Validation Loss: 0.5295\n",
            "Epoch 97/100, Train Loss: 0.3433, Validation Loss: 0.5317\n",
            "Epoch 98/100, Train Loss: 0.3426, Validation Loss: 0.5327\n",
            "Epoch 99/100, Train Loss: 0.3420, Validation Loss: 0.5344\n",
            "Epoch 100/100, Train Loss: 0.3412, Validation Loss: 0.5368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6920, Validation Loss: 0.6923\n",
            "Epoch 2/100, Train Loss: 0.6879, Validation Loss: 0.6855\n",
            "Epoch 3/100, Train Loss: 0.6838, Validation Loss: 0.6789\n",
            "Epoch 4/100, Train Loss: 0.6797, Validation Loss: 0.6723\n",
            "Epoch 5/100, Train Loss: 0.6755, Validation Loss: 0.6654\n",
            "Epoch 6/100, Train Loss: 0.6709, Validation Loss: 0.6580\n",
            "Epoch 7/100, Train Loss: 0.6660, Validation Loss: 0.6500\n",
            "Epoch 8/100, Train Loss: 0.6608, Validation Loss: 0.6416\n",
            "Epoch 9/100, Train Loss: 0.6553, Validation Loss: 0.6327\n",
            "Epoch 10/100, Train Loss: 0.6494, Validation Loss: 0.6233\n",
            "Epoch 11/100, Train Loss: 0.6431, Validation Loss: 0.6134\n",
            "Epoch 12/100, Train Loss: 0.6364, Validation Loss: 0.6030\n",
            "Epoch 13/100, Train Loss: 0.6293, Validation Loss: 0.5925\n",
            "Epoch 14/100, Train Loss: 0.6221, Validation Loss: 0.5821\n",
            "Epoch 15/100, Train Loss: 0.6147, Validation Loss: 0.5717\n",
            "Epoch 16/100, Train Loss: 0.6070, Validation Loss: 0.5616\n",
            "Epoch 17/100, Train Loss: 0.5994, Validation Loss: 0.5519\n",
            "Epoch 18/100, Train Loss: 0.5917, Validation Loss: 0.5426\n",
            "Epoch 19/100, Train Loss: 0.5841, Validation Loss: 0.5340\n",
            "Epoch 20/100, Train Loss: 0.5767, Validation Loss: 0.5258\n",
            "Epoch 21/100, Train Loss: 0.5697, Validation Loss: 0.5185\n",
            "Epoch 22/100, Train Loss: 0.5628, Validation Loss: 0.5116\n",
            "Epoch 23/100, Train Loss: 0.5565, Validation Loss: 0.5052\n",
            "Epoch 24/100, Train Loss: 0.5502, Validation Loss: 0.4995\n",
            "Epoch 25/100, Train Loss: 0.5443, Validation Loss: 0.4941\n",
            "Epoch 26/100, Train Loss: 0.5387, Validation Loss: 0.4891\n",
            "Epoch 27/100, Train Loss: 0.5335, Validation Loss: 0.4848\n",
            "Epoch 28/100, Train Loss: 0.5283, Validation Loss: 0.4800\n",
            "Epoch 29/100, Train Loss: 0.5233, Validation Loss: 0.4751\n",
            "Epoch 30/100, Train Loss: 0.5188, Validation Loss: 0.4711\n",
            "Epoch 31/100, Train Loss: 0.5141, Validation Loss: 0.4678\n",
            "Epoch 32/100, Train Loss: 0.5097, Validation Loss: 0.4639\n",
            "Epoch 33/100, Train Loss: 0.5057, Validation Loss: 0.4612\n",
            "Epoch 34/100, Train Loss: 0.5015, Validation Loss: 0.4579\n",
            "Epoch 35/100, Train Loss: 0.4975, Validation Loss: 0.4545\n",
            "Epoch 36/100, Train Loss: 0.4940, Validation Loss: 0.4528\n",
            "Epoch 37/100, Train Loss: 0.4902, Validation Loss: 0.4491\n",
            "Epoch 38/100, Train Loss: 0.4864, Validation Loss: 0.4475\n",
            "Epoch 39/100, Train Loss: 0.4830, Validation Loss: 0.4447\n",
            "Epoch 40/100, Train Loss: 0.4795, Validation Loss: 0.4432\n",
            "Epoch 41/100, Train Loss: 0.4761, Validation Loss: 0.4420\n",
            "Epoch 42/100, Train Loss: 0.4727, Validation Loss: 0.4373\n",
            "Epoch 43/100, Train Loss: 0.4697, Validation Loss: 0.4354\n",
            "Epoch 44/100, Train Loss: 0.4666, Validation Loss: 0.4340\n",
            "Epoch 45/100, Train Loss: 0.4637, Validation Loss: 0.4348\n",
            "Epoch 46/100, Train Loss: 0.4608, Validation Loss: 0.4306\n",
            "Epoch 47/100, Train Loss: 0.4578, Validation Loss: 0.4300\n",
            "Epoch 48/100, Train Loss: 0.4550, Validation Loss: 0.4260\n",
            "Epoch 49/100, Train Loss: 0.4521, Validation Loss: 0.4253\n",
            "Epoch 50/100, Train Loss: 0.4496, Validation Loss: 0.4257\n",
            "Epoch 51/100, Train Loss: 0.4465, Validation Loss: 0.4218\n",
            "Epoch 52/100, Train Loss: 0.4441, Validation Loss: 0.4215\n",
            "Epoch 53/100, Train Loss: 0.4416, Validation Loss: 0.4192\n",
            "Epoch 54/100, Train Loss: 0.4389, Validation Loss: 0.4175\n",
            "Epoch 55/100, Train Loss: 0.4368, Validation Loss: 0.4172\n",
            "Epoch 56/100, Train Loss: 0.4340, Validation Loss: 0.4166\n",
            "Epoch 57/100, Train Loss: 0.4318, Validation Loss: 0.4165\n",
            "Epoch 58/100, Train Loss: 0.4294, Validation Loss: 0.4196\n",
            "Epoch 59/100, Train Loss: 0.4273, Validation Loss: 0.4148\n",
            "Epoch 60/100, Train Loss: 0.4249, Validation Loss: 0.4140\n",
            "Epoch 61/100, Train Loss: 0.4224, Validation Loss: 0.4185\n",
            "Epoch 62/100, Train Loss: 0.4204, Validation Loss: 0.4155\n",
            "Epoch 63/100, Train Loss: 0.4180, Validation Loss: 0.4098\n",
            "Epoch 64/100, Train Loss: 0.4161, Validation Loss: 0.4180\n",
            "Epoch 65/100, Train Loss: 0.4142, Validation Loss: 0.4125\n",
            "Epoch 66/100, Train Loss: 0.4119, Validation Loss: 0.4081\n",
            "Epoch 67/100, Train Loss: 0.4100, Validation Loss: 0.4121\n",
            "Epoch 68/100, Train Loss: 0.4080, Validation Loss: 0.4101\n",
            "Epoch 69/100, Train Loss: 0.4062, Validation Loss: 0.4091\n",
            "Epoch 70/100, Train Loss: 0.4043, Validation Loss: 0.4141\n",
            "Epoch 71/100, Train Loss: 0.4025, Validation Loss: 0.4087\n",
            "Epoch 72/100, Train Loss: 0.4007, Validation Loss: 0.4141\n",
            "Epoch 73/100, Train Loss: 0.3991, Validation Loss: 0.4154\n",
            "Epoch 74/100, Train Loss: 0.3972, Validation Loss: 0.4073\n",
            "Epoch 75/100, Train Loss: 0.3954, Validation Loss: 0.4109\n",
            "Epoch 76/100, Train Loss: 0.3938, Validation Loss: 0.4127\n",
            "Epoch 77/100, Train Loss: 0.3922, Validation Loss: 0.4026\n",
            "Epoch 78/100, Train Loss: 0.3908, Validation Loss: 0.4139\n",
            "Epoch 79/100, Train Loss: 0.3892, Validation Loss: 0.4082\n",
            "Epoch 80/100, Train Loss: 0.3874, Validation Loss: 0.4024\n",
            "Epoch 81/100, Train Loss: 0.3862, Validation Loss: 0.4112\n",
            "Epoch 82/100, Train Loss: 0.3847, Validation Loss: 0.4183\n",
            "Epoch 83/100, Train Loss: 0.3833, Validation Loss: 0.4133\n",
            "Epoch 84/100, Train Loss: 0.3818, Validation Loss: 0.4049\n",
            "Epoch 85/100, Train Loss: 0.3804, Validation Loss: 0.4051\n",
            "Epoch 86/100, Train Loss: 0.3792, Validation Loss: 0.4042\n",
            "Epoch 87/100, Train Loss: 0.3782, Validation Loss: 0.4053\n",
            "Epoch 88/100, Train Loss: 0.3769, Validation Loss: 0.4078\n",
            "Epoch 89/100, Train Loss: 0.3758, Validation Loss: 0.4117\n",
            "Epoch 90/100, Train Loss: 0.3746, Validation Loss: 0.4122\n",
            "Epoch 91/100, Train Loss: 0.3736, Validation Loss: 0.4117\n",
            "Epoch 92/100, Train Loss: 0.3723, Validation Loss: 0.4081\n",
            "Epoch 93/100, Train Loss: 0.3712, Validation Loss: 0.4120\n",
            "Epoch 94/100, Train Loss: 0.3703, Validation Loss: 0.4111\n",
            "Epoch 95/100, Train Loss: 0.3693, Validation Loss: 0.4142\n",
            "Epoch 96/100, Train Loss: 0.3681, Validation Loss: 0.4281\n",
            "Epoch 97/100, Train Loss: 0.3674, Validation Loss: 0.4243\n",
            "Epoch 98/100, Train Loss: 0.3665, Validation Loss: 0.4112\n",
            "Epoch 99/100, Train Loss: 0.3656, Validation Loss: 0.4172\n",
            "Epoch 100/100, Train Loss: 0.3647, Validation Loss: 0.4150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6924, Validation Loss: 0.6883\n",
            "Epoch 2/100, Train Loss: 0.6881, Validation Loss: 0.6808\n",
            "Epoch 3/100, Train Loss: 0.6841, Validation Loss: 0.6738\n",
            "Epoch 4/100, Train Loss: 0.6800, Validation Loss: 0.6665\n",
            "Epoch 5/100, Train Loss: 0.6758, Validation Loss: 0.6588\n",
            "Epoch 6/100, Train Loss: 0.6713, Validation Loss: 0.6507\n",
            "Epoch 7/100, Train Loss: 0.6665, Validation Loss: 0.6419\n",
            "Epoch 8/100, Train Loss: 0.6613, Validation Loss: 0.6326\n",
            "Epoch 9/100, Train Loss: 0.6557, Validation Loss: 0.6228\n",
            "Epoch 10/100, Train Loss: 0.6497, Validation Loss: 0.6125\n",
            "Epoch 11/100, Train Loss: 0.6433, Validation Loss: 0.6018\n",
            "Epoch 12/100, Train Loss: 0.6366, Validation Loss: 0.5912\n",
            "Epoch 13/100, Train Loss: 0.6297, Validation Loss: 0.5809\n",
            "Epoch 14/100, Train Loss: 0.6226, Validation Loss: 0.5706\n",
            "Epoch 15/100, Train Loss: 0.6154, Validation Loss: 0.5605\n",
            "Epoch 16/100, Train Loss: 0.6082, Validation Loss: 0.5507\n",
            "Epoch 17/100, Train Loss: 0.6010, Validation Loss: 0.5417\n",
            "Epoch 18/100, Train Loss: 0.5940, Validation Loss: 0.5329\n",
            "Epoch 19/100, Train Loss: 0.5870, Validation Loss: 0.5253\n",
            "Epoch 20/100, Train Loss: 0.5802, Validation Loss: 0.5178\n",
            "Epoch 21/100, Train Loss: 0.5736, Validation Loss: 0.5097\n",
            "Epoch 22/100, Train Loss: 0.5673, Validation Loss: 0.5044\n",
            "Epoch 23/100, Train Loss: 0.5611, Validation Loss: 0.4994\n",
            "Epoch 24/100, Train Loss: 0.5551, Validation Loss: 0.4933\n",
            "Epoch 25/100, Train Loss: 0.5492, Validation Loss: 0.4880\n",
            "Epoch 26/100, Train Loss: 0.5436, Validation Loss: 0.4856\n",
            "Epoch 27/100, Train Loss: 0.5380, Validation Loss: 0.4786\n",
            "Epoch 28/100, Train Loss: 0.5329, Validation Loss: 0.4756\n",
            "Epoch 29/100, Train Loss: 0.5276, Validation Loss: 0.4758\n",
            "Epoch 30/100, Train Loss: 0.5228, Validation Loss: 0.4719\n",
            "Epoch 31/100, Train Loss: 0.5183, Validation Loss: 0.4688\n",
            "Epoch 32/100, Train Loss: 0.5135, Validation Loss: 0.4645\n",
            "Epoch 33/100, Train Loss: 0.5090, Validation Loss: 0.4632\n",
            "Epoch 34/100, Train Loss: 0.5046, Validation Loss: 0.4593\n",
            "Epoch 35/100, Train Loss: 0.5004, Validation Loss: 0.4593\n",
            "Epoch 36/100, Train Loss: 0.4964, Validation Loss: 0.4572\n",
            "Epoch 37/100, Train Loss: 0.4923, Validation Loss: 0.4561\n",
            "Epoch 38/100, Train Loss: 0.4885, Validation Loss: 0.4540\n",
            "Epoch 39/100, Train Loss: 0.4846, Validation Loss: 0.4520\n",
            "Epoch 40/100, Train Loss: 0.4810, Validation Loss: 0.4524\n",
            "Epoch 41/100, Train Loss: 0.4774, Validation Loss: 0.4515\n",
            "Epoch 42/100, Train Loss: 0.4738, Validation Loss: 0.4525\n",
            "Epoch 43/100, Train Loss: 0.4703, Validation Loss: 0.4490\n",
            "Epoch 44/100, Train Loss: 0.4673, Validation Loss: 0.4500\n",
            "Epoch 45/100, Train Loss: 0.4637, Validation Loss: 0.4496\n",
            "Epoch 46/100, Train Loss: 0.4605, Validation Loss: 0.4485\n",
            "Epoch 47/100, Train Loss: 0.4574, Validation Loss: 0.4478\n",
            "Epoch 48/100, Train Loss: 0.4543, Validation Loss: 0.4491\n",
            "Epoch 49/100, Train Loss: 0.4512, Validation Loss: 0.4498\n",
            "Epoch 50/100, Train Loss: 0.4483, Validation Loss: 0.4502\n",
            "Epoch 51/100, Train Loss: 0.4453, Validation Loss: 0.4487\n",
            "Epoch 52/100, Train Loss: 0.4428, Validation Loss: 0.4521\n",
            "Epoch 53/100, Train Loss: 0.4399, Validation Loss: 0.4519\n",
            "Epoch 54/100, Train Loss: 0.4373, Validation Loss: 0.4502\n",
            "Epoch 55/100, Train Loss: 0.4347, Validation Loss: 0.4504\n",
            "Epoch 56/100, Train Loss: 0.4321, Validation Loss: 0.4518\n",
            "Epoch 57/100, Train Loss: 0.4294, Validation Loss: 0.4484\n",
            "Epoch 58/100, Train Loss: 0.4271, Validation Loss: 0.4532\n",
            "Epoch 59/100, Train Loss: 0.4246, Validation Loss: 0.4509\n",
            "Epoch 60/100, Train Loss: 0.4224, Validation Loss: 0.4522\n",
            "Epoch 61/100, Train Loss: 0.4200, Validation Loss: 0.4572\n",
            "Epoch 62/100, Train Loss: 0.4177, Validation Loss: 0.4532\n",
            "Epoch 63/100, Train Loss: 0.4154, Validation Loss: 0.4573\n",
            "Epoch 64/100, Train Loss: 0.4132, Validation Loss: 0.4580\n",
            "Epoch 65/100, Train Loss: 0.4113, Validation Loss: 0.4594\n",
            "Epoch 66/100, Train Loss: 0.4091, Validation Loss: 0.4586\n",
            "Epoch 67/100, Train Loss: 0.4072, Validation Loss: 0.4601\n",
            "Epoch 68/100, Train Loss: 0.4051, Validation Loss: 0.4582\n",
            "Epoch 69/100, Train Loss: 0.4031, Validation Loss: 0.4600\n",
            "Epoch 70/100, Train Loss: 0.4013, Validation Loss: 0.4609\n",
            "Epoch 71/100, Train Loss: 0.3994, Validation Loss: 0.4623\n",
            "Epoch 72/100, Train Loss: 0.3976, Validation Loss: 0.4616\n",
            "Epoch 73/100, Train Loss: 0.3957, Validation Loss: 0.4617\n",
            "Epoch 74/100, Train Loss: 0.3942, Validation Loss: 0.4618\n",
            "Epoch 75/100, Train Loss: 0.3927, Validation Loss: 0.4687\n",
            "Epoch 76/100, Train Loss: 0.3907, Validation Loss: 0.4674\n",
            "Epoch 77/100, Train Loss: 0.3891, Validation Loss: 0.4709\n",
            "Epoch 78/100, Train Loss: 0.3874, Validation Loss: 0.4702\n",
            "Epoch 79/100, Train Loss: 0.3859, Validation Loss: 0.4710\n",
            "Epoch 80/100, Train Loss: 0.3842, Validation Loss: 0.4659\n",
            "Epoch 81/100, Train Loss: 0.3826, Validation Loss: 0.4657\n",
            "Epoch 82/100, Train Loss: 0.3813, Validation Loss: 0.4766\n",
            "Epoch 83/100, Train Loss: 0.3799, Validation Loss: 0.4715\n",
            "Epoch 84/100, Train Loss: 0.3784, Validation Loss: 0.4787\n",
            "Epoch 85/100, Train Loss: 0.3770, Validation Loss: 0.4790\n",
            "Epoch 86/100, Train Loss: 0.3757, Validation Loss: 0.4860\n",
            "Epoch 87/100, Train Loss: 0.3742, Validation Loss: 0.4786\n",
            "Epoch 88/100, Train Loss: 0.3730, Validation Loss: 0.4851\n",
            "Epoch 89/100, Train Loss: 0.3716, Validation Loss: 0.4741\n",
            "Epoch 90/100, Train Loss: 0.3706, Validation Loss: 0.4814\n",
            "Epoch 91/100, Train Loss: 0.3694, Validation Loss: 0.4840\n",
            "Epoch 92/100, Train Loss: 0.3682, Validation Loss: 0.4837\n",
            "Epoch 93/100, Train Loss: 0.3670, Validation Loss: 0.4809\n",
            "Epoch 94/100, Train Loss: 0.3661, Validation Loss: 0.4867\n",
            "Epoch 95/100, Train Loss: 0.3652, Validation Loss: 0.4894\n",
            "Epoch 96/100, Train Loss: 0.3640, Validation Loss: 0.4866\n",
            "Epoch 97/100, Train Loss: 0.3631, Validation Loss: 0.4919\n",
            "Epoch 98/100, Train Loss: 0.3621, Validation Loss: 0.4911\n",
            "Epoch 99/100, Train Loss: 0.3613, Validation Loss: 0.4916\n",
            "Epoch 100/100, Train Loss: 0.3602, Validation Loss: 0.4975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6913, Validation Loss: 0.6833\n",
            "Epoch 2/100, Train Loss: 0.6860, Validation Loss: 0.6733\n",
            "Epoch 3/100, Train Loss: 0.6805, Validation Loss: 0.6628\n",
            "Epoch 4/100, Train Loss: 0.6749, Validation Loss: 0.6522\n",
            "Epoch 5/100, Train Loss: 0.6690, Validation Loss: 0.6411\n",
            "Epoch 6/100, Train Loss: 0.6630, Validation Loss: 0.6303\n",
            "Epoch 7/100, Train Loss: 0.6569, Validation Loss: 0.6191\n",
            "Epoch 8/100, Train Loss: 0.6508, Validation Loss: 0.6081\n",
            "Epoch 9/100, Train Loss: 0.6445, Validation Loss: 0.5969\n",
            "Epoch 10/100, Train Loss: 0.6383, Validation Loss: 0.5862\n",
            "Epoch 11/100, Train Loss: 0.6319, Validation Loss: 0.5756\n",
            "Epoch 12/100, Train Loss: 0.6255, Validation Loss: 0.5650\n",
            "Epoch 13/100, Train Loss: 0.6191, Validation Loss: 0.5540\n",
            "Epoch 14/100, Train Loss: 0.6125, Validation Loss: 0.5438\n",
            "Epoch 15/100, Train Loss: 0.6059, Validation Loss: 0.5337\n",
            "Epoch 16/100, Train Loss: 0.5993, Validation Loss: 0.5231\n",
            "Epoch 17/100, Train Loss: 0.5925, Validation Loss: 0.5133\n",
            "Epoch 18/100, Train Loss: 0.5859, Validation Loss: 0.5040\n",
            "Epoch 19/100, Train Loss: 0.5793, Validation Loss: 0.4949\n",
            "Epoch 20/100, Train Loss: 0.5728, Validation Loss: 0.4859\n",
            "Epoch 21/100, Train Loss: 0.5664, Validation Loss: 0.4774\n",
            "Epoch 22/100, Train Loss: 0.5601, Validation Loss: 0.4707\n",
            "Epoch 23/100, Train Loss: 0.5540, Validation Loss: 0.4631\n",
            "Epoch 24/100, Train Loss: 0.5482, Validation Loss: 0.4556\n",
            "Epoch 25/100, Train Loss: 0.5423, Validation Loss: 0.4478\n",
            "Epoch 26/100, Train Loss: 0.5369, Validation Loss: 0.4432\n",
            "Epoch 27/100, Train Loss: 0.5316, Validation Loss: 0.4357\n",
            "Epoch 28/100, Train Loss: 0.5263, Validation Loss: 0.4314\n",
            "Epoch 29/100, Train Loss: 0.5212, Validation Loss: 0.4293\n",
            "Epoch 30/100, Train Loss: 0.5165, Validation Loss: 0.4229\n",
            "Epoch 31/100, Train Loss: 0.5118, Validation Loss: 0.4191\n",
            "Epoch 32/100, Train Loss: 0.5073, Validation Loss: 0.4160\n",
            "Epoch 33/100, Train Loss: 0.5030, Validation Loss: 0.4110\n",
            "Epoch 34/100, Train Loss: 0.4987, Validation Loss: 0.4095\n",
            "Epoch 35/100, Train Loss: 0.4946, Validation Loss: 0.4047\n",
            "Epoch 36/100, Train Loss: 0.4908, Validation Loss: 0.4031\n",
            "Epoch 37/100, Train Loss: 0.4867, Validation Loss: 0.4041\n",
            "Epoch 38/100, Train Loss: 0.4831, Validation Loss: 0.3979\n",
            "Epoch 39/100, Train Loss: 0.4795, Validation Loss: 0.3963\n",
            "Epoch 40/100, Train Loss: 0.4759, Validation Loss: 0.3956\n",
            "Epoch 41/100, Train Loss: 0.4725, Validation Loss: 0.3922\n",
            "Epoch 42/100, Train Loss: 0.4690, Validation Loss: 0.3910\n",
            "Epoch 43/100, Train Loss: 0.4658, Validation Loss: 0.3942\n",
            "Epoch 44/100, Train Loss: 0.4627, Validation Loss: 0.3918\n",
            "Epoch 45/100, Train Loss: 0.4595, Validation Loss: 0.3885\n",
            "Epoch 46/100, Train Loss: 0.4565, Validation Loss: 0.3892\n",
            "Epoch 47/100, Train Loss: 0.4535, Validation Loss: 0.3867\n",
            "Epoch 48/100, Train Loss: 0.4506, Validation Loss: 0.3846\n",
            "Epoch 49/100, Train Loss: 0.4478, Validation Loss: 0.3836\n",
            "Epoch 50/100, Train Loss: 0.4453, Validation Loss: 0.3843\n",
            "Epoch 51/100, Train Loss: 0.4424, Validation Loss: 0.3832\n",
            "Epoch 52/100, Train Loss: 0.4398, Validation Loss: 0.3806\n",
            "Epoch 53/100, Train Loss: 0.4373, Validation Loss: 0.3822\n",
            "Epoch 54/100, Train Loss: 0.4349, Validation Loss: 0.3847\n",
            "Epoch 55/100, Train Loss: 0.4323, Validation Loss: 0.3823\n",
            "Epoch 56/100, Train Loss: 0.4300, Validation Loss: 0.3812\n",
            "Epoch 57/100, Train Loss: 0.4274, Validation Loss: 0.3853\n",
            "Epoch 58/100, Train Loss: 0.4254, Validation Loss: 0.3796\n",
            "Epoch 59/100, Train Loss: 0.4232, Validation Loss: 0.3813\n",
            "Epoch 60/100, Train Loss: 0.4212, Validation Loss: 0.3809\n",
            "Epoch 61/100, Train Loss: 0.4189, Validation Loss: 0.3793\n",
            "Epoch 62/100, Train Loss: 0.4169, Validation Loss: 0.3799\n",
            "Epoch 63/100, Train Loss: 0.4145, Validation Loss: 0.3855\n",
            "Epoch 64/100, Train Loss: 0.4124, Validation Loss: 0.3778\n",
            "Epoch 65/100, Train Loss: 0.4106, Validation Loss: 0.3817\n",
            "Epoch 66/100, Train Loss: 0.4089, Validation Loss: 0.3801\n",
            "Epoch 67/100, Train Loss: 0.4068, Validation Loss: 0.3794\n",
            "Epoch 68/100, Train Loss: 0.4050, Validation Loss: 0.3767\n",
            "Epoch 69/100, Train Loss: 0.4033, Validation Loss: 0.3790\n",
            "Epoch 70/100, Train Loss: 0.4013, Validation Loss: 0.3777\n",
            "Epoch 71/100, Train Loss: 0.3995, Validation Loss: 0.3847\n",
            "Epoch 72/100, Train Loss: 0.3980, Validation Loss: 0.3816\n",
            "Epoch 73/100, Train Loss: 0.3963, Validation Loss: 0.3796\n",
            "Epoch 74/100, Train Loss: 0.3947, Validation Loss: 0.3829\n",
            "Epoch 75/100, Train Loss: 0.3929, Validation Loss: 0.3852\n",
            "Epoch 76/100, Train Loss: 0.3915, Validation Loss: 0.3848\n",
            "Epoch 77/100, Train Loss: 0.3899, Validation Loss: 0.3796\n",
            "Epoch 78/100, Train Loss: 0.3880, Validation Loss: 0.3898\n",
            "Epoch 79/100, Train Loss: 0.3870, Validation Loss: 0.3832\n",
            "Epoch 80/100, Train Loss: 0.3856, Validation Loss: 0.3853\n",
            "Epoch 81/100, Train Loss: 0.3841, Validation Loss: 0.3828\n",
            "Epoch 82/100, Train Loss: 0.3825, Validation Loss: 0.3874\n",
            "Epoch 83/100, Train Loss: 0.3812, Validation Loss: 0.3858\n",
            "Epoch 84/100, Train Loss: 0.3798, Validation Loss: 0.3892\n",
            "Epoch 85/100, Train Loss: 0.3786, Validation Loss: 0.3936\n",
            "Epoch 86/100, Train Loss: 0.3772, Validation Loss: 0.3868\n",
            "Epoch 87/100, Train Loss: 0.3760, Validation Loss: 0.3898\n",
            "Epoch 88/100, Train Loss: 0.3747, Validation Loss: 0.3899\n",
            "Epoch 89/100, Train Loss: 0.3735, Validation Loss: 0.3876\n",
            "Epoch 90/100, Train Loss: 0.3725, Validation Loss: 0.3884\n",
            "Epoch 91/100, Train Loss: 0.3714, Validation Loss: 0.3906\n",
            "Epoch 92/100, Train Loss: 0.3704, Validation Loss: 0.3934\n",
            "Epoch 93/100, Train Loss: 0.3690, Validation Loss: 0.3932\n",
            "Epoch 94/100, Train Loss: 0.3679, Validation Loss: 0.3910\n",
            "Epoch 95/100, Train Loss: 0.3670, Validation Loss: 0.3987\n",
            "Epoch 96/100, Train Loss: 0.3661, Validation Loss: 0.4003\n",
            "Epoch 97/100, Train Loss: 0.3651, Validation Loss: 0.3996\n",
            "Epoch 98/100, Train Loss: 0.3643, Validation Loss: 0.3979\n",
            "Epoch 99/100, Train Loss: 0.3632, Validation Loss: 0.4051\n",
            "Epoch 100/100, Train Loss: 0.3625, Validation Loss: 0.3988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6919, Validation Loss: 0.6843\n",
            "Epoch 2/100, Train Loss: 0.6880, Validation Loss: 0.6779\n",
            "Epoch 3/100, Train Loss: 0.6843, Validation Loss: 0.6719\n",
            "Epoch 4/100, Train Loss: 0.6807, Validation Loss: 0.6657\n",
            "Epoch 5/100, Train Loss: 0.6768, Validation Loss: 0.6591\n",
            "Epoch 6/100, Train Loss: 0.6726, Validation Loss: 0.6520\n",
            "Epoch 7/100, Train Loss: 0.6679, Validation Loss: 0.6441\n",
            "Epoch 8/100, Train Loss: 0.6626, Validation Loss: 0.6353\n",
            "Epoch 9/100, Train Loss: 0.6568, Validation Loss: 0.6258\n",
            "Epoch 10/100, Train Loss: 0.6505, Validation Loss: 0.6158\n",
            "Epoch 11/100, Train Loss: 0.6436, Validation Loss: 0.6053\n",
            "Epoch 12/100, Train Loss: 0.6365, Validation Loss: 0.5947\n",
            "Epoch 13/100, Train Loss: 0.6290, Validation Loss: 0.5840\n",
            "Epoch 14/100, Train Loss: 0.6214, Validation Loss: 0.5735\n",
            "Epoch 15/100, Train Loss: 0.6136, Validation Loss: 0.5634\n",
            "Epoch 16/100, Train Loss: 0.6059, Validation Loss: 0.5536\n",
            "Epoch 17/100, Train Loss: 0.5982, Validation Loss: 0.5443\n",
            "Epoch 18/100, Train Loss: 0.5907, Validation Loss: 0.5356\n",
            "Epoch 19/100, Train Loss: 0.5834, Validation Loss: 0.5275\n",
            "Epoch 20/100, Train Loss: 0.5764, Validation Loss: 0.5202\n",
            "Epoch 21/100, Train Loss: 0.5697, Validation Loss: 0.5134\n",
            "Epoch 22/100, Train Loss: 0.5633, Validation Loss: 0.5074\n",
            "Epoch 23/100, Train Loss: 0.5572, Validation Loss: 0.5017\n",
            "Epoch 24/100, Train Loss: 0.5512, Validation Loss: 0.4967\n",
            "Epoch 25/100, Train Loss: 0.5456, Validation Loss: 0.4917\n",
            "Epoch 26/100, Train Loss: 0.5401, Validation Loss: 0.4866\n",
            "Epoch 27/100, Train Loss: 0.5351, Validation Loss: 0.4820\n",
            "Epoch 28/100, Train Loss: 0.5299, Validation Loss: 0.4784\n",
            "Epoch 29/100, Train Loss: 0.5250, Validation Loss: 0.4736\n",
            "Epoch 30/100, Train Loss: 0.5205, Validation Loss: 0.4699\n",
            "Epoch 31/100, Train Loss: 0.5156, Validation Loss: 0.4653\n",
            "Epoch 32/100, Train Loss: 0.5112, Validation Loss: 0.4616\n",
            "Epoch 33/100, Train Loss: 0.5069, Validation Loss: 0.4573\n",
            "Epoch 34/100, Train Loss: 0.5027, Validation Loss: 0.4536\n",
            "Epoch 35/100, Train Loss: 0.4985, Validation Loss: 0.4505\n",
            "Epoch 36/100, Train Loss: 0.4945, Validation Loss: 0.4470\n",
            "Epoch 37/100, Train Loss: 0.4906, Validation Loss: 0.4442\n",
            "Epoch 38/100, Train Loss: 0.4866, Validation Loss: 0.4421\n",
            "Epoch 39/100, Train Loss: 0.4827, Validation Loss: 0.4385\n",
            "Epoch 40/100, Train Loss: 0.4795, Validation Loss: 0.4383\n",
            "Epoch 41/100, Train Loss: 0.4758, Validation Loss: 0.4370\n",
            "Epoch 42/100, Train Loss: 0.4724, Validation Loss: 0.4343\n",
            "Epoch 43/100, Train Loss: 0.4687, Validation Loss: 0.4318\n",
            "Epoch 44/100, Train Loss: 0.4656, Validation Loss: 0.4311\n",
            "Epoch 45/100, Train Loss: 0.4625, Validation Loss: 0.4300\n",
            "Epoch 46/100, Train Loss: 0.4591, Validation Loss: 0.4264\n",
            "Epoch 47/100, Train Loss: 0.4560, Validation Loss: 0.4292\n",
            "Epoch 48/100, Train Loss: 0.4529, Validation Loss: 0.4276\n",
            "Epoch 49/100, Train Loss: 0.4502, Validation Loss: 0.4256\n",
            "Epoch 50/100, Train Loss: 0.4472, Validation Loss: 0.4235\n",
            "Epoch 51/100, Train Loss: 0.4443, Validation Loss: 0.4245\n",
            "Epoch 52/100, Train Loss: 0.4418, Validation Loss: 0.4248\n",
            "Epoch 53/100, Train Loss: 0.4388, Validation Loss: 0.4237\n",
            "Epoch 54/100, Train Loss: 0.4363, Validation Loss: 0.4243\n",
            "Epoch 55/100, Train Loss: 0.4338, Validation Loss: 0.4227\n",
            "Epoch 56/100, Train Loss: 0.4313, Validation Loss: 0.4267\n",
            "Epoch 57/100, Train Loss: 0.4287, Validation Loss: 0.4227\n",
            "Epoch 58/100, Train Loss: 0.4265, Validation Loss: 0.4247\n",
            "Epoch 59/100, Train Loss: 0.4241, Validation Loss: 0.4227\n",
            "Epoch 60/100, Train Loss: 0.4220, Validation Loss: 0.4231\n",
            "Epoch 61/100, Train Loss: 0.4197, Validation Loss: 0.4233\n",
            "Epoch 62/100, Train Loss: 0.4174, Validation Loss: 0.4296\n",
            "Epoch 63/100, Train Loss: 0.4152, Validation Loss: 0.4255\n",
            "Epoch 64/100, Train Loss: 0.4132, Validation Loss: 0.4259\n",
            "Epoch 65/100, Train Loss: 0.4115, Validation Loss: 0.4280\n",
            "Epoch 66/100, Train Loss: 0.4090, Validation Loss: 0.4249\n",
            "Epoch 67/100, Train Loss: 0.4074, Validation Loss: 0.4268\n",
            "Epoch 68/100, Train Loss: 0.4055, Validation Loss: 0.4264\n",
            "Epoch 69/100, Train Loss: 0.4037, Validation Loss: 0.4274\n",
            "Epoch 70/100, Train Loss: 0.4021, Validation Loss: 0.4293\n",
            "Epoch 71/100, Train Loss: 0.4002, Validation Loss: 0.4273\n",
            "Epoch 72/100, Train Loss: 0.3986, Validation Loss: 0.4308\n",
            "Epoch 73/100, Train Loss: 0.3970, Validation Loss: 0.4311\n",
            "Epoch 74/100, Train Loss: 0.3951, Validation Loss: 0.4303\n",
            "Epoch 75/100, Train Loss: 0.3934, Validation Loss: 0.4323\n",
            "Epoch 76/100, Train Loss: 0.3921, Validation Loss: 0.4321\n",
            "Epoch 77/100, Train Loss: 0.3903, Validation Loss: 0.4337\n",
            "Epoch 78/100, Train Loss: 0.3890, Validation Loss: 0.4327\n",
            "Epoch 79/100, Train Loss: 0.3873, Validation Loss: 0.4326\n",
            "Epoch 80/100, Train Loss: 0.3864, Validation Loss: 0.4336\n",
            "Epoch 81/100, Train Loss: 0.3845, Validation Loss: 0.4341\n",
            "Epoch 82/100, Train Loss: 0.3829, Validation Loss: 0.4331\n",
            "Epoch 83/100, Train Loss: 0.3818, Validation Loss: 0.4360\n",
            "Epoch 84/100, Train Loss: 0.3803, Validation Loss: 0.4382\n",
            "Epoch 85/100, Train Loss: 0.3789, Validation Loss: 0.4362\n",
            "Epoch 86/100, Train Loss: 0.3777, Validation Loss: 0.4391\n",
            "Epoch 87/100, Train Loss: 0.3765, Validation Loss: 0.4391\n",
            "Epoch 88/100, Train Loss: 0.3754, Validation Loss: 0.4392\n",
            "Epoch 89/100, Train Loss: 0.3741, Validation Loss: 0.4422\n",
            "Epoch 90/100, Train Loss: 0.3728, Validation Loss: 0.4394\n",
            "Epoch 91/100, Train Loss: 0.3720, Validation Loss: 0.4414\n",
            "Epoch 92/100, Train Loss: 0.3708, Validation Loss: 0.4437\n",
            "Epoch 93/100, Train Loss: 0.3697, Validation Loss: 0.4442\n",
            "Epoch 94/100, Train Loss: 0.3687, Validation Loss: 0.4440\n",
            "Epoch 95/100, Train Loss: 0.3680, Validation Loss: 0.4452\n",
            "Epoch 96/100, Train Loss: 0.3667, Validation Loss: 0.4488\n",
            "Epoch 97/100, Train Loss: 0.3659, Validation Loss: 0.4501\n",
            "Epoch 98/100, Train Loss: 0.3651, Validation Loss: 0.4481\n",
            "Epoch 99/100, Train Loss: 0.3641, Validation Loss: 0.4494\n",
            "Epoch 100/100, Train Loss: 0.3633, Validation Loss: 0.4519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6914, Validation Loss: 0.6899\n",
            "Epoch 2/100, Train Loss: 0.6874, Validation Loss: 0.6831\n",
            "Epoch 3/100, Train Loss: 0.6834, Validation Loss: 0.6761\n",
            "Epoch 4/100, Train Loss: 0.6790, Validation Loss: 0.6686\n",
            "Epoch 5/100, Train Loss: 0.6743, Validation Loss: 0.6607\n",
            "Epoch 6/100, Train Loss: 0.6692, Validation Loss: 0.6522\n",
            "Epoch 7/100, Train Loss: 0.6638, Validation Loss: 0.6432\n",
            "Epoch 8/100, Train Loss: 0.6580, Validation Loss: 0.6337\n",
            "Epoch 9/100, Train Loss: 0.6519, Validation Loss: 0.6239\n",
            "Epoch 10/100, Train Loss: 0.6457, Validation Loss: 0.6137\n",
            "Epoch 11/100, Train Loss: 0.6392, Validation Loss: 0.6030\n",
            "Epoch 12/100, Train Loss: 0.6325, Validation Loss: 0.5920\n",
            "Epoch 13/100, Train Loss: 0.6256, Validation Loss: 0.5810\n",
            "Epoch 14/100, Train Loss: 0.6187, Validation Loss: 0.5699\n",
            "Epoch 15/100, Train Loss: 0.6116, Validation Loss: 0.5588\n",
            "Epoch 16/100, Train Loss: 0.6045, Validation Loss: 0.5479\n",
            "Epoch 17/100, Train Loss: 0.5975, Validation Loss: 0.5374\n",
            "Epoch 18/100, Train Loss: 0.5903, Validation Loss: 0.5273\n",
            "Epoch 19/100, Train Loss: 0.5833, Validation Loss: 0.5173\n",
            "Epoch 20/100, Train Loss: 0.5765, Validation Loss: 0.5082\n",
            "Epoch 21/100, Train Loss: 0.5698, Validation Loss: 0.4995\n",
            "Epoch 22/100, Train Loss: 0.5632, Validation Loss: 0.4915\n",
            "Epoch 23/100, Train Loss: 0.5570, Validation Loss: 0.4841\n",
            "Epoch 24/100, Train Loss: 0.5511, Validation Loss: 0.4770\n",
            "Epoch 25/100, Train Loss: 0.5452, Validation Loss: 0.4710\n",
            "Epoch 26/100, Train Loss: 0.5399, Validation Loss: 0.4650\n",
            "Epoch 27/100, Train Loss: 0.5343, Validation Loss: 0.4592\n",
            "Epoch 28/100, Train Loss: 0.5293, Validation Loss: 0.4538\n",
            "Epoch 29/100, Train Loss: 0.5243, Validation Loss: 0.4491\n",
            "Epoch 30/100, Train Loss: 0.5194, Validation Loss: 0.4446\n",
            "Epoch 31/100, Train Loss: 0.5147, Validation Loss: 0.4405\n",
            "Epoch 32/100, Train Loss: 0.5102, Validation Loss: 0.4368\n",
            "Epoch 33/100, Train Loss: 0.5057, Validation Loss: 0.4329\n",
            "Epoch 34/100, Train Loss: 0.5016, Validation Loss: 0.4293\n",
            "Epoch 35/100, Train Loss: 0.4975, Validation Loss: 0.4258\n",
            "Epoch 36/100, Train Loss: 0.4934, Validation Loss: 0.4226\n",
            "Epoch 37/100, Train Loss: 0.4895, Validation Loss: 0.4195\n",
            "Epoch 38/100, Train Loss: 0.4857, Validation Loss: 0.4164\n",
            "Epoch 39/100, Train Loss: 0.4817, Validation Loss: 0.4136\n",
            "Epoch 40/100, Train Loss: 0.4784, Validation Loss: 0.4109\n",
            "Epoch 41/100, Train Loss: 0.4749, Validation Loss: 0.4081\n",
            "Epoch 42/100, Train Loss: 0.4715, Validation Loss: 0.4058\n",
            "Epoch 43/100, Train Loss: 0.4679, Validation Loss: 0.4042\n",
            "Epoch 44/100, Train Loss: 0.4643, Validation Loss: 0.4029\n",
            "Epoch 45/100, Train Loss: 0.4613, Validation Loss: 0.3999\n",
            "Epoch 46/100, Train Loss: 0.4585, Validation Loss: 0.3982\n",
            "Epoch 47/100, Train Loss: 0.4553, Validation Loss: 0.3961\n",
            "Epoch 48/100, Train Loss: 0.4524, Validation Loss: 0.3953\n",
            "Epoch 49/100, Train Loss: 0.4492, Validation Loss: 0.3940\n",
            "Epoch 50/100, Train Loss: 0.4465, Validation Loss: 0.3930\n",
            "Epoch 51/100, Train Loss: 0.4437, Validation Loss: 0.3915\n",
            "Epoch 52/100, Train Loss: 0.4411, Validation Loss: 0.3903\n",
            "Epoch 53/100, Train Loss: 0.4384, Validation Loss: 0.3899\n",
            "Epoch 54/100, Train Loss: 0.4358, Validation Loss: 0.3880\n",
            "Epoch 55/100, Train Loss: 0.4333, Validation Loss: 0.3866\n",
            "Epoch 56/100, Train Loss: 0.4306, Validation Loss: 0.3865\n",
            "Epoch 57/100, Train Loss: 0.4281, Validation Loss: 0.3855\n",
            "Epoch 58/100, Train Loss: 0.4258, Validation Loss: 0.3850\n",
            "Epoch 59/100, Train Loss: 0.4235, Validation Loss: 0.3837\n",
            "Epoch 60/100, Train Loss: 0.4212, Validation Loss: 0.3831\n",
            "Epoch 61/100, Train Loss: 0.4187, Validation Loss: 0.3855\n",
            "Epoch 62/100, Train Loss: 0.4165, Validation Loss: 0.3834\n",
            "Epoch 63/100, Train Loss: 0.4147, Validation Loss: 0.3821\n",
            "Epoch 64/100, Train Loss: 0.4124, Validation Loss: 0.3809\n",
            "Epoch 65/100, Train Loss: 0.4103, Validation Loss: 0.3806\n",
            "Epoch 66/100, Train Loss: 0.4084, Validation Loss: 0.3816\n",
            "Epoch 67/100, Train Loss: 0.4065, Validation Loss: 0.3810\n",
            "Epoch 68/100, Train Loss: 0.4045, Validation Loss: 0.3801\n",
            "Epoch 69/100, Train Loss: 0.4026, Validation Loss: 0.3810\n",
            "Epoch 70/100, Train Loss: 0.4007, Validation Loss: 0.3808\n",
            "Epoch 71/100, Train Loss: 0.3989, Validation Loss: 0.3819\n",
            "Epoch 72/100, Train Loss: 0.3970, Validation Loss: 0.3807\n",
            "Epoch 73/100, Train Loss: 0.3952, Validation Loss: 0.3817\n",
            "Epoch 74/100, Train Loss: 0.3935, Validation Loss: 0.3812\n",
            "Epoch 75/100, Train Loss: 0.3918, Validation Loss: 0.3809\n",
            "Epoch 76/100, Train Loss: 0.3902, Validation Loss: 0.3810\n",
            "Epoch 77/100, Train Loss: 0.3887, Validation Loss: 0.3812\n",
            "Epoch 78/100, Train Loss: 0.3872, Validation Loss: 0.3825\n",
            "Epoch 79/100, Train Loss: 0.3858, Validation Loss: 0.3816\n",
            "Epoch 80/100, Train Loss: 0.3841, Validation Loss: 0.3830\n",
            "Epoch 81/100, Train Loss: 0.3826, Validation Loss: 0.3842\n",
            "Epoch 82/100, Train Loss: 0.3812, Validation Loss: 0.3838\n",
            "Epoch 83/100, Train Loss: 0.3797, Validation Loss: 0.3849\n",
            "Epoch 84/100, Train Loss: 0.3784, Validation Loss: 0.3844\n",
            "Epoch 85/100, Train Loss: 0.3770, Validation Loss: 0.3859\n",
            "Epoch 86/100, Train Loss: 0.3760, Validation Loss: 0.3852\n",
            "Epoch 87/100, Train Loss: 0.3745, Validation Loss: 0.3867\n",
            "Epoch 88/100, Train Loss: 0.3733, Validation Loss: 0.3858\n",
            "Epoch 89/100, Train Loss: 0.3722, Validation Loss: 0.3873\n",
            "Epoch 90/100, Train Loss: 0.3706, Validation Loss: 0.3920\n",
            "Epoch 91/100, Train Loss: 0.3699, Validation Loss: 0.3893\n",
            "Epoch 92/100, Train Loss: 0.3689, Validation Loss: 0.3900\n",
            "Epoch 93/100, Train Loss: 0.3677, Validation Loss: 0.3913\n",
            "Epoch 94/100, Train Loss: 0.3666, Validation Loss: 0.3924\n",
            "Epoch 95/100, Train Loss: 0.3658, Validation Loss: 0.3923\n",
            "Epoch 96/100, Train Loss: 0.3646, Validation Loss: 0.3922\n",
            "Epoch 97/100, Train Loss: 0.3637, Validation Loss: 0.3932\n",
            "Epoch 98/100, Train Loss: 0.3628, Validation Loss: 0.3939\n",
            "Epoch 99/100, Train Loss: 0.3619, Validation Loss: 0.3938\n",
            "Epoch 100/100, Train Loss: 0.3609, Validation Loss: 0.3981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6729, Validation Loss: 0.6023\n",
            "Epoch 2/100, Train Loss: 0.6126, Validation Loss: 0.4969\n",
            "Epoch 3/100, Train Loss: 0.5543, Validation Loss: 0.4378\n",
            "Epoch 4/100, Train Loss: 0.5112, Validation Loss: 0.3960\n",
            "Epoch 5/100, Train Loss: 0.4813, Validation Loss: 0.3812\n",
            "Epoch 6/100, Train Loss: 0.4572, Validation Loss: 0.3738\n",
            "Epoch 7/100, Train Loss: 0.4325, Validation Loss: 0.3625\n",
            "Epoch 8/100, Train Loss: 0.4155, Validation Loss: 0.3613\n",
            "Epoch 9/100, Train Loss: 0.4001, Validation Loss: 0.3685\n",
            "Epoch 10/100, Train Loss: 0.3837, Validation Loss: 0.3643\n",
            "Epoch 11/100, Train Loss: 0.3753, Validation Loss: 0.3804\n",
            "Epoch 12/100, Train Loss: 0.3607, Validation Loss: 0.4203\n",
            "Epoch 13/100, Train Loss: 0.3543, Validation Loss: 0.3933\n",
            "Epoch 14/100, Train Loss: 0.3484, Validation Loss: 0.4097\n",
            "Epoch 15/100, Train Loss: 0.3405, Validation Loss: 0.4502\n",
            "Epoch 16/100, Train Loss: 0.3356, Validation Loss: 0.4386\n",
            "Epoch 17/100, Train Loss: 0.3328, Validation Loss: 0.4656\n",
            "Epoch 18/100, Train Loss: 0.3308, Validation Loss: 0.4778\n",
            "Epoch 19/100, Train Loss: 0.3290, Validation Loss: 0.4731\n",
            "Epoch 20/100, Train Loss: 0.3283, Validation Loss: 0.4802\n",
            "Epoch 21/100, Train Loss: 0.3275, Validation Loss: 0.4951\n",
            "Epoch 22/100, Train Loss: 0.3270, Validation Loss: 0.4944\n",
            "Epoch 23/100, Train Loss: 0.3266, Validation Loss: 0.5086\n",
            "Epoch 24/100, Train Loss: 0.3263, Validation Loss: 0.5152\n",
            "Epoch 25/100, Train Loss: 0.3260, Validation Loss: 0.5258\n",
            "Epoch 26/100, Train Loss: 0.3250, Validation Loss: 0.5268\n",
            "Epoch 27/100, Train Loss: 0.3246, Validation Loss: 0.5337\n",
            "Epoch 28/100, Train Loss: 0.3244, Validation Loss: 0.5385\n",
            "Epoch 29/100, Train Loss: 0.3243, Validation Loss: 0.5449\n",
            "Epoch 30/100, Train Loss: 0.3242, Validation Loss: 0.5519\n",
            "Epoch 31/100, Train Loss: 0.3240, Validation Loss: 0.5552\n",
            "Epoch 32/100, Train Loss: 0.3240, Validation Loss: 0.5582\n",
            "Epoch 33/100, Train Loss: 0.3239, Validation Loss: 0.5629\n",
            "Epoch 34/100, Train Loss: 0.3238, Validation Loss: 0.5664\n",
            "Epoch 35/100, Train Loss: 0.3237, Validation Loss: 0.5729\n",
            "Epoch 36/100, Train Loss: 0.3237, Validation Loss: 0.5747\n",
            "Epoch 37/100, Train Loss: 0.3236, Validation Loss: 0.5775\n",
            "Epoch 38/100, Train Loss: 0.3236, Validation Loss: 0.5802\n",
            "Epoch 39/100, Train Loss: 0.3235, Validation Loss: 0.5847\n",
            "Epoch 40/100, Train Loss: 0.3235, Validation Loss: 0.5875\n",
            "Epoch 41/100, Train Loss: 0.3234, Validation Loss: 0.5920\n",
            "Epoch 42/100, Train Loss: 0.3234, Validation Loss: 0.5938\n",
            "Epoch 43/100, Train Loss: 0.3234, Validation Loss: 0.5972\n",
            "Epoch 44/100, Train Loss: 0.3234, Validation Loss: 0.5994\n",
            "Epoch 45/100, Train Loss: 0.3233, Validation Loss: 0.6030\n",
            "Epoch 46/100, Train Loss: 0.3233, Validation Loss: 0.6051\n",
            "Epoch 47/100, Train Loss: 0.3233, Validation Loss: 0.6083\n",
            "Epoch 48/100, Train Loss: 0.3233, Validation Loss: 0.6102\n",
            "Epoch 49/100, Train Loss: 0.3232, Validation Loss: 0.6124\n",
            "Epoch 50/100, Train Loss: 0.3232, Validation Loss: 0.6140\n",
            "Epoch 51/100, Train Loss: 0.3232, Validation Loss: 0.6178\n",
            "Epoch 52/100, Train Loss: 0.3232, Validation Loss: 0.6198\n",
            "Epoch 53/100, Train Loss: 0.3232, Validation Loss: 0.6218\n",
            "Epoch 54/100, Train Loss: 0.3231, Validation Loss: 0.6230\n",
            "Epoch 55/100, Train Loss: 0.3231, Validation Loss: 0.6257\n",
            "Epoch 56/100, Train Loss: 0.3231, Validation Loss: 0.6282\n",
            "Epoch 57/100, Train Loss: 0.3231, Validation Loss: 0.6293\n",
            "Epoch 58/100, Train Loss: 0.3231, Validation Loss: 0.6309\n",
            "Epoch 59/100, Train Loss: 0.3231, Validation Loss: 0.6335\n",
            "Epoch 60/100, Train Loss: 0.3231, Validation Loss: 0.6348\n",
            "Epoch 61/100, Train Loss: 0.3230, Validation Loss: 0.6367\n",
            "Epoch 62/100, Train Loss: 0.3230, Validation Loss: 0.6363\n",
            "Epoch 63/100, Train Loss: 0.3230, Validation Loss: 0.6374\n",
            "Epoch 64/100, Train Loss: 0.3230, Validation Loss: 0.6369\n",
            "Epoch 65/100, Train Loss: 0.3225, Validation Loss: 0.6400\n",
            "Epoch 66/100, Train Loss: 0.3224, Validation Loss: 0.6453\n",
            "Epoch 67/100, Train Loss: 0.3224, Validation Loss: 0.6471\n",
            "Epoch 68/100, Train Loss: 0.3224, Validation Loss: 0.6494\n",
            "Epoch 69/100, Train Loss: 0.3224, Validation Loss: 0.6519\n",
            "Epoch 70/100, Train Loss: 0.3224, Validation Loss: 0.6532\n",
            "Epoch 71/100, Train Loss: 0.3224, Validation Loss: 0.6552\n",
            "Epoch 72/100, Train Loss: 0.3224, Validation Loss: 0.6563\n",
            "Epoch 73/100, Train Loss: 0.3223, Validation Loss: 0.6575\n",
            "Epoch 74/100, Train Loss: 0.3223, Validation Loss: 0.6604\n",
            "Epoch 75/100, Train Loss: 0.3223, Validation Loss: 0.6616\n",
            "Epoch 76/100, Train Loss: 0.3223, Validation Loss: 0.6628\n",
            "Epoch 77/100, Train Loss: 0.3223, Validation Loss: 0.6645\n",
            "Epoch 78/100, Train Loss: 0.3223, Validation Loss: 0.6649\n",
            "Epoch 79/100, Train Loss: 0.3223, Validation Loss: 0.6667\n",
            "Epoch 80/100, Train Loss: 0.3223, Validation Loss: 0.6677\n",
            "Epoch 81/100, Train Loss: 0.3223, Validation Loss: 0.6692\n",
            "Epoch 82/100, Train Loss: 0.3223, Validation Loss: 0.6711\n",
            "Epoch 83/100, Train Loss: 0.3223, Validation Loss: 0.6726\n",
            "Epoch 84/100, Train Loss: 0.3223, Validation Loss: 0.6740\n",
            "Epoch 85/100, Train Loss: 0.3223, Validation Loss: 0.6750\n",
            "Epoch 86/100, Train Loss: 0.3223, Validation Loss: 0.6758\n",
            "Epoch 87/100, Train Loss: 0.3223, Validation Loss: 0.6770\n",
            "Epoch 88/100, Train Loss: 0.3223, Validation Loss: 0.6789\n",
            "Epoch 89/100, Train Loss: 0.3223, Validation Loss: 0.6800\n",
            "Epoch 90/100, Train Loss: 0.3223, Validation Loss: 0.6810\n",
            "Epoch 91/100, Train Loss: 0.3222, Validation Loss: 0.6821\n",
            "Epoch 92/100, Train Loss: 0.3222, Validation Loss: 0.6838\n",
            "Epoch 93/100, Train Loss: 0.3222, Validation Loss: 0.6843\n",
            "Epoch 94/100, Train Loss: 0.3222, Validation Loss: 0.6852\n",
            "Epoch 95/100, Train Loss: 0.3222, Validation Loss: 0.6869\n",
            "Epoch 96/100, Train Loss: 0.3222, Validation Loss: 0.6879\n",
            "Epoch 97/100, Train Loss: 0.3222, Validation Loss: 0.6891\n",
            "Epoch 98/100, Train Loss: 0.3222, Validation Loss: 0.6896\n",
            "Epoch 99/100, Train Loss: 0.3222, Validation Loss: 0.6910\n",
            "Epoch 100/100, Train Loss: 0.3222, Validation Loss: 0.6917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6694, Validation Loss: 0.5971\n",
            "Epoch 2/100, Train Loss: 0.5978, Validation Loss: 0.4992\n",
            "Epoch 3/100, Train Loss: 0.5408, Validation Loss: 0.4544\n",
            "Epoch 4/100, Train Loss: 0.4973, Validation Loss: 0.4231\n",
            "Epoch 5/100, Train Loss: 0.4664, Validation Loss: 0.4444\n",
            "Epoch 6/100, Train Loss: 0.4403, Validation Loss: 0.4383\n",
            "Epoch 7/100, Train Loss: 0.4180, Validation Loss: 0.4299\n",
            "Epoch 8/100, Train Loss: 0.4041, Validation Loss: 0.4413\n",
            "Epoch 9/100, Train Loss: 0.3854, Validation Loss: 0.4408\n",
            "Epoch 10/100, Train Loss: 0.3707, Validation Loss: 0.4611\n",
            "Epoch 11/100, Train Loss: 0.3608, Validation Loss: 0.4613\n",
            "Epoch 12/100, Train Loss: 0.3550, Validation Loss: 0.4818\n",
            "Epoch 13/100, Train Loss: 0.3480, Validation Loss: 0.5263\n",
            "Epoch 14/100, Train Loss: 0.3408, Validation Loss: 0.5876\n",
            "Epoch 15/100, Train Loss: 0.3359, Validation Loss: 0.5656\n",
            "Epoch 16/100, Train Loss: 0.3326, Validation Loss: 0.5511\n",
            "Epoch 17/100, Train Loss: 0.3311, Validation Loss: 0.5536\n",
            "Epoch 18/100, Train Loss: 0.3301, Validation Loss: 0.5649\n",
            "Epoch 19/100, Train Loss: 0.3282, Validation Loss: 0.5829\n",
            "Epoch 20/100, Train Loss: 0.3276, Validation Loss: 0.5850\n",
            "Epoch 21/100, Train Loss: 0.3269, Validation Loss: 0.5970\n",
            "Epoch 22/100, Train Loss: 0.3266, Validation Loss: 0.6048\n",
            "Epoch 23/100, Train Loss: 0.3263, Validation Loss: 0.6115\n",
            "Epoch 24/100, Train Loss: 0.3260, Validation Loss: 0.6181\n",
            "Epoch 25/100, Train Loss: 0.3259, Validation Loss: 0.6220\n",
            "Epoch 26/100, Train Loss: 0.3255, Validation Loss: 0.6258\n",
            "Epoch 27/100, Train Loss: 0.3252, Validation Loss: 0.6345\n",
            "Epoch 28/100, Train Loss: 0.3247, Validation Loss: 0.6440\n",
            "Epoch 29/100, Train Loss: 0.3243, Validation Loss: 0.6504\n",
            "Epoch 30/100, Train Loss: 0.3242, Validation Loss: 0.6574\n",
            "Epoch 31/100, Train Loss: 0.3241, Validation Loss: 0.6640\n",
            "Epoch 32/100, Train Loss: 0.3240, Validation Loss: 0.6678\n",
            "Epoch 33/100, Train Loss: 0.3239, Validation Loss: 0.6706\n",
            "Epoch 34/100, Train Loss: 0.3238, Validation Loss: 0.6755\n",
            "Epoch 35/100, Train Loss: 0.3237, Validation Loss: 0.6819\n",
            "Epoch 36/100, Train Loss: 0.3236, Validation Loss: 0.6861\n",
            "Epoch 37/100, Train Loss: 0.3236, Validation Loss: 0.6923\n",
            "Epoch 38/100, Train Loss: 0.3234, Validation Loss: 0.6985\n",
            "Epoch 39/100, Train Loss: 0.3229, Validation Loss: 0.6994\n",
            "Epoch 40/100, Train Loss: 0.3229, Validation Loss: 0.7025\n",
            "Epoch 41/100, Train Loss: 0.3228, Validation Loss: 0.7067\n",
            "Epoch 42/100, Train Loss: 0.3228, Validation Loss: 0.7094\n",
            "Epoch 43/100, Train Loss: 0.3228, Validation Loss: 0.7141\n",
            "Epoch 44/100, Train Loss: 0.3227, Validation Loss: 0.7156\n",
            "Epoch 45/100, Train Loss: 0.3227, Validation Loss: 0.7193\n",
            "Epoch 46/100, Train Loss: 0.3227, Validation Loss: 0.7223\n",
            "Epoch 47/100, Train Loss: 0.3226, Validation Loss: 0.7249\n",
            "Epoch 48/100, Train Loss: 0.3226, Validation Loss: 0.7289\n",
            "Epoch 49/100, Train Loss: 0.3226, Validation Loss: 0.7301\n",
            "Epoch 50/100, Train Loss: 0.3226, Validation Loss: 0.7336\n",
            "Epoch 51/100, Train Loss: 0.3226, Validation Loss: 0.7362\n",
            "Epoch 52/100, Train Loss: 0.3225, Validation Loss: 0.7387\n",
            "Epoch 53/100, Train Loss: 0.3225, Validation Loss: 0.7415\n",
            "Epoch 54/100, Train Loss: 0.3225, Validation Loss: 0.7435\n",
            "Epoch 55/100, Train Loss: 0.3225, Validation Loss: 0.7467\n",
            "Epoch 56/100, Train Loss: 0.3225, Validation Loss: 0.7485\n",
            "Epoch 57/100, Train Loss: 0.3225, Validation Loss: 0.7509\n",
            "Epoch 58/100, Train Loss: 0.3225, Validation Loss: 0.7535\n",
            "Epoch 59/100, Train Loss: 0.3224, Validation Loss: 0.7555\n",
            "Epoch 60/100, Train Loss: 0.3224, Validation Loss: 0.7577\n",
            "Epoch 61/100, Train Loss: 0.3224, Validation Loss: 0.7598\n",
            "Epoch 62/100, Train Loss: 0.3224, Validation Loss: 0.7624\n",
            "Epoch 63/100, Train Loss: 0.3224, Validation Loss: 0.7639\n",
            "Epoch 64/100, Train Loss: 0.3224, Validation Loss: 0.7659\n",
            "Epoch 65/100, Train Loss: 0.3224, Validation Loss: 0.7681\n",
            "Epoch 66/100, Train Loss: 0.3224, Validation Loss: 0.7701\n",
            "Epoch 67/100, Train Loss: 0.3224, Validation Loss: 0.7723\n",
            "Epoch 68/100, Train Loss: 0.3224, Validation Loss: 0.7740\n",
            "Epoch 69/100, Train Loss: 0.3223, Validation Loss: 0.7753\n",
            "Epoch 70/100, Train Loss: 0.3223, Validation Loss: 0.7779\n",
            "Epoch 71/100, Train Loss: 0.3223, Validation Loss: 0.7791\n",
            "Epoch 72/100, Train Loss: 0.3223, Validation Loss: 0.7813\n",
            "Epoch 73/100, Train Loss: 0.3223, Validation Loss: 0.7828\n",
            "Epoch 74/100, Train Loss: 0.3223, Validation Loss: 0.7849\n",
            "Epoch 75/100, Train Loss: 0.3223, Validation Loss: 0.7862\n",
            "Epoch 76/100, Train Loss: 0.3223, Validation Loss: 0.7878\n",
            "Epoch 77/100, Train Loss: 0.3223, Validation Loss: 0.7898\n",
            "Epoch 78/100, Train Loss: 0.3223, Validation Loss: 0.7912\n",
            "Epoch 79/100, Train Loss: 0.3223, Validation Loss: 0.7923\n",
            "Epoch 80/100, Train Loss: 0.3223, Validation Loss: 0.7942\n",
            "Epoch 81/100, Train Loss: 0.3223, Validation Loss: 0.7953\n",
            "Epoch 82/100, Train Loss: 0.3223, Validation Loss: 0.7971\n",
            "Epoch 83/100, Train Loss: 0.3223, Validation Loss: 0.7988\n",
            "Epoch 84/100, Train Loss: 0.3223, Validation Loss: 0.8002\n",
            "Epoch 85/100, Train Loss: 0.3223, Validation Loss: 0.8017\n",
            "Epoch 86/100, Train Loss: 0.3223, Validation Loss: 0.8034\n",
            "Epoch 87/100, Train Loss: 0.3223, Validation Loss: 0.8044\n",
            "Epoch 88/100, Train Loss: 0.3222, Validation Loss: 0.8060\n",
            "Epoch 89/100, Train Loss: 0.3222, Validation Loss: 0.8071\n",
            "Epoch 90/100, Train Loss: 0.3222, Validation Loss: 0.8084\n",
            "Epoch 91/100, Train Loss: 0.3222, Validation Loss: 0.8098\n",
            "Epoch 92/100, Train Loss: 0.3222, Validation Loss: 0.8111\n",
            "Epoch 93/100, Train Loss: 0.3222, Validation Loss: 0.8125\n",
            "Epoch 94/100, Train Loss: 0.3222, Validation Loss: 0.8138\n",
            "Epoch 95/100, Train Loss: 0.3222, Validation Loss: 0.8150\n",
            "Epoch 96/100, Train Loss: 0.3222, Validation Loss: 0.8163\n",
            "Epoch 97/100, Train Loss: 0.3222, Validation Loss: 0.8177\n",
            "Epoch 98/100, Train Loss: 0.3222, Validation Loss: 0.8186\n",
            "Epoch 99/100, Train Loss: 0.3222, Validation Loss: 0.8199\n",
            "Epoch 100/100, Train Loss: 0.3222, Validation Loss: 0.8212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6706, Validation Loss: 0.5926\n",
            "Epoch 2/100, Train Loss: 0.5983, Validation Loss: 0.4926\n",
            "Epoch 3/100, Train Loss: 0.5334, Validation Loss: 0.4607\n",
            "Epoch 4/100, Train Loss: 0.4905, Validation Loss: 0.4661\n",
            "Epoch 5/100, Train Loss: 0.4591, Validation Loss: 0.4417\n",
            "Epoch 6/100, Train Loss: 0.4317, Validation Loss: 0.5023\n",
            "Epoch 7/100, Train Loss: 0.4095, Validation Loss: 0.4860\n",
            "Epoch 8/100, Train Loss: 0.3919, Validation Loss: 0.5258\n",
            "Epoch 9/100, Train Loss: 0.3773, Validation Loss: 0.5071\n",
            "Epoch 10/100, Train Loss: 0.3664, Validation Loss: 0.5626\n",
            "Epoch 11/100, Train Loss: 0.3536, Validation Loss: 0.5449\n",
            "Epoch 12/100, Train Loss: 0.3491, Validation Loss: 0.5500\n",
            "Epoch 13/100, Train Loss: 0.3428, Validation Loss: 0.5662\n",
            "Epoch 14/100, Train Loss: 0.3368, Validation Loss: 0.5787\n",
            "Epoch 15/100, Train Loss: 0.3329, Validation Loss: 0.5952\n",
            "Epoch 16/100, Train Loss: 0.3298, Validation Loss: 0.6215\n",
            "Epoch 17/100, Train Loss: 0.3272, Validation Loss: 0.6211\n",
            "Epoch 18/100, Train Loss: 0.3258, Validation Loss: 0.6466\n",
            "Epoch 19/100, Train Loss: 0.3248, Validation Loss: 0.6683\n",
            "Epoch 20/100, Train Loss: 0.3241, Validation Loss: 0.6890\n",
            "Epoch 21/100, Train Loss: 0.3236, Validation Loss: 0.6752\n",
            "Epoch 22/100, Train Loss: 0.3229, Validation Loss: 0.6872\n",
            "Epoch 23/100, Train Loss: 0.3226, Validation Loss: 0.6967\n",
            "Epoch 24/100, Train Loss: 0.3220, Validation Loss: 0.7086\n",
            "Epoch 25/100, Train Loss: 0.3217, Validation Loss: 0.7238\n",
            "Epoch 26/100, Train Loss: 0.3209, Validation Loss: 0.7239\n",
            "Epoch 27/100, Train Loss: 0.3207, Validation Loss: 0.7328\n",
            "Epoch 28/100, Train Loss: 0.3205, Validation Loss: 0.7389\n",
            "Epoch 29/100, Train Loss: 0.3204, Validation Loss: 0.7418\n",
            "Epoch 30/100, Train Loss: 0.3203, Validation Loss: 0.7472\n",
            "Epoch 31/100, Train Loss: 0.3202, Validation Loss: 0.7574\n",
            "Epoch 32/100, Train Loss: 0.3201, Validation Loss: 0.7614\n",
            "Epoch 33/100, Train Loss: 0.3200, Validation Loss: 0.7676\n",
            "Epoch 34/100, Train Loss: 0.3200, Validation Loss: 0.7732\n",
            "Epoch 35/100, Train Loss: 0.3199, Validation Loss: 0.7758\n",
            "Epoch 36/100, Train Loss: 0.3199, Validation Loss: 0.7843\n",
            "Epoch 37/100, Train Loss: 0.3198, Validation Loss: 0.7830\n",
            "Epoch 38/100, Train Loss: 0.3198, Validation Loss: 0.7938\n",
            "Epoch 39/100, Train Loss: 0.3197, Validation Loss: 0.7964\n",
            "Epoch 40/100, Train Loss: 0.3197, Validation Loss: 0.8126\n",
            "Epoch 41/100, Train Loss: 0.3192, Validation Loss: 0.8105\n",
            "Epoch 42/100, Train Loss: 0.3191, Validation Loss: 0.8165\n",
            "Epoch 43/100, Train Loss: 0.3190, Validation Loss: 0.8155\n",
            "Epoch 44/100, Train Loss: 0.3190, Validation Loss: 0.8180\n",
            "Epoch 45/100, Train Loss: 0.3190, Validation Loss: 0.8240\n",
            "Epoch 46/100, Train Loss: 0.3189, Validation Loss: 0.8250\n",
            "Epoch 47/100, Train Loss: 0.3189, Validation Loss: 0.8302\n",
            "Epoch 48/100, Train Loss: 0.3189, Validation Loss: 0.8331\n",
            "Epoch 49/100, Train Loss: 0.3189, Validation Loss: 0.8364\n",
            "Epoch 50/100, Train Loss: 0.3188, Validation Loss: 0.8372\n",
            "Epoch 51/100, Train Loss: 0.3188, Validation Loss: 0.8403\n",
            "Epoch 52/100, Train Loss: 0.3188, Validation Loss: 0.8446\n",
            "Epoch 53/100, Train Loss: 0.3188, Validation Loss: 0.8476\n",
            "Epoch 54/100, Train Loss: 0.3188, Validation Loss: 0.8483\n",
            "Epoch 55/100, Train Loss: 0.3188, Validation Loss: 0.8525\n",
            "Epoch 56/100, Train Loss: 0.3187, Validation Loss: 0.8555\n",
            "Epoch 57/100, Train Loss: 0.3187, Validation Loss: 0.8574\n",
            "Epoch 58/100, Train Loss: 0.3187, Validation Loss: 0.8593\n",
            "Epoch 59/100, Train Loss: 0.3187, Validation Loss: 0.8621\n",
            "Epoch 60/100, Train Loss: 0.3187, Validation Loss: 0.8638\n",
            "Epoch 61/100, Train Loss: 0.3187, Validation Loss: 0.8656\n",
            "Epoch 62/100, Train Loss: 0.3187, Validation Loss: 0.8699\n",
            "Epoch 63/100, Train Loss: 0.3187, Validation Loss: 0.8723\n",
            "Epoch 64/100, Train Loss: 0.3186, Validation Loss: 0.8733\n",
            "Epoch 65/100, Train Loss: 0.3186, Validation Loss: 0.8760\n",
            "Epoch 66/100, Train Loss: 0.3186, Validation Loss: 0.8782\n",
            "Epoch 67/100, Train Loss: 0.3186, Validation Loss: 0.8790\n",
            "Epoch 68/100, Train Loss: 0.3186, Validation Loss: 0.8836\n",
            "Epoch 69/100, Train Loss: 0.3186, Validation Loss: 0.8837\n",
            "Epoch 70/100, Train Loss: 0.3186, Validation Loss: 0.8863\n",
            "Epoch 71/100, Train Loss: 0.3186, Validation Loss: 0.8885\n",
            "Epoch 72/100, Train Loss: 0.3186, Validation Loss: 0.8900\n",
            "Epoch 73/100, Train Loss: 0.3186, Validation Loss: 0.8923\n",
            "Epoch 74/100, Train Loss: 0.3186, Validation Loss: 0.8950\n",
            "Epoch 75/100, Train Loss: 0.3186, Validation Loss: 0.8968\n",
            "Epoch 76/100, Train Loss: 0.3186, Validation Loss: 0.8973\n",
            "Epoch 77/100, Train Loss: 0.3185, Validation Loss: 0.8990\n",
            "Epoch 78/100, Train Loss: 0.3185, Validation Loss: 0.9018\n",
            "Epoch 79/100, Train Loss: 0.3185, Validation Loss: 0.9027\n",
            "Epoch 80/100, Train Loss: 0.3185, Validation Loss: 0.9047\n",
            "Epoch 81/100, Train Loss: 0.3185, Validation Loss: 0.9066\n",
            "Epoch 82/100, Train Loss: 0.3185, Validation Loss: 0.9069\n",
            "Epoch 83/100, Train Loss: 0.3185, Validation Loss: 0.9105\n",
            "Epoch 84/100, Train Loss: 0.3185, Validation Loss: 0.9128\n",
            "Epoch 85/100, Train Loss: 0.3185, Validation Loss: 0.9118\n",
            "Epoch 86/100, Train Loss: 0.3185, Validation Loss: 0.9140\n",
            "Epoch 87/100, Train Loss: 0.3185, Validation Loss: 0.9172\n",
            "Epoch 88/100, Train Loss: 0.3185, Validation Loss: 0.9181\n",
            "Epoch 89/100, Train Loss: 0.3185, Validation Loss: 0.9194\n",
            "Epoch 90/100, Train Loss: 0.3185, Validation Loss: 0.9211\n",
            "Epoch 91/100, Train Loss: 0.3185, Validation Loss: 0.9215\n",
            "Epoch 92/100, Train Loss: 0.3185, Validation Loss: 0.9245\n",
            "Epoch 93/100, Train Loss: 0.3185, Validation Loss: 0.9256\n",
            "Epoch 94/100, Train Loss: 0.3185, Validation Loss: 0.9263\n",
            "Epoch 95/100, Train Loss: 0.3185, Validation Loss: 0.9272\n",
            "Epoch 96/100, Train Loss: 0.3185, Validation Loss: 0.9292\n",
            "Epoch 97/100, Train Loss: 0.3185, Validation Loss: 0.9300\n",
            "Epoch 98/100, Train Loss: 0.3185, Validation Loss: 0.9313\n",
            "Epoch 99/100, Train Loss: 0.3185, Validation Loss: 0.9319\n",
            "Epoch 100/100, Train Loss: 0.3185, Validation Loss: 0.9336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6780, Validation Loss: 0.6196\n",
            "Epoch 2/100, Train Loss: 0.6227, Validation Loss: 0.5176\n",
            "Epoch 3/100, Train Loss: 0.5623, Validation Loss: 0.4523\n",
            "Epoch 4/100, Train Loss: 0.5139, Validation Loss: 0.4303\n",
            "Epoch 5/100, Train Loss: 0.4797, Validation Loss: 0.4283\n",
            "Epoch 6/100, Train Loss: 0.4508, Validation Loss: 0.4537\n",
            "Epoch 7/100, Train Loss: 0.4286, Validation Loss: 0.4492\n",
            "Epoch 8/100, Train Loss: 0.4065, Validation Loss: 0.4618\n",
            "Epoch 9/100, Train Loss: 0.3890, Validation Loss: 0.4874\n",
            "Epoch 10/100, Train Loss: 0.3721, Validation Loss: 0.5666\n",
            "Epoch 11/100, Train Loss: 0.3597, Validation Loss: 0.5715\n",
            "Epoch 12/100, Train Loss: 0.3515, Validation Loss: 0.6027\n",
            "Epoch 13/100, Train Loss: 0.3436, Validation Loss: 0.5939\n",
            "Epoch 14/100, Train Loss: 0.3362, Validation Loss: 0.6234\n",
            "Epoch 15/100, Train Loss: 0.3315, Validation Loss: 0.6726\n",
            "Epoch 16/100, Train Loss: 0.3290, Validation Loss: 0.7100\n",
            "Epoch 17/100, Train Loss: 0.3274, Validation Loss: 0.7238\n",
            "Epoch 18/100, Train Loss: 0.3265, Validation Loss: 0.7305\n",
            "Epoch 19/100, Train Loss: 0.3258, Validation Loss: 0.7566\n",
            "Epoch 20/100, Train Loss: 0.3253, Validation Loss: 0.7729\n",
            "Epoch 21/100, Train Loss: 0.3249, Validation Loss: 0.7858\n",
            "Epoch 22/100, Train Loss: 0.3246, Validation Loss: 0.7946\n",
            "Epoch 23/100, Train Loss: 0.3243, Validation Loss: 0.8019\n",
            "Epoch 24/100, Train Loss: 0.3240, Validation Loss: 0.8095\n",
            "Epoch 25/100, Train Loss: 0.3235, Validation Loss: 0.8176\n",
            "Epoch 26/100, Train Loss: 0.3230, Validation Loss: 0.8295\n",
            "Epoch 27/100, Train Loss: 0.3226, Validation Loss: 0.8364\n",
            "Epoch 28/100, Train Loss: 0.3224, Validation Loss: 0.8468\n",
            "Epoch 29/100, Train Loss: 0.3221, Validation Loss: 0.8592\n",
            "Epoch 30/100, Train Loss: 0.3217, Validation Loss: 0.8560\n",
            "Epoch 31/100, Train Loss: 0.3216, Validation Loss: 0.8680\n",
            "Epoch 32/100, Train Loss: 0.3215, Validation Loss: 0.8647\n",
            "Epoch 33/100, Train Loss: 0.3213, Validation Loss: 0.8641\n",
            "Epoch 34/100, Train Loss: 0.3209, Validation Loss: 0.8810\n",
            "Epoch 35/100, Train Loss: 0.3207, Validation Loss: 0.8850\n",
            "Epoch 36/100, Train Loss: 0.3206, Validation Loss: 0.9090\n",
            "Epoch 37/100, Train Loss: 0.3201, Validation Loss: 0.9031\n",
            "Epoch 38/100, Train Loss: 0.3199, Validation Loss: 0.9059\n",
            "Epoch 39/100, Train Loss: 0.3198, Validation Loss: 0.9164\n",
            "Epoch 40/100, Train Loss: 0.3198, Validation Loss: 0.9210\n",
            "Epoch 41/100, Train Loss: 0.3197, Validation Loss: 0.9274\n",
            "Epoch 42/100, Train Loss: 0.3197, Validation Loss: 0.9337\n",
            "Epoch 43/100, Train Loss: 0.3197, Validation Loss: 0.9379\n",
            "Epoch 44/100, Train Loss: 0.3196, Validation Loss: 0.9417\n",
            "Epoch 45/100, Train Loss: 0.3196, Validation Loss: 0.9428\n",
            "Epoch 46/100, Train Loss: 0.3196, Validation Loss: 0.9524\n",
            "Epoch 47/100, Train Loss: 0.3195, Validation Loss: 0.9559\n",
            "Epoch 48/100, Train Loss: 0.3195, Validation Loss: 0.9597\n",
            "Epoch 49/100, Train Loss: 0.3195, Validation Loss: 0.9649\n",
            "Epoch 50/100, Train Loss: 0.3195, Validation Loss: 0.9636\n",
            "Epoch 51/100, Train Loss: 0.3194, Validation Loss: 0.9695\n",
            "Epoch 52/100, Train Loss: 0.3194, Validation Loss: 0.9771\n",
            "Epoch 53/100, Train Loss: 0.3194, Validation Loss: 0.9788\n",
            "Epoch 54/100, Train Loss: 0.3194, Validation Loss: 0.9833\n",
            "Epoch 55/100, Train Loss: 0.3194, Validation Loss: 0.9888\n",
            "Epoch 56/100, Train Loss: 0.3194, Validation Loss: 0.9895\n",
            "Epoch 57/100, Train Loss: 0.3193, Validation Loss: 0.9914\n",
            "Epoch 58/100, Train Loss: 0.3193, Validation Loss: 0.9958\n",
            "Epoch 59/100, Train Loss: 0.3193, Validation Loss: 0.9973\n",
            "Epoch 60/100, Train Loss: 0.3193, Validation Loss: 1.0009\n",
            "Epoch 61/100, Train Loss: 0.3193, Validation Loss: 1.0056\n",
            "Epoch 62/100, Train Loss: 0.3193, Validation Loss: 1.0097\n",
            "Epoch 63/100, Train Loss: 0.3193, Validation Loss: 1.0124\n",
            "Epoch 64/100, Train Loss: 0.3193, Validation Loss: 1.0130\n",
            "Epoch 65/100, Train Loss: 0.3193, Validation Loss: 1.0179\n",
            "Epoch 66/100, Train Loss: 0.3193, Validation Loss: 1.0197\n",
            "Epoch 67/100, Train Loss: 0.3192, Validation Loss: 1.0232\n",
            "Epoch 68/100, Train Loss: 0.3192, Validation Loss: 1.0261\n",
            "Epoch 69/100, Train Loss: 0.3192, Validation Loss: 1.0286\n",
            "Epoch 70/100, Train Loss: 0.3192, Validation Loss: 1.0309\n",
            "Epoch 71/100, Train Loss: 0.3192, Validation Loss: 1.0339\n",
            "Epoch 72/100, Train Loss: 0.3192, Validation Loss: 1.0354\n",
            "Epoch 73/100, Train Loss: 0.3192, Validation Loss: 1.0378\n",
            "Epoch 74/100, Train Loss: 0.3192, Validation Loss: 1.0403\n",
            "Epoch 75/100, Train Loss: 0.3192, Validation Loss: 1.0435\n",
            "Epoch 76/100, Train Loss: 0.3192, Validation Loss: 1.0440\n",
            "Epoch 77/100, Train Loss: 0.3192, Validation Loss: 1.0482\n",
            "Epoch 78/100, Train Loss: 0.3192, Validation Loss: 1.0516\n",
            "Epoch 79/100, Train Loss: 0.3192, Validation Loss: 1.0528\n",
            "Epoch 80/100, Train Loss: 0.3192, Validation Loss: 1.0564\n",
            "Epoch 81/100, Train Loss: 0.3192, Validation Loss: 1.0572\n",
            "Epoch 82/100, Train Loss: 0.3191, Validation Loss: 1.0584\n",
            "Epoch 83/100, Train Loss: 0.3191, Validation Loss: 1.0614\n",
            "Epoch 84/100, Train Loss: 0.3191, Validation Loss: 1.0647\n",
            "Epoch 85/100, Train Loss: 0.3191, Validation Loss: 1.0660\n",
            "Epoch 86/100, Train Loss: 0.3191, Validation Loss: 1.0685\n",
            "Epoch 87/100, Train Loss: 0.3191, Validation Loss: 1.0706\n",
            "Epoch 88/100, Train Loss: 0.3191, Validation Loss: 1.0722\n",
            "Epoch 89/100, Train Loss: 0.3191, Validation Loss: 1.0745\n",
            "Epoch 90/100, Train Loss: 0.3191, Validation Loss: 1.0760\n",
            "Epoch 91/100, Train Loss: 0.3191, Validation Loss: 1.0769\n",
            "Epoch 92/100, Train Loss: 0.3191, Validation Loss: 1.0795\n",
            "Epoch 93/100, Train Loss: 0.3191, Validation Loss: 1.0818\n",
            "Epoch 94/100, Train Loss: 0.3191, Validation Loss: 1.0836\n",
            "Epoch 95/100, Train Loss: 0.3191, Validation Loss: 1.0855\n",
            "Epoch 96/100, Train Loss: 0.3191, Validation Loss: 1.0869\n",
            "Epoch 97/100, Train Loss: 0.3191, Validation Loss: 1.0892\n",
            "Epoch 98/100, Train Loss: 0.3191, Validation Loss: 1.0908\n",
            "Epoch 99/100, Train Loss: 0.3191, Validation Loss: 1.0926\n",
            "Epoch 100/100, Train Loss: 0.3191, Validation Loss: 1.0940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6729, Validation Loss: 0.6057\n",
            "Epoch 2/100, Train Loss: 0.6158, Validation Loss: 0.5260\n",
            "Epoch 3/100, Train Loss: 0.5547, Validation Loss: 0.4878\n",
            "Epoch 4/100, Train Loss: 0.5126, Validation Loss: 0.4570\n",
            "Epoch 5/100, Train Loss: 0.4803, Validation Loss: 0.4413\n",
            "Epoch 6/100, Train Loss: 0.4572, Validation Loss: 0.4557\n",
            "Epoch 7/100, Train Loss: 0.4351, Validation Loss: 0.4476\n",
            "Epoch 8/100, Train Loss: 0.4152, Validation Loss: 0.4505\n",
            "Epoch 9/100, Train Loss: 0.3996, Validation Loss: 0.4589\n",
            "Epoch 10/100, Train Loss: 0.3871, Validation Loss: 0.5041\n",
            "Epoch 11/100, Train Loss: 0.3722, Validation Loss: 0.4813\n",
            "Epoch 12/100, Train Loss: 0.3616, Validation Loss: 0.4974\n",
            "Epoch 13/100, Train Loss: 0.3537, Validation Loss: 0.5909\n",
            "Epoch 14/100, Train Loss: 0.3441, Validation Loss: 0.5475\n",
            "Epoch 15/100, Train Loss: 0.3403, Validation Loss: 0.5808\n",
            "Epoch 16/100, Train Loss: 0.3352, Validation Loss: 0.5981\n",
            "Epoch 17/100, Train Loss: 0.3324, Validation Loss: 0.5916\n",
            "Epoch 18/100, Train Loss: 0.3296, Validation Loss: 0.5945\n",
            "Epoch 19/100, Train Loss: 0.3278, Validation Loss: 0.6130\n",
            "Epoch 20/100, Train Loss: 0.3268, Validation Loss: 0.6299\n",
            "Epoch 21/100, Train Loss: 0.3258, Validation Loss: 0.6274\n",
            "Epoch 22/100, Train Loss: 0.3252, Validation Loss: 0.6498\n",
            "Epoch 23/100, Train Loss: 0.3245, Validation Loss: 0.6549\n",
            "Epoch 24/100, Train Loss: 0.3240, Validation Loss: 0.6670\n",
            "Epoch 25/100, Train Loss: 0.3237, Validation Loss: 0.6755\n",
            "Epoch 26/100, Train Loss: 0.3235, Validation Loss: 0.6836\n",
            "Epoch 27/100, Train Loss: 0.3233, Validation Loss: 0.7070\n",
            "Epoch 28/100, Train Loss: 0.3231, Validation Loss: 0.7032\n",
            "Epoch 29/100, Train Loss: 0.3230, Validation Loss: 0.7121\n",
            "Epoch 30/100, Train Loss: 0.3229, Validation Loss: 0.7052\n",
            "Epoch 31/100, Train Loss: 0.3227, Validation Loss: 0.7047\n",
            "Epoch 32/100, Train Loss: 0.3226, Validation Loss: 0.7102\n",
            "Epoch 33/100, Train Loss: 0.3226, Validation Loss: 0.7308\n",
            "Epoch 34/100, Train Loss: 0.3222, Validation Loss: 0.7272\n",
            "Epoch 35/100, Train Loss: 0.3220, Validation Loss: 0.7379\n",
            "Epoch 36/100, Train Loss: 0.3219, Validation Loss: 0.7470\n",
            "Epoch 37/100, Train Loss: 0.3218, Validation Loss: 0.7377\n",
            "Epoch 38/100, Train Loss: 0.3217, Validation Loss: 0.7460\n",
            "Epoch 39/100, Train Loss: 0.3217, Validation Loss: 0.7502\n",
            "Epoch 40/100, Train Loss: 0.3214, Validation Loss: 0.7585\n",
            "Epoch 41/100, Train Loss: 0.3211, Validation Loss: 0.7586\n",
            "Epoch 42/100, Train Loss: 0.3210, Validation Loss: 0.7725\n",
            "Epoch 43/100, Train Loss: 0.3210, Validation Loss: 0.7759\n",
            "Epoch 44/100, Train Loss: 0.3209, Validation Loss: 0.7822\n",
            "Epoch 45/100, Train Loss: 0.3209, Validation Loss: 0.7823\n",
            "Epoch 46/100, Train Loss: 0.3209, Validation Loss: 0.7843\n",
            "Epoch 47/100, Train Loss: 0.3208, Validation Loss: 0.7924\n",
            "Epoch 48/100, Train Loss: 0.3208, Validation Loss: 0.7949\n",
            "Epoch 49/100, Train Loss: 0.3208, Validation Loss: 0.8008\n",
            "Epoch 50/100, Train Loss: 0.3208, Validation Loss: 0.8011\n",
            "Epoch 51/100, Train Loss: 0.3207, Validation Loss: 0.8058\n",
            "Epoch 52/100, Train Loss: 0.3207, Validation Loss: 0.8061\n",
            "Epoch 53/100, Train Loss: 0.3207, Validation Loss: 0.8153\n",
            "Epoch 54/100, Train Loss: 0.3207, Validation Loss: 0.8166\n",
            "Epoch 55/100, Train Loss: 0.3207, Validation Loss: 0.8185\n",
            "Epoch 56/100, Train Loss: 0.3206, Validation Loss: 0.8241\n",
            "Epoch 57/100, Train Loss: 0.3206, Validation Loss: 0.8252\n",
            "Epoch 58/100, Train Loss: 0.3206, Validation Loss: 0.8282\n",
            "Epoch 59/100, Train Loss: 0.3206, Validation Loss: 0.8312\n",
            "Epoch 60/100, Train Loss: 0.3206, Validation Loss: 0.8298\n",
            "Epoch 61/100, Train Loss: 0.3206, Validation Loss: 0.8383\n",
            "Epoch 62/100, Train Loss: 0.3206, Validation Loss: 0.8366\n",
            "Epoch 63/100, Train Loss: 0.3206, Validation Loss: 0.8396\n",
            "Epoch 64/100, Train Loss: 0.3205, Validation Loss: 0.8462\n",
            "Epoch 65/100, Train Loss: 0.3205, Validation Loss: 0.8462\n",
            "Epoch 66/100, Train Loss: 0.3205, Validation Loss: 0.8476\n",
            "Epoch 67/100, Train Loss: 0.3205, Validation Loss: 0.8496\n",
            "Epoch 68/100, Train Loss: 0.3205, Validation Loss: 0.8529\n",
            "Epoch 69/100, Train Loss: 0.3205, Validation Loss: 0.8554\n",
            "Epoch 70/100, Train Loss: 0.3205, Validation Loss: 0.8560\n",
            "Epoch 71/100, Train Loss: 0.3205, Validation Loss: 0.8611\n",
            "Epoch 72/100, Train Loss: 0.3205, Validation Loss: 0.8630\n",
            "Epoch 73/100, Train Loss: 0.3205, Validation Loss: 0.8636\n",
            "Epoch 74/100, Train Loss: 0.3205, Validation Loss: 0.8667\n",
            "Epoch 75/100, Train Loss: 0.3204, Validation Loss: 0.8695\n",
            "Epoch 76/100, Train Loss: 0.3204, Validation Loss: 0.8694\n",
            "Epoch 77/100, Train Loss: 0.3204, Validation Loss: 0.8727\n",
            "Epoch 78/100, Train Loss: 0.3204, Validation Loss: 0.8745\n",
            "Epoch 79/100, Train Loss: 0.3204, Validation Loss: 0.8775\n",
            "Epoch 80/100, Train Loss: 0.3204, Validation Loss: 0.8789\n",
            "Epoch 81/100, Train Loss: 0.3204, Validation Loss: 0.8808\n",
            "Epoch 82/100, Train Loss: 0.3204, Validation Loss: 0.8827\n",
            "Epoch 83/100, Train Loss: 0.3204, Validation Loss: 0.8842\n",
            "Epoch 84/100, Train Loss: 0.3204, Validation Loss: 0.8884\n",
            "Epoch 85/100, Train Loss: 0.3204, Validation Loss: 0.8883\n",
            "Epoch 86/100, Train Loss: 0.3204, Validation Loss: 0.8905\n",
            "Epoch 87/100, Train Loss: 0.3204, Validation Loss: 0.8911\n",
            "Epoch 88/100, Train Loss: 0.3204, Validation Loss: 0.8942\n",
            "Epoch 89/100, Train Loss: 0.3204, Validation Loss: 0.8969\n",
            "Epoch 90/100, Train Loss: 0.3204, Validation Loss: 0.8970\n",
            "Epoch 91/100, Train Loss: 0.3204, Validation Loss: 0.8983\n",
            "Epoch 92/100, Train Loss: 0.3204, Validation Loss: 0.9009\n",
            "Epoch 93/100, Train Loss: 0.3204, Validation Loss: 0.9022\n",
            "Epoch 94/100, Train Loss: 0.3204, Validation Loss: 0.9052\n",
            "Epoch 95/100, Train Loss: 0.3204, Validation Loss: 0.9068\n",
            "Epoch 96/100, Train Loss: 0.3204, Validation Loss: 0.9059\n",
            "Epoch 97/100, Train Loss: 0.3203, Validation Loss: 0.9083\n",
            "Epoch 98/100, Train Loss: 0.3203, Validation Loss: 0.9088\n",
            "Epoch 99/100, Train Loss: 0.3203, Validation Loss: 0.9109\n",
            "Epoch 100/100, Train Loss: 0.3203, Validation Loss: 0.9124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6776, Validation Loss: 0.6199\n",
            "Epoch 2/100, Train Loss: 0.6206, Validation Loss: 0.5235\n",
            "Epoch 3/100, Train Loss: 0.5629, Validation Loss: 0.4771\n",
            "Epoch 4/100, Train Loss: 0.5225, Validation Loss: 0.4673\n",
            "Epoch 5/100, Train Loss: 0.4913, Validation Loss: 0.4508\n",
            "Epoch 6/100, Train Loss: 0.4669, Validation Loss: 0.4270\n",
            "Epoch 7/100, Train Loss: 0.4470, Validation Loss: 0.4172\n",
            "Epoch 8/100, Train Loss: 0.4331, Validation Loss: 0.4434\n",
            "Epoch 9/100, Train Loss: 0.4176, Validation Loss: 0.5008\n",
            "Epoch 10/100, Train Loss: 0.4076, Validation Loss: 0.4547\n",
            "Epoch 11/100, Train Loss: 0.3928, Validation Loss: 0.5121\n",
            "Epoch 12/100, Train Loss: 0.3815, Validation Loss: 0.4599\n",
            "Epoch 13/100, Train Loss: 0.3743, Validation Loss: 0.4601\n",
            "Epoch 14/100, Train Loss: 0.3633, Validation Loss: 0.4877\n",
            "Epoch 15/100, Train Loss: 0.3653, Validation Loss: 0.4659\n",
            "Epoch 16/100, Train Loss: 0.3543, Validation Loss: 0.4750\n",
            "Epoch 17/100, Train Loss: 0.3490, Validation Loss: 0.4887\n",
            "Epoch 18/100, Train Loss: 0.3483, Validation Loss: 0.5128\n",
            "Epoch 19/100, Train Loss: 0.3431, Validation Loss: 0.5416\n",
            "Epoch 20/100, Train Loss: 0.3418, Validation Loss: 0.4977\n",
            "Epoch 21/100, Train Loss: 0.3386, Validation Loss: 0.5357\n",
            "Epoch 22/100, Train Loss: 0.3398, Validation Loss: 0.5190\n",
            "Epoch 23/100, Train Loss: 0.3420, Validation Loss: 0.4954\n",
            "Epoch 24/100, Train Loss: 0.3367, Validation Loss: 0.4983\n",
            "Epoch 25/100, Train Loss: 0.3361, Validation Loss: 0.4963\n",
            "Epoch 26/100, Train Loss: 0.3347, Validation Loss: 0.5004\n",
            "Epoch 27/100, Train Loss: 0.3357, Validation Loss: 0.5000\n",
            "Epoch 28/100, Train Loss: 0.3394, Validation Loss: 0.5613\n",
            "Epoch 29/100, Train Loss: 0.3349, Validation Loss: 0.5015\n",
            "Epoch 30/100, Train Loss: 0.3326, Validation Loss: 0.5044\n",
            "Epoch 31/100, Train Loss: 0.3322, Validation Loss: 0.5031\n",
            "Epoch 32/100, Train Loss: 0.3328, Validation Loss: 0.4988\n",
            "Epoch 33/100, Train Loss: 0.3322, Validation Loss: 0.5021\n",
            "Epoch 34/100, Train Loss: 0.3322, Validation Loss: 0.5045\n",
            "Epoch 35/100, Train Loss: 0.3322, Validation Loss: 0.4973\n",
            "Epoch 36/100, Train Loss: 0.3338, Validation Loss: 0.7876\n",
            "Epoch 37/100, Train Loss: 0.3368, Validation Loss: 0.5081\n",
            "Epoch 38/100, Train Loss: 0.3327, Validation Loss: 0.5069\n",
            "Epoch 39/100, Train Loss: 0.3323, Validation Loss: 0.5268\n",
            "Epoch 40/100, Train Loss: 0.3323, Validation Loss: 0.5101\n",
            "Epoch 41/100, Train Loss: 0.3332, Validation Loss: 0.4970\n",
            "Epoch 42/100, Train Loss: 0.3381, Validation Loss: 0.4951\n",
            "Epoch 43/100, Train Loss: 0.3325, Validation Loss: 0.5040\n",
            "Epoch 44/100, Train Loss: 0.3319, Validation Loss: 0.5055\n",
            "Epoch 45/100, Train Loss: 0.3321, Validation Loss: 0.5040\n",
            "Epoch 46/100, Train Loss: 0.3317, Validation Loss: 0.4872\n",
            "Epoch 47/100, Train Loss: 0.3318, Validation Loss: 0.4940\n",
            "Epoch 48/100, Train Loss: 0.3330, Validation Loss: 0.4929\n",
            "Epoch 49/100, Train Loss: 0.3321, Validation Loss: 0.4955\n",
            "Epoch 50/100, Train Loss: 0.3448, Validation Loss: 0.5329\n",
            "Epoch 51/100, Train Loss: 0.3352, Validation Loss: 0.5203\n",
            "Epoch 52/100, Train Loss: 0.3332, Validation Loss: 0.5081\n",
            "Epoch 53/100, Train Loss: 0.3319, Validation Loss: 0.5001\n",
            "Epoch 54/100, Train Loss: 0.3323, Validation Loss: 0.5047\n",
            "Epoch 55/100, Train Loss: 0.3319, Validation Loss: 0.5075\n",
            "Epoch 56/100, Train Loss: 0.3316, Validation Loss: 0.5035\n",
            "Epoch 57/100, Train Loss: 0.3315, Validation Loss: 0.5031\n",
            "Epoch 58/100, Train Loss: 0.3320, Validation Loss: 0.4962\n",
            "Epoch 59/100, Train Loss: 0.3316, Validation Loss: 0.5016\n",
            "Epoch 60/100, Train Loss: 0.3313, Validation Loss: 0.4958\n",
            "Epoch 61/100, Train Loss: 0.3321, Validation Loss: 0.4947\n",
            "Epoch 62/100, Train Loss: 0.3313, Validation Loss: 0.5003\n",
            "Epoch 63/100, Train Loss: 0.3315, Validation Loss: 0.4915\n",
            "Epoch 64/100, Train Loss: 0.3316, Validation Loss: 0.5722\n",
            "Epoch 65/100, Train Loss: 0.3313, Validation Loss: 0.4995\n",
            "Epoch 66/100, Train Loss: 0.3304, Validation Loss: 0.5206\n",
            "Epoch 67/100, Train Loss: 0.3316, Validation Loss: 0.5019\n",
            "Epoch 68/100, Train Loss: 0.3309, Validation Loss: 0.4944\n",
            "Epoch 69/100, Train Loss: 0.3317, Validation Loss: 0.5089\n",
            "Epoch 70/100, Train Loss: 0.3303, Validation Loss: 0.4920\n",
            "Epoch 71/100, Train Loss: 0.3302, Validation Loss: 0.5008\n",
            "Epoch 72/100, Train Loss: 0.3305, Validation Loss: 0.4934\n",
            "Epoch 73/100, Train Loss: 0.3299, Validation Loss: 0.4975\n",
            "Epoch 74/100, Train Loss: 0.3312, Validation Loss: 0.5141\n",
            "Epoch 75/100, Train Loss: 0.3307, Validation Loss: 0.4912\n",
            "Epoch 76/100, Train Loss: 0.3300, Validation Loss: 0.4910\n",
            "Epoch 77/100, Train Loss: 0.3301, Validation Loss: 0.4981\n",
            "Epoch 78/100, Train Loss: 0.3302, Validation Loss: 0.4945\n",
            "Epoch 79/100, Train Loss: 0.3362, Validation Loss: 0.5960\n",
            "Epoch 80/100, Train Loss: 0.3319, Validation Loss: 0.5063\n",
            "Epoch 81/100, Train Loss: 0.3304, Validation Loss: 0.4944\n",
            "Epoch 82/100, Train Loss: 0.3306, Validation Loss: 0.4996\n",
            "Epoch 83/100, Train Loss: 0.3296, Validation Loss: 0.4982\n",
            "Epoch 84/100, Train Loss: 0.3302, Validation Loss: 0.4988\n",
            "Epoch 85/100, Train Loss: 0.3302, Validation Loss: 0.5042\n",
            "Epoch 86/100, Train Loss: 0.3302, Validation Loss: 0.4955\n",
            "Epoch 87/100, Train Loss: 0.3301, Validation Loss: 0.4996\n",
            "Epoch 88/100, Train Loss: 0.3299, Validation Loss: 0.4930\n",
            "Epoch 89/100, Train Loss: 0.3301, Validation Loss: 0.5193\n",
            "Epoch 90/100, Train Loss: 0.3298, Validation Loss: 0.5024\n",
            "Epoch 91/100, Train Loss: 0.3297, Validation Loss: 0.4867\n",
            "Epoch 92/100, Train Loss: 0.3327, Validation Loss: 0.4934\n",
            "Epoch 93/100, Train Loss: 0.3299, Validation Loss: 0.4880\n",
            "Epoch 94/100, Train Loss: 0.3333, Validation Loss: 0.8130\n",
            "Epoch 95/100, Train Loss: 0.3613, Validation Loss: 0.5093\n",
            "Epoch 96/100, Train Loss: 0.3307, Validation Loss: 0.4985\n",
            "Epoch 97/100, Train Loss: 0.3290, Validation Loss: 0.4908\n",
            "Epoch 98/100, Train Loss: 0.3304, Validation Loss: 0.5049\n",
            "Epoch 99/100, Train Loss: 0.3292, Validation Loss: 0.4856\n",
            "Epoch 100/100, Train Loss: 0.3289, Validation Loss: 0.4928\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6706, Validation Loss: 0.6088\n",
            "Epoch 2/100, Train Loss: 0.6054, Validation Loss: 0.5114\n",
            "Epoch 3/100, Train Loss: 0.5493, Validation Loss: 0.4745\n",
            "Epoch 4/100, Train Loss: 0.5123, Validation Loss: 0.4456\n",
            "Epoch 5/100, Train Loss: 0.4832, Validation Loss: 0.4309\n",
            "Epoch 6/100, Train Loss: 0.4634, Validation Loss: 0.4152\n",
            "Epoch 7/100, Train Loss: 0.4417, Validation Loss: 0.4285\n",
            "Epoch 8/100, Train Loss: 0.4285, Validation Loss: 0.4304\n",
            "Epoch 9/100, Train Loss: 0.4110, Validation Loss: 0.5266\n",
            "Epoch 10/100, Train Loss: 0.3961, Validation Loss: 0.4125\n",
            "Epoch 11/100, Train Loss: 0.3856, Validation Loss: 0.4718\n",
            "Epoch 12/100, Train Loss: 0.3811, Validation Loss: 0.5182\n",
            "Epoch 13/100, Train Loss: 0.3712, Validation Loss: 0.4657\n",
            "Epoch 14/100, Train Loss: 0.3629, Validation Loss: 0.4650\n",
            "Epoch 15/100, Train Loss: 0.3570, Validation Loss: 0.4715\n",
            "Epoch 16/100, Train Loss: 0.3530, Validation Loss: 0.4681\n",
            "Epoch 17/100, Train Loss: 0.3549, Validation Loss: 0.5274\n",
            "Epoch 18/100, Train Loss: 0.3524, Validation Loss: 0.4627\n",
            "Epoch 19/100, Train Loss: 0.3470, Validation Loss: 0.5031\n",
            "Epoch 20/100, Train Loss: 0.3418, Validation Loss: 0.4879\n",
            "Epoch 21/100, Train Loss: 0.3407, Validation Loss: 0.4864\n",
            "Epoch 22/100, Train Loss: 0.3400, Validation Loss: 0.4800\n",
            "Epoch 23/100, Train Loss: 0.3392, Validation Loss: 0.5026\n",
            "Epoch 24/100, Train Loss: 0.3377, Validation Loss: 0.4583\n",
            "Epoch 25/100, Train Loss: 0.3378, Validation Loss: 0.4738\n",
            "Epoch 26/100, Train Loss: 0.3363, Validation Loss: 0.5289\n",
            "Epoch 27/100, Train Loss: 0.3358, Validation Loss: 0.4873\n",
            "Epoch 28/100, Train Loss: 0.3351, Validation Loss: 0.4814\n",
            "Epoch 29/100, Train Loss: 0.3362, Validation Loss: 0.4695\n",
            "Epoch 30/100, Train Loss: 0.3351, Validation Loss: 0.4818\n",
            "Epoch 31/100, Train Loss: 0.3354, Validation Loss: 0.4939\n",
            "Epoch 32/100, Train Loss: 0.3346, Validation Loss: 0.4869\n",
            "Epoch 33/100, Train Loss: 0.3340, Validation Loss: 0.4718\n",
            "Epoch 34/100, Train Loss: 0.3331, Validation Loss: 0.4838\n",
            "Epoch 35/100, Train Loss: 0.3325, Validation Loss: 0.4851\n",
            "Epoch 36/100, Train Loss: 0.3324, Validation Loss: 0.5006\n",
            "Epoch 37/100, Train Loss: 0.3361, Validation Loss: 0.4917\n",
            "Epoch 38/100, Train Loss: 0.3324, Validation Loss: 0.4714\n",
            "Epoch 39/100, Train Loss: 0.3323, Validation Loss: 0.4734\n",
            "Epoch 40/100, Train Loss: 0.3328, Validation Loss: 0.4721\n",
            "Epoch 41/100, Train Loss: 0.3320, Validation Loss: 0.4743\n",
            "Epoch 42/100, Train Loss: 0.3310, Validation Loss: 0.4733\n",
            "Epoch 43/100, Train Loss: 0.3315, Validation Loss: 0.4990\n",
            "Epoch 44/100, Train Loss: 0.3309, Validation Loss: 0.4708\n",
            "Epoch 45/100, Train Loss: 0.3307, Validation Loss: 0.4956\n",
            "Epoch 46/100, Train Loss: 0.3309, Validation Loss: 0.4685\n",
            "Epoch 47/100, Train Loss: 0.3322, Validation Loss: 0.4716\n",
            "Epoch 48/100, Train Loss: 0.3311, Validation Loss: 0.4741\n",
            "Epoch 49/100, Train Loss: 0.3317, Validation Loss: 0.4739\n",
            "Epoch 50/100, Train Loss: 0.3312, Validation Loss: 0.4783\n",
            "Epoch 51/100, Train Loss: 0.3305, Validation Loss: 0.4817\n",
            "Epoch 52/100, Train Loss: 0.3307, Validation Loss: 0.4767\n",
            "Epoch 53/100, Train Loss: 0.3306, Validation Loss: 0.4690\n",
            "Epoch 54/100, Train Loss: 0.3311, Validation Loss: 0.4802\n",
            "Epoch 55/100, Train Loss: 0.3322, Validation Loss: 0.5027\n",
            "Epoch 56/100, Train Loss: 0.3312, Validation Loss: 0.4738\n",
            "Epoch 57/100, Train Loss: 0.3310, Validation Loss: 0.4726\n",
            "Epoch 58/100, Train Loss: 0.3371, Validation Loss: 0.5032\n",
            "Epoch 59/100, Train Loss: 0.3320, Validation Loss: 0.4745\n",
            "Epoch 60/100, Train Loss: 0.3303, Validation Loss: 0.5003\n",
            "Epoch 61/100, Train Loss: 0.3303, Validation Loss: 0.4722\n",
            "Epoch 62/100, Train Loss: 0.3302, Validation Loss: 0.4855\n",
            "Epoch 63/100, Train Loss: 0.3312, Validation Loss: 0.4648\n",
            "Epoch 64/100, Train Loss: 0.3307, Validation Loss: 0.4728\n",
            "Epoch 65/100, Train Loss: 0.3306, Validation Loss: 0.4698\n",
            "Epoch 66/100, Train Loss: 0.3310, Validation Loss: 0.4709\n",
            "Epoch 67/100, Train Loss: 0.3303, Validation Loss: 0.4804\n",
            "Epoch 68/100, Train Loss: 0.3305, Validation Loss: 0.4648\n",
            "Epoch 69/100, Train Loss: 0.3307, Validation Loss: 0.4917\n",
            "Epoch 70/100, Train Loss: 0.3306, Validation Loss: 0.4936\n",
            "Epoch 71/100, Train Loss: 0.3307, Validation Loss: 0.4771\n",
            "Epoch 72/100, Train Loss: 0.3307, Validation Loss: 0.4650\n",
            "Epoch 73/100, Train Loss: 0.3311, Validation Loss: 0.4934\n",
            "Epoch 74/100, Train Loss: 0.3304, Validation Loss: 0.4628\n",
            "Epoch 75/100, Train Loss: 0.3319, Validation Loss: 0.4766\n",
            "Epoch 76/100, Train Loss: 0.3309, Validation Loss: 0.4716\n",
            "Epoch 77/100, Train Loss: 0.3305, Validation Loss: 0.4641\n",
            "Epoch 78/100, Train Loss: 0.3317, Validation Loss: 0.4685\n",
            "Epoch 79/100, Train Loss: 0.3307, Validation Loss: 0.4651\n",
            "Epoch 80/100, Train Loss: 0.3305, Validation Loss: 0.4759\n",
            "Epoch 81/100, Train Loss: 0.3308, Validation Loss: 0.4758\n",
            "Epoch 82/100, Train Loss: 0.3336, Validation Loss: 0.4676\n",
            "Epoch 83/100, Train Loss: 0.3307, Validation Loss: 0.4705\n",
            "Epoch 84/100, Train Loss: 0.3305, Validation Loss: 0.4756\n",
            "Epoch 85/100, Train Loss: 0.3303, Validation Loss: 0.4811\n",
            "Epoch 86/100, Train Loss: 0.3306, Validation Loss: 0.4645\n",
            "Epoch 87/100, Train Loss: 0.3303, Validation Loss: 0.4583\n",
            "Epoch 88/100, Train Loss: 0.3311, Validation Loss: 0.4806\n",
            "Epoch 89/100, Train Loss: 0.3306, Validation Loss: 0.4740\n",
            "Epoch 90/100, Train Loss: 0.3303, Validation Loss: 0.5032\n",
            "Epoch 91/100, Train Loss: 0.3307, Validation Loss: 0.4633\n",
            "Epoch 92/100, Train Loss: 0.3302, Validation Loss: 0.4641\n",
            "Epoch 93/100, Train Loss: 0.3305, Validation Loss: 0.4822\n",
            "Epoch 94/100, Train Loss: 0.3310, Validation Loss: 0.4709\n",
            "Epoch 95/100, Train Loss: 0.3306, Validation Loss: 0.4705\n",
            "Epoch 96/100, Train Loss: 0.3304, Validation Loss: 0.4675\n",
            "Epoch 97/100, Train Loss: 0.3306, Validation Loss: 0.4630\n",
            "Epoch 98/100, Train Loss: 0.3302, Validation Loss: 0.4863\n",
            "Epoch 99/100, Train Loss: 0.3304, Validation Loss: 0.4713\n",
            "Epoch 100/100, Train Loss: 0.3327, Validation Loss: 0.4794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6699, Validation Loss: 0.6085\n",
            "Epoch 2/100, Train Loss: 0.6086, Validation Loss: 0.5301\n",
            "Epoch 3/100, Train Loss: 0.5512, Validation Loss: 0.5110\n",
            "Epoch 4/100, Train Loss: 0.5140, Validation Loss: 0.4657\n",
            "Epoch 5/100, Train Loss: 0.4825, Validation Loss: 0.4858\n",
            "Epoch 6/100, Train Loss: 0.4595, Validation Loss: 0.5518\n",
            "Epoch 7/100, Train Loss: 0.4408, Validation Loss: 0.4682\n",
            "Epoch 8/100, Train Loss: 0.4234, Validation Loss: 0.5178\n",
            "Epoch 9/100, Train Loss: 0.4102, Validation Loss: 0.4814\n",
            "Epoch 10/100, Train Loss: 0.3986, Validation Loss: 0.6128\n",
            "Epoch 11/100, Train Loss: 0.3844, Validation Loss: 0.5177\n",
            "Epoch 12/100, Train Loss: 0.3765, Validation Loss: 0.5206\n",
            "Epoch 13/100, Train Loss: 0.3710, Validation Loss: 0.5792\n",
            "Epoch 14/100, Train Loss: 0.3625, Validation Loss: 0.5508\n",
            "Epoch 15/100, Train Loss: 0.3592, Validation Loss: 0.5846\n",
            "Epoch 16/100, Train Loss: 0.3564, Validation Loss: 0.5646\n",
            "Epoch 17/100, Train Loss: 0.3464, Validation Loss: 0.5696\n",
            "Epoch 18/100, Train Loss: 0.3419, Validation Loss: 0.8445\n",
            "Epoch 19/100, Train Loss: 0.3411, Validation Loss: 0.5458\n",
            "Epoch 20/100, Train Loss: 0.3411, Validation Loss: 0.6145\n",
            "Epoch 21/100, Train Loss: 0.3414, Validation Loss: 0.5886\n",
            "Epoch 22/100, Train Loss: 0.3355, Validation Loss: 0.5777\n",
            "Epoch 23/100, Train Loss: 0.3344, Validation Loss: 0.5678\n",
            "Epoch 24/100, Train Loss: 0.3354, Validation Loss: 0.5796\n",
            "Epoch 25/100, Train Loss: 0.3337, Validation Loss: 0.5464\n",
            "Epoch 26/100, Train Loss: 0.3345, Validation Loss: 0.5379\n",
            "Epoch 27/100, Train Loss: 0.3341, Validation Loss: 0.5551\n",
            "Epoch 28/100, Train Loss: 0.3339, Validation Loss: 0.5553\n",
            "Epoch 29/100, Train Loss: 0.3336, Validation Loss: 0.5760\n",
            "Epoch 30/100, Train Loss: 0.3331, Validation Loss: 0.5666\n",
            "Epoch 31/100, Train Loss: 0.3325, Validation Loss: 0.9127\n",
            "Epoch 32/100, Train Loss: 0.3332, Validation Loss: 0.5441\n",
            "Epoch 33/100, Train Loss: 0.3318, Validation Loss: 0.5653\n",
            "Epoch 34/100, Train Loss: 0.3325, Validation Loss: 0.5467\n",
            "Epoch 35/100, Train Loss: 0.3316, Validation Loss: 0.5427\n",
            "Epoch 36/100, Train Loss: 0.3309, Validation Loss: 0.5682\n",
            "Epoch 37/100, Train Loss: 0.3309, Validation Loss: 0.5507\n",
            "Epoch 38/100, Train Loss: 0.3304, Validation Loss: 0.5442\n",
            "Epoch 39/100, Train Loss: 0.3305, Validation Loss: 0.6152\n",
            "Epoch 40/100, Train Loss: 0.3319, Validation Loss: 0.5279\n",
            "Epoch 41/100, Train Loss: 0.3306, Validation Loss: 0.5429\n",
            "Epoch 42/100, Train Loss: 0.3302, Validation Loss: 0.5632\n",
            "Epoch 43/100, Train Loss: 0.3298, Validation Loss: 0.5908\n",
            "Epoch 44/100, Train Loss: 0.3310, Validation Loss: 0.5416\n",
            "Epoch 45/100, Train Loss: 0.3297, Validation Loss: 0.5522\n",
            "Epoch 46/100, Train Loss: 0.3295, Validation Loss: 0.5606\n",
            "Epoch 47/100, Train Loss: 0.3297, Validation Loss: 0.5644\n",
            "Epoch 48/100, Train Loss: 0.3298, Validation Loss: 0.5339\n",
            "Epoch 49/100, Train Loss: 0.3297, Validation Loss: 0.5768\n",
            "Epoch 50/100, Train Loss: 0.3301, Validation Loss: 0.5436\n",
            "Epoch 51/100, Train Loss: 0.3297, Validation Loss: 0.5450\n",
            "Epoch 52/100, Train Loss: 0.3297, Validation Loss: 0.5314\n",
            "Epoch 53/100, Train Loss: 0.3301, Validation Loss: 0.6210\n",
            "Epoch 54/100, Train Loss: 0.3304, Validation Loss: 0.5797\n",
            "Epoch 55/100, Train Loss: 0.3298, Validation Loss: 0.5778\n",
            "Epoch 56/100, Train Loss: 0.3294, Validation Loss: 0.5281\n",
            "Epoch 57/100, Train Loss: 0.3297, Validation Loss: 0.6026\n",
            "Epoch 58/100, Train Loss: 0.3300, Validation Loss: 0.5555\n",
            "Epoch 59/100, Train Loss: 0.3297, Validation Loss: 0.5282\n",
            "Epoch 60/100, Train Loss: 0.3297, Validation Loss: 0.5460\n",
            "Epoch 61/100, Train Loss: 0.3298, Validation Loss: 0.5852\n",
            "Epoch 62/100, Train Loss: 0.3325, Validation Loss: 0.5473\n",
            "Epoch 63/100, Train Loss: 0.3295, Validation Loss: 0.6563\n",
            "Epoch 64/100, Train Loss: 0.3320, Validation Loss: 0.5607\n",
            "Epoch 65/100, Train Loss: 0.3299, Validation Loss: 0.5500\n",
            "Epoch 66/100, Train Loss: 0.3288, Validation Loss: 0.5434\n",
            "Epoch 67/100, Train Loss: 0.3283, Validation Loss: 0.5845\n",
            "Epoch 68/100, Train Loss: 0.3285, Validation Loss: 0.5601\n",
            "Epoch 69/100, Train Loss: 0.3286, Validation Loss: 0.5575\n",
            "Epoch 70/100, Train Loss: 0.3290, Validation Loss: 0.5576\n",
            "Epoch 71/100, Train Loss: 0.3294, Validation Loss: 0.5456\n",
            "Epoch 72/100, Train Loss: 0.3289, Validation Loss: 0.5474\n",
            "Epoch 73/100, Train Loss: 0.3294, Validation Loss: 0.5328\n",
            "Epoch 74/100, Train Loss: 0.3282, Validation Loss: 0.5299\n",
            "Epoch 75/100, Train Loss: 0.3342, Validation Loss: 0.5783\n",
            "Epoch 76/100, Train Loss: 0.3290, Validation Loss: 0.5594\n",
            "Epoch 77/100, Train Loss: 0.3283, Validation Loss: 0.5329\n",
            "Epoch 78/100, Train Loss: 0.3282, Validation Loss: 0.5600\n",
            "Epoch 79/100, Train Loss: 0.3288, Validation Loss: 0.5597\n",
            "Epoch 80/100, Train Loss: 0.3285, Validation Loss: 0.5724\n",
            "Epoch 81/100, Train Loss: 0.3280, Validation Loss: 0.5369\n",
            "Epoch 82/100, Train Loss: 0.3280, Validation Loss: 0.5391\n",
            "Epoch 83/100, Train Loss: 0.3276, Validation Loss: 0.5676\n",
            "Epoch 84/100, Train Loss: 0.3283, Validation Loss: 0.5499\n",
            "Epoch 85/100, Train Loss: 0.3291, Validation Loss: 0.5501\n",
            "Epoch 86/100, Train Loss: 0.3282, Validation Loss: 0.5551\n",
            "Epoch 87/100, Train Loss: 0.3278, Validation Loss: 0.5542\n",
            "Epoch 88/100, Train Loss: 0.3316, Validation Loss: 0.5398\n",
            "Epoch 89/100, Train Loss: 0.3294, Validation Loss: 0.5435\n",
            "Epoch 90/100, Train Loss: 0.3287, Validation Loss: 0.5439\n",
            "Epoch 91/100, Train Loss: 0.3281, Validation Loss: 0.5316\n",
            "Epoch 92/100, Train Loss: 0.3285, Validation Loss: 0.5591\n",
            "Epoch 93/100, Train Loss: 0.3277, Validation Loss: 0.5434\n",
            "Epoch 94/100, Train Loss: 0.3275, Validation Loss: 0.5406\n",
            "Epoch 95/100, Train Loss: 0.3280, Validation Loss: 0.5875\n",
            "Epoch 96/100, Train Loss: 0.3289, Validation Loss: 0.5864\n",
            "Epoch 97/100, Train Loss: 0.3290, Validation Loss: 0.5461\n",
            "Epoch 98/100, Train Loss: 0.3282, Validation Loss: 0.5583\n",
            "Epoch 99/100, Train Loss: 0.3277, Validation Loss: 0.5601\n",
            "Epoch 100/100, Train Loss: 0.3353, Validation Loss: 0.5420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6801, Validation Loss: 0.6219\n",
            "Epoch 2/100, Train Loss: 0.6294, Validation Loss: 0.5087\n",
            "Epoch 3/100, Train Loss: 0.5704, Validation Loss: 0.4323\n",
            "Epoch 4/100, Train Loss: 0.5290, Validation Loss: 0.3961\n",
            "Epoch 5/100, Train Loss: 0.4968, Validation Loss: 0.3695\n",
            "Epoch 6/100, Train Loss: 0.4686, Validation Loss: 0.3823\n",
            "Epoch 7/100, Train Loss: 0.4511, Validation Loss: 0.3519\n",
            "Epoch 8/100, Train Loss: 0.4296, Validation Loss: 0.3614\n",
            "Epoch 9/100, Train Loss: 0.4151, Validation Loss: 0.3777\n",
            "Epoch 10/100, Train Loss: 0.4032, Validation Loss: 0.3648\n",
            "Epoch 11/100, Train Loss: 0.3935, Validation Loss: 0.3442\n",
            "Epoch 12/100, Train Loss: 0.3817, Validation Loss: 0.3740\n",
            "Epoch 13/100, Train Loss: 0.3796, Validation Loss: 0.3611\n",
            "Epoch 14/100, Train Loss: 0.3652, Validation Loss: 0.3740\n",
            "Epoch 15/100, Train Loss: 0.3575, Validation Loss: 0.3761\n",
            "Epoch 16/100, Train Loss: 0.3566, Validation Loss: 0.3896\n",
            "Epoch 17/100, Train Loss: 0.3494, Validation Loss: 0.3754\n",
            "Epoch 18/100, Train Loss: 0.3473, Validation Loss: 0.3891\n",
            "Epoch 19/100, Train Loss: 0.3463, Validation Loss: 0.3694\n",
            "Epoch 20/100, Train Loss: 0.3421, Validation Loss: 0.3771\n",
            "Epoch 21/100, Train Loss: 0.3411, Validation Loss: 0.3991\n",
            "Epoch 22/100, Train Loss: 0.3408, Validation Loss: 0.3861\n",
            "Epoch 23/100, Train Loss: 0.3416, Validation Loss: 0.3791\n",
            "Epoch 24/100, Train Loss: 0.3389, Validation Loss: 0.3708\n",
            "Epoch 25/100, Train Loss: 0.3385, Validation Loss: 0.3751\n",
            "Epoch 26/100, Train Loss: 0.3395, Validation Loss: 0.3832\n",
            "Epoch 27/100, Train Loss: 0.3373, Validation Loss: 0.3741\n",
            "Epoch 28/100, Train Loss: 0.3376, Validation Loss: 0.3775\n",
            "Epoch 29/100, Train Loss: 0.3404, Validation Loss: 0.3759\n",
            "Epoch 30/100, Train Loss: 0.3355, Validation Loss: 0.3710\n",
            "Epoch 31/100, Train Loss: 0.3353, Validation Loss: 0.3695\n",
            "Epoch 32/100, Train Loss: 0.3349, Validation Loss: 0.3692\n",
            "Epoch 33/100, Train Loss: 0.3373, Validation Loss: 0.3714\n",
            "Epoch 34/100, Train Loss: 0.3351, Validation Loss: 0.3782\n",
            "Epoch 35/100, Train Loss: 0.3349, Validation Loss: 0.3684\n",
            "Epoch 36/100, Train Loss: 0.3345, Validation Loss: 0.3818\n",
            "Epoch 37/100, Train Loss: 0.3349, Validation Loss: 0.3780\n",
            "Epoch 38/100, Train Loss: 0.3350, Validation Loss: 0.3668\n",
            "Epoch 39/100, Train Loss: 0.3348, Validation Loss: 0.3776\n",
            "Epoch 40/100, Train Loss: 0.3345, Validation Loss: 0.3704\n",
            "Epoch 41/100, Train Loss: 0.3340, Validation Loss: 0.3704\n",
            "Epoch 42/100, Train Loss: 0.3339, Validation Loss: 0.3788\n",
            "Epoch 43/100, Train Loss: 0.3333, Validation Loss: 0.3714\n",
            "Epoch 44/100, Train Loss: 0.3333, Validation Loss: 0.3659\n",
            "Epoch 45/100, Train Loss: 0.3340, Validation Loss: 0.3846\n",
            "Epoch 46/100, Train Loss: 0.3332, Validation Loss: 0.3684\n",
            "Epoch 47/100, Train Loss: 0.3332, Validation Loss: 0.3638\n",
            "Epoch 48/100, Train Loss: 0.3330, Validation Loss: 0.3857\n",
            "Epoch 49/100, Train Loss: 0.3336, Validation Loss: 0.3714\n",
            "Epoch 50/100, Train Loss: 0.3332, Validation Loss: 0.3762\n",
            "Epoch 51/100, Train Loss: 0.3334, Validation Loss: 0.3619\n",
            "Epoch 52/100, Train Loss: 0.3329, Validation Loss: 0.3647\n",
            "Epoch 53/100, Train Loss: 0.3329, Validation Loss: 0.3573\n",
            "Epoch 54/100, Train Loss: 0.3324, Validation Loss: 0.3964\n",
            "Epoch 55/100, Train Loss: 0.3325, Validation Loss: 0.3621\n",
            "Epoch 56/100, Train Loss: 0.3321, Validation Loss: 0.3602\n",
            "Epoch 57/100, Train Loss: 0.3323, Validation Loss: 0.3768\n",
            "Epoch 58/100, Train Loss: 0.3320, Validation Loss: 0.3569\n",
            "Epoch 59/100, Train Loss: 0.3320, Validation Loss: 0.3706\n",
            "Epoch 60/100, Train Loss: 0.3330, Validation Loss: 0.3752\n",
            "Epoch 61/100, Train Loss: 0.3327, Validation Loss: 0.3612\n",
            "Epoch 62/100, Train Loss: 0.3328, Validation Loss: 0.3509\n",
            "Epoch 63/100, Train Loss: 0.3323, Validation Loss: 0.3659\n",
            "Epoch 64/100, Train Loss: 0.3321, Validation Loss: 0.3671\n",
            "Epoch 65/100, Train Loss: 0.3325, Validation Loss: 0.3645\n",
            "Epoch 66/100, Train Loss: 0.3380, Validation Loss: 0.6887\n",
            "Epoch 67/100, Train Loss: 0.3481, Validation Loss: 0.4126\n",
            "Epoch 68/100, Train Loss: 0.3325, Validation Loss: 0.3805\n",
            "Epoch 69/100, Train Loss: 0.3313, Validation Loss: 0.3751\n",
            "Epoch 70/100, Train Loss: 0.3317, Validation Loss: 0.3680\n",
            "Epoch 71/100, Train Loss: 0.3316, Validation Loss: 0.3711\n",
            "Epoch 72/100, Train Loss: 0.3318, Validation Loss: 0.3649\n",
            "Epoch 73/100, Train Loss: 0.3318, Validation Loss: 0.3662\n",
            "Epoch 74/100, Train Loss: 0.3333, Validation Loss: 0.3594\n",
            "Epoch 75/100, Train Loss: 0.3322, Validation Loss: 0.3677\n",
            "Epoch 76/100, Train Loss: 0.3319, Validation Loss: 0.3678\n",
            "Epoch 77/100, Train Loss: 0.3323, Validation Loss: 0.3603\n",
            "Epoch 78/100, Train Loss: 0.3361, Validation Loss: 0.3694\n",
            "Epoch 79/100, Train Loss: 0.3324, Validation Loss: 0.3557\n",
            "Epoch 80/100, Train Loss: 0.3320, Validation Loss: 0.3645\n",
            "Epoch 81/100, Train Loss: 0.3319, Validation Loss: 0.3615\n",
            "Epoch 82/100, Train Loss: 0.3318, Validation Loss: 0.3645\n",
            "Epoch 83/100, Train Loss: 0.3319, Validation Loss: 0.3665\n",
            "Epoch 84/100, Train Loss: 0.3326, Validation Loss: 0.3639\n",
            "Epoch 85/100, Train Loss: 0.3313, Validation Loss: 0.3649\n",
            "Epoch 86/100, Train Loss: 0.3316, Validation Loss: 0.3713\n",
            "Epoch 87/100, Train Loss: 0.3312, Validation Loss: 0.3558\n",
            "Epoch 88/100, Train Loss: 0.3313, Validation Loss: 0.3671\n",
            "Epoch 89/100, Train Loss: 0.3323, Validation Loss: 0.3667\n",
            "Epoch 90/100, Train Loss: 0.3532, Validation Loss: 0.4267\n",
            "Epoch 91/100, Train Loss: 0.3354, Validation Loss: 0.3802\n",
            "Epoch 92/100, Train Loss: 0.3308, Validation Loss: 0.3707\n",
            "Epoch 93/100, Train Loss: 0.3299, Validation Loss: 0.3761\n",
            "Epoch 94/100, Train Loss: 0.3308, Validation Loss: 0.3793\n",
            "Epoch 95/100, Train Loss: 0.3301, Validation Loss: 0.3731\n",
            "Epoch 96/100, Train Loss: 0.3302, Validation Loss: 0.3611\n",
            "Epoch 97/100, Train Loss: 0.3308, Validation Loss: 0.3540\n",
            "Epoch 98/100, Train Loss: 0.3303, Validation Loss: 0.3605\n",
            "Epoch 99/100, Train Loss: 0.3307, Validation Loss: 0.3686\n",
            "Epoch 100/100, Train Loss: 0.3306, Validation Loss: 0.3658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
            "<ipython-input-44-82be80b692ea>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6721, Validation Loss: 0.6027\n",
            "Epoch 2/100, Train Loss: 0.6111, Validation Loss: 0.5111\n",
            "Epoch 3/100, Train Loss: 0.5518, Validation Loss: 0.4771\n",
            "Epoch 4/100, Train Loss: 0.5124, Validation Loss: 0.4313\n",
            "Epoch 5/100, Train Loss: 0.4788, Validation Loss: 0.4247\n",
            "Epoch 6/100, Train Loss: 0.4583, Validation Loss: 0.4301\n",
            "Epoch 7/100, Train Loss: 0.4385, Validation Loss: 0.4302\n",
            "Epoch 8/100, Train Loss: 0.4202, Validation Loss: 0.4407\n",
            "Epoch 9/100, Train Loss: 0.4098, Validation Loss: 0.4984\n",
            "Epoch 10/100, Train Loss: 0.3945, Validation Loss: 0.4498\n",
            "Epoch 11/100, Train Loss: 0.3870, Validation Loss: 0.4463\n",
            "Epoch 12/100, Train Loss: 0.3831, Validation Loss: 0.4305\n",
            "Epoch 13/100, Train Loss: 0.3706, Validation Loss: 0.4274\n",
            "Epoch 14/100, Train Loss: 0.3654, Validation Loss: 0.4545\n",
            "Epoch 15/100, Train Loss: 0.3622, Validation Loss: 0.4325\n",
            "Epoch 16/100, Train Loss: 0.3542, Validation Loss: 0.4317\n",
            "Epoch 17/100, Train Loss: 0.3509, Validation Loss: 0.4564\n",
            "Epoch 18/100, Train Loss: 0.3474, Validation Loss: 0.4487\n",
            "Epoch 19/100, Train Loss: 0.3477, Validation Loss: 0.4541\n",
            "Epoch 20/100, Train Loss: 0.3419, Validation Loss: 0.4361\n",
            "Epoch 21/100, Train Loss: 0.3398, Validation Loss: 0.4322\n",
            "Epoch 22/100, Train Loss: 0.3383, Validation Loss: 0.4316\n",
            "Epoch 23/100, Train Loss: 0.3377, Validation Loss: 0.4399\n",
            "Epoch 24/100, Train Loss: 0.3370, Validation Loss: 0.4487\n",
            "Epoch 25/100, Train Loss: 0.3367, Validation Loss: 0.4540\n",
            "Epoch 26/100, Train Loss: 0.3360, Validation Loss: 0.4437\n",
            "Epoch 27/100, Train Loss: 0.3363, Validation Loss: 0.4318\n",
            "Epoch 28/100, Train Loss: 0.3358, Validation Loss: 0.4543\n",
            "Epoch 29/100, Train Loss: 0.3356, Validation Loss: 0.4372\n",
            "Epoch 30/100, Train Loss: 0.3353, Validation Loss: 0.4376\n",
            "Epoch 31/100, Train Loss: 0.3353, Validation Loss: 0.4403\n",
            "Epoch 32/100, Train Loss: 0.3374, Validation Loss: 0.4628\n",
            "Epoch 33/100, Train Loss: 0.3352, Validation Loss: 0.4398\n",
            "Epoch 34/100, Train Loss: 0.3336, Validation Loss: 0.4353\n",
            "Epoch 35/100, Train Loss: 0.3343, Validation Loss: 0.4215\n",
            "Epoch 36/100, Train Loss: 0.3346, Validation Loss: 0.4378\n",
            "Epoch 37/100, Train Loss: 0.3335, Validation Loss: 0.4325\n",
            "Epoch 38/100, Train Loss: 0.3335, Validation Loss: 0.4238\n",
            "Epoch 39/100, Train Loss: 0.3328, Validation Loss: 0.4409\n",
            "Epoch 40/100, Train Loss: 0.3326, Validation Loss: 0.4308\n",
            "Epoch 41/100, Train Loss: 0.3324, Validation Loss: 0.4279\n",
            "Epoch 42/100, Train Loss: 0.3339, Validation Loss: 0.4249\n",
            "Epoch 43/100, Train Loss: 0.3318, Validation Loss: 0.4301\n",
            "Epoch 44/100, Train Loss: 0.3323, Validation Loss: 0.4327\n",
            "Epoch 45/100, Train Loss: 0.3326, Validation Loss: 0.4250\n",
            "Epoch 46/100, Train Loss: 0.3308, Validation Loss: 0.4219\n",
            "Epoch 47/100, Train Loss: 0.3312, Validation Loss: 0.4294\n",
            "Epoch 48/100, Train Loss: 0.3308, Validation Loss: 0.4246\n",
            "Epoch 49/100, Train Loss: 0.3306, Validation Loss: 0.4253\n",
            "Epoch 50/100, Train Loss: 0.3315, Validation Loss: 0.4254\n",
            "Epoch 51/100, Train Loss: 0.3308, Validation Loss: 0.4345\n",
            "Epoch 52/100, Train Loss: 0.3314, Validation Loss: 0.4332\n",
            "Epoch 53/100, Train Loss: 0.3308, Validation Loss: 0.4230\n",
            "Epoch 54/100, Train Loss: 0.3305, Validation Loss: 0.4309\n",
            "Epoch 55/100, Train Loss: 0.3304, Validation Loss: 0.4173\n",
            "Epoch 56/100, Train Loss: 0.3303, Validation Loss: 0.4238\n",
            "Epoch 57/100, Train Loss: 0.3300, Validation Loss: 0.4243\n",
            "Epoch 58/100, Train Loss: 0.3305, Validation Loss: 0.4278\n",
            "Epoch 59/100, Train Loss: 0.3300, Validation Loss: 0.4435\n",
            "Epoch 60/100, Train Loss: 0.3308, Validation Loss: 0.4289\n",
            "Epoch 61/100, Train Loss: 0.3307, Validation Loss: 0.4254\n",
            "Epoch 62/100, Train Loss: 0.3301, Validation Loss: 0.4323\n",
            "Epoch 63/100, Train Loss: 0.3421, Validation Loss: 0.4847\n",
            "Epoch 64/100, Train Loss: 0.3378, Validation Loss: 0.4267\n",
            "Epoch 65/100, Train Loss: 0.3306, Validation Loss: 0.4275\n",
            "Epoch 66/100, Train Loss: 0.3294, Validation Loss: 0.4374\n",
            "Epoch 67/100, Train Loss: 0.3298, Validation Loss: 0.4269\n",
            "Epoch 68/100, Train Loss: 0.3297, Validation Loss: 0.4244\n",
            "Epoch 69/100, Train Loss: 0.3306, Validation Loss: 0.4326\n",
            "Epoch 70/100, Train Loss: 0.3305, Validation Loss: 0.5811\n",
            "Epoch 71/100, Train Loss: 0.3355, Validation Loss: 0.4262\n",
            "Epoch 72/100, Train Loss: 0.3292, Validation Loss: 0.4238\n",
            "Epoch 73/100, Train Loss: 0.3298, Validation Loss: 0.4263\n",
            "Epoch 74/100, Train Loss: 0.3297, Validation Loss: 0.4291\n",
            "Epoch 75/100, Train Loss: 0.3296, Validation Loss: 0.4264\n",
            "Epoch 76/100, Train Loss: 0.3306, Validation Loss: 0.4207\n",
            "Epoch 77/100, Train Loss: 0.3298, Validation Loss: 0.4290\n",
            "Epoch 78/100, Train Loss: 0.3310, Validation Loss: 0.4204\n",
            "Epoch 79/100, Train Loss: 0.3305, Validation Loss: 0.4350\n",
            "Epoch 80/100, Train Loss: 0.3300, Validation Loss: 0.4288\n",
            "Epoch 81/100, Train Loss: 0.3298, Validation Loss: 0.4257\n",
            "Epoch 82/100, Train Loss: 0.3303, Validation Loss: 0.4265\n",
            "Epoch 83/100, Train Loss: 0.3306, Validation Loss: 0.4362\n",
            "Epoch 84/100, Train Loss: 0.3303, Validation Loss: 0.4323\n",
            "Epoch 85/100, Train Loss: 0.3301, Validation Loss: 0.4318\n",
            "Epoch 86/100, Train Loss: 0.3297, Validation Loss: 0.4215\n",
            "Epoch 87/100, Train Loss: 0.3299, Validation Loss: 0.4255\n",
            "Epoch 88/100, Train Loss: 0.3298, Validation Loss: 0.4182\n",
            "Epoch 89/100, Train Loss: 0.3339, Validation Loss: 0.4308\n",
            "Epoch 90/100, Train Loss: 0.3299, Validation Loss: 0.4238\n",
            "Epoch 91/100, Train Loss: 0.3297, Validation Loss: 0.4273\n",
            "Epoch 92/100, Train Loss: 0.3301, Validation Loss: 0.4267\n",
            "Epoch 93/100, Train Loss: 0.3300, Validation Loss: 0.4285\n",
            "Epoch 94/100, Train Loss: 0.3322, Validation Loss: 0.4304\n",
            "Epoch 95/100, Train Loss: 0.3317, Validation Loss: 0.4221\n",
            "Epoch 96/100, Train Loss: 0.3297, Validation Loss: 0.4256\n",
            "Epoch 97/100, Train Loss: 0.3296, Validation Loss: 0.4172\n",
            "Epoch 98/100, Train Loss: 0.3299, Validation Loss: 0.4417\n",
            "Epoch 99/100, Train Loss: 0.3301, Validation Loss: 0.4209\n",
            "Epoch 100/100, Train Loss: 0.3302, Validation Loss: 0.4257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-82be80b692ea>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
            "<ipython-input-44-82be80b692ea>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n"
          ]
        }
      ],
      "source": [
        "# Performing cross-validation to get train/val accuracy\n",
        "# for all hyper-parameter settings in the list below.\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "weight_decays = [0., 0.01]\n",
        "batch_size = 50\n",
        "n_epochs = 100\n",
        "n_folds = 5\n",
        "\n",
        "results = []\n",
        "for lr in learning_rates:\n",
        "    for wd in weight_decays:\n",
        "        val_accs = []  # to store validation accuracy for each fold\n",
        "        train_accs = []  # to store training accuracy for each fold\n",
        "\n",
        "        # iterating over folds, using \"shuffle=True\" as datapoints are not shuffled\n",
        "        kf = KFold(n_splits=n_folds, shuffle=True)\n",
        "\n",
        "        for train_index, val_index in kf.split(X):\n",
        "\n",
        "\n",
        "\n",
        "            # Splitting data into train and validation\n",
        "\n",
        "            X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
        "            y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
        "\n",
        "\n",
        "            # Creating data loaders to pass to training loop\n",
        "\n",
        "            train_dataset = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
        "            val_dataset = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n",
        "\n",
        "\n",
        "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            # Initializing model, criterion (Cross entropy loss), and optimizer (SGD with various hyperparameters)\n",
        "\n",
        "            model = MyMLP()\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = SGD(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "\n",
        "            # Calling the training function\n",
        "            train(model, train_loader, val_loader, n_epochs, optimizer, criterion, verbose=False)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # Using the trained model to estimate train/val accuracy\n",
        "                # (Our model outputs logits, argmax is good to get the class prediction corresponding to max logit)\n",
        "\n",
        "                # Choosing class with max logit\n",
        "                train_predictions = torch.argmax(model(torch.tensor(X_train_fold)), axis=1)\n",
        "                val_predictions = torch.argmax(model(torch.tensor(X_val_fold)), axis=1)\n",
        "\n",
        "\n",
        "                train_acc = accuracy_score(y_train_fold, train_predictions)\n",
        "                train_accs.append(train_acc)\n",
        "\n",
        "                val_acc = accuracy_score(y_val_fold, val_predictions)\n",
        "                val_accs.append(val_acc)\n",
        "\n",
        "\n",
        "        # For each hyper-parameter, I'm storing the parameter values and the mean and standard error of accuracy in a list in \"results\".\n",
        "        train_std, train_mean = torch.std_mean(torch.tensor(train_accs))\n",
        "        val_std, val_mean = torch.std_mean(torch.tensor(val_accs))\n",
        "        rootn = torch.sqrt(torch.tensor(n_folds))  # n is number of folds\n",
        "        train_se, val_se = train_std / rootn, val_std / rootn\n",
        "        # Storing learning rate, weight decay value, train mean accuracy, standard error, val mean accuracy, standard error\n",
        "        results.append((lr, wd, train_mean.item(), train_se.item(), val_mean.item(), val_se.item()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeUCeMLQy7WI"
      },
      "source": [
        "## Show result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSptZKUry7WI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "outputId": "c097c5dc-9e69-476d-9399-b27630260e98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training results\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "learning_rate            0.001            0.010            0.100\n",
              "weight_decay                                                    \n",
              "0.00           0.744 +/- 0.018  0.986 +/- 0.002  0.993 +/- 0.001\n",
              "0.01           0.754 +/- 0.010  0.983 +/- 0.001  0.996 +/- 0.000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2d99da95-9df4-4fed-ab36-6c64ae4396b4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>learning_rate</th>\n",
              "      <th>0.001</th>\n",
              "      <th>0.010</th>\n",
              "      <th>0.100</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_decay</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.00</th>\n",
              "      <td>0.744 +/- 0.018</td>\n",
              "      <td>0.986 +/- 0.002</td>\n",
              "      <td>0.993 +/- 0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.01</th>\n",
              "      <td>0.754 +/- 0.010</td>\n",
              "      <td>0.983 +/- 0.001</td>\n",
              "      <td>0.996 +/- 0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2d99da95-9df4-4fed-ab36-6c64ae4396b4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2d99da95-9df4-4fed-ab36-6c64ae4396b4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2d99da95-9df4-4fed-ab36-6c64ae4396b4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f3984d17-6bf5-4eeb-a954-b82ef365a89e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f3984d17-6bf5-4eeb-a954-b82ef365a89e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f3984d17-6bf5-4eeb-a954-b82ef365a89e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pivot_df",
              "summary": "{\n  \"name\": \"pivot_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": 0.001,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.754 +/- 0.010\",\n          \"0.744 +/- 0.018\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.01,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.983 +/- 0.001\",\n          \"0.986 +/- 0.002\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.1,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.996 +/- 0.000\",\n          \"0.993 +/- 0.001\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation results\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "learning_rate            0.001            0.010            0.100\n",
              "weight_decay                                                    \n",
              "0.00           0.708 +/- 0.029  0.814 +/- 0.007  0.818 +/- 0.008\n",
              "0.01           0.713 +/- 0.014  0.816 +/- 0.006  0.812 +/- 0.006"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a0b73364-c2f4-410c-9026-8c346cddae68\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>learning_rate</th>\n",
              "      <th>0.001</th>\n",
              "      <th>0.010</th>\n",
              "      <th>0.100</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_decay</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.00</th>\n",
              "      <td>0.708 +/- 0.029</td>\n",
              "      <td>0.814 +/- 0.007</td>\n",
              "      <td>0.818 +/- 0.008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.01</th>\n",
              "      <td>0.713 +/- 0.014</td>\n",
              "      <td>0.816 +/- 0.006</td>\n",
              "      <td>0.812 +/- 0.006</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a0b73364-c2f4-410c-9026-8c346cddae68')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a0b73364-c2f4-410c-9026-8c346cddae68 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a0b73364-c2f4-410c-9026-8c346cddae68');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-770a7dc0-5843-4814-8b6b-3bc45c6b83a7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-770a7dc0-5843-4814-8b6b-3bc45c6b83a7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-770a7dc0-5843-4814-8b6b-3bc45c6b83a7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pivot_df",
              "summary": "{\n  \"name\": \"pivot_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": 0.001,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.713 +/- 0.014\",\n          \"0.708 +/- 0.029\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.01,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.816 +/- 0.006\",\n          \"0.814 +/- 0.007\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0.1,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"0.812 +/- 0.006\",\n          \"0.818 +/- 0.008\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# You should be able to see a best train acc > 95% , and a best val acc > 80%\n",
        "\n",
        "# Create a DataFrame from the list of tuples, with labeled columns\n",
        "column_names = ['learning_rate', 'weight_decay', 'train_mean', 'train_se','val_mean', 'val_se']\n",
        "df = pd.DataFrame(results, columns=column_names)\n",
        "\n",
        "# Make pretty printable strings, with standard error bars\n",
        "df['train_output'] = df.apply(lambda row: f\"{row['train_mean']:.3f} +/- {row['train_se']:.3f}\", axis=1)\n",
        "df['val_output'] = df.apply(lambda row: f\"{row['val_mean']:.3f} +/- {row['val_se']:.3f}\", axis=1)\n",
        "\n",
        "print('Training results')\n",
        "pivot_df = df.pivot(index='weight_decay', columns='learning_rate', values='train_output')\n",
        "display(pivot_df)\n",
        "\n",
        "print('Validation results')\n",
        "pivot_df = df.pivot(index='weight_decay', columns='learning_rate', values='val_output')\n",
        "display(pivot_df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}